name: Generate BYL Project Files with Full Transcripts

on:
  push:
    paths:
      - 'index/**'
      - 'episodes/**'
  workflow_dispatch:

jobs:
  generate:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install pyyaml

      - name: Classify and generate project files
        run: |
          cat << 'PYTHON_SCRIPT' > classify_and_generate.py
          import os
          import yaml
          import re
          from pathlib import Path
          from datetime import datetime, timezone

          # â”€â”€ Constants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          # 200k tokens â‰ˆ 800k words â‰ˆ ~4.8MB of plain text (conservative: use 4MB)
          MAX_CHARS_PER_FILE = 4_000_000

          # â”€â”€ Topic â†’ Project mapping â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          TOPIC_TO_PROJECT = {
            "product-strategy":        1, "product-market-fit":      1, "innovation":              1,
            "business-strategy":       1, "strategy":                1, "founder-mode":            1,
            "entrepreneurship":        1, "bootstrapping":           1, "network-effects":         1,
            "marketplaces":            1, "airbnb":                  1, "stripe":                  1,

            "product-management":      2, "product-development":     2, "product-leadership":      2,
            "prioritization":          2, "okrs":                    2, "decision-making":         2,
            "experimentation":         2, "ab-testing":              2, "agile":                   2,
            "focus":                   2, "google":                  2, "microsoft":               2,

            "growth-strategy":         3, "startup-growth":          3, "product-led-growth":      3,
            "retention":               3, "sales":                   3, "enterprise-sales":        3,
            "marketing":               3, "brand-building":          3, "branding":                3,
            "word-of-mouth":           3, "community-building":      3, "uber":                    3,
            "linkedin":                3,

            "ai":                      4, "machine-learning":        4, "chatgpt":                 4,
            "openai":                  4,

            "leadership":              5, "management":              5, "organizational-design":   5,
            "executive-coaching":      5, "company-culture":         5, "startup-culture":         5,
            "feedback":                5, "influence":               5, "power":                   5,
            "communication":           5, "storytelling":            5, "slack":                   5,

            "hiring":                  6, "recruiting":              6, "team-building":           6,
            "mentorship":              6, "executive-search":        6, "remote-work":             6,
            "work-life-balance":       6, "facebook":                6, "meta":                    6,

            "engineering":             7, "productivity":            7, "time-management":         7,
            "design":                  7, "user-experience":         7, "customer-experience":     7,
            "customer-research":       7,

            "career-development":      8, "career-growth":           8, "personal-development":    8,
            "personal-branding":       8, "personal-transformation": 8, "skill-building":          8,
            "networking":              8, "anxiety-management":      8, "mental-health":           8,
            "stress-management":       8, "psychology":              8, "neuroscience":            8,

            "analytics":               9, "data-analytics":          9,

            "venture-capital":        10, "media-relations":        10,
          }

          MULTI_PROJECT_TOPICS = {
            "experimentation":    [2, 4, 9],
            "ab-testing":         [2, 9],
            "machine-learning":   [4, 9],
            "okrs":               [2, 7],
            "agile":              [2, 7],
            "focus":              [2, 7],
            "slack":              [5, 7],
            "mentorship":         [6, 8],
            "entrepreneurship":   [1, 10],
            "business-strategy":  [1, 10],
            "bootstrapping":      [1, 10],
            "enterprise-sales":   [3, 10],
          }

          PROJECT_NAMES = {
            1:  "Product Strategy & Vision",
            2:  "Product Management & Development",
            3:  "Growth & Revenue",
            4:  "AI & Machine Learning",
            5:  "Leadership & Management",
            6:  "Hiring, Teams & People",
            7:  "Workflow, Productivity & Engineering",
            8:  "Career & Personal Growth",
            9:  "Data, Analytics & Experimentation",
            10: "Venture, Finance & Strategy",
          }

          PROJECT_FILE_PREFIX = {
            1:  "01-product-strategy-vision",
            2:  "02-product-management-development",
            3:  "03-growth-revenue",
            4:  "04-ai-machine-learning",
            5:  "05-leadership-management",
            6:  "06-hiring-teams-people",
            7:  "07-workflow-productivity-engineering",
            8:  "08-career-personal-growth",
            9:  "09-data-analytics-experimentation",
            10: "10-venture-finance-strategy",
          }

          # â”€â”€ Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

          def normalize_tag(tag):
              return re.sub(r'[^a-z0-9]+', '-', tag.lower()).strip('-')

          def get_projects_for_tags(tags):
              projects = set()
              for tag in tags:
                  norm = normalize_tag(tag)
                  if norm in MULTI_PROJECT_TOPICS:
                      projects.update(MULTI_PROJECT_TOPICS[norm])
                  elif norm in TOPIC_TO_PROJECT:
                      projects.add(TOPIC_TO_PROJECT[norm])
              return projects

          def parse_transcript(filepath):
              try:
                  content = filepath.read_text(encoding='utf-8', errors='replace')
                  parts = content.split('---', 2)
                  if len(parts) >= 3:
                      try:
                          meta = yaml.safe_load(parts[1]) or {}
                      except:
                          meta = {}
                      body = parts[2].strip()
                  else:
                      meta = {}
                      body = content.strip()
                  return meta, body
              except Exception as e:
                  print(f"  WARNING: Could not parse {filepath}: {e}")
                  return {}, ""

          def get_tags_from_transcript(meta, body, guest_folder):
              tags = []
              for field in ['tags', 'keywords', 'topics', 'categories']:
                  val = meta.get(field)
                  if isinstance(val, list):
                      tags.extend([str(t) for t in val])
                  elif isinstance(val, str):
                      tags.extend([t.strip() for t in val.split(',')])
              folder_norm = normalize_tag(guest_folder)
              if folder_norm in TOPIC_TO_PROJECT:
                  tags.append(folder_norm)
              if not tags:
                  text_to_scan = (
                      str(meta.get('title', '')) + ' ' +
                      str(meta.get('description', '')) + ' ' +
                      body[:3000]
                  ).lower()
                  for topic in TOPIC_TO_PROJECT.keys():
                      if topic.replace('-', ' ') in text_to_scan:
                          tags.append(topic)
              return tags

          def build_transcript_block(t):
              """Render a single transcript as a markdown block."""
              meta = t['meta']
              guest = meta.get('guest', t['dir'])
              title = meta.get('title', f"Episode: {t['dir']}")
              date  = meta.get('publish_date', 'Unknown')
              url   = meta.get('youtube_url', '')
              tags  = t['tags']

              block = f"## {title}\n"
              block += f"**Guest:** {guest}  \n"
              block += f"**Published:** {date}  \n"
              if url:
                  block += f"**YouTube:** {url}  \n"
              if tags:
                  block += f"**Tags:** {', '.join(tags[:10])}  \n"
              block += "\n"
              block += t['body']
              block += "\n\n---\n\n"
              return block

          def write_chunks(output_dir, prefix, proj_name, timestamp, header_block, transcript_blocks):
              """
              Write one or more chunk files so no single file exceeds MAX_CHARS_PER_FILE.
              Returns list of filenames written.
              """
              files_written = []
              chunk_index = 1
              current_chars = 0
              current_blocks = []

              def flush_chunk(blocks, index, is_last, total_chunks_hint="?"):
                  filename = f"{prefix}-part{index}.md" if total_chunks_hint != 1 else f"{prefix}.md"
                  filepath = output_dir / filename
                  with open(filepath, 'w', encoding='utf-8') as f:
                      f.write(f"# BYL Brain: {proj_name}")
                      if total_chunks_hint != 1:
                          f.write(f" (Part {index})")
                      f.write(f"\n")
                      f.write(f"_Auto-generated from Lenny's Podcast Transcripts Archive_\n")
                      f.write(f"_Last updated: {timestamp}_\n")
                      if total_chunks_hint != 1:
                          f.write(f"_This is part {index} of a multi-part project file._\n")
                      f.write(f"\n---\n\n")
                      if index == 1:
                          f.write(header_block)
                      f.write("## FULL TRANSCRIPTS\n\n")
                      f.write("---\n\n")
                      for b in blocks:
                          f.write(b)
                  size_mb = filepath.stat().st_size / (1024 * 1024)
                  print(f"    -> Wrote {filename} ({len(blocks)} episodes, {size_mb:.2f} MB)")
                  return filename

              # First pass: figure out how many chunks we need
              total_size = len(header_block)
              for b in transcript_blocks:
                  total_size += len(b)
              estimated_chunks = max(1, -(-total_size // MAX_CHARS_PER_FILE))  # ceiling div

              # Second pass: write chunks
              chunk_files = []
              current_chars = len(header_block) if estimated_chunks == 1 else 0
              current_blocks = []

              for b in transcript_blocks:
                  if current_chars + len(b) > MAX_CHARS_PER_FILE and current_blocks:
                      fname = flush_chunk(current_blocks, chunk_index, False, estimated_chunks)
                      chunk_files.append(fname)
                      chunk_index += 1
                      current_blocks = []
                      current_chars = 0
                  current_blocks.append(b)
                  current_chars += len(b)

              if current_blocks:
                  fname = flush_chunk(current_blocks, chunk_index, True, estimated_chunks if estimated_chunks > 1 else 1)
                  chunk_files.append(fname)

              # If only 1 chunk, rename to no suffix
              if len(chunk_files) == 1:
                  old = output_dir / chunk_files[0]
                  new = output_dir / f"{prefix}.md"
                  if old != new:
                      old.rename(new)
                  chunk_files = [f"{prefix}.md"]

              return chunk_files

          # â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

          episodes_dir = Path('episodes')
          output_dir = Path('byl-projects')
          output_dir.mkdir(exist_ok=True)

          # Clean old files
          for f in output_dir.glob('*.md'):
              f.unlink()

          all_transcripts = []
          episode_dirs = sorted([d for d in episodes_dir.iterdir() if d.is_dir()])
          print(f"Found {len(episode_dirs)} episode directories\n")

          for ep_dir in episode_dirs:
              transcript_file = ep_dir / 'transcript.md'
              if not transcript_file.exists():
                  continue
              meta, body = parse_transcript(transcript_file)
              tags = get_tags_from_transcript(meta, body, ep_dir.name)
              projects = get_projects_for_tags(tags)
              if not projects:
                  projects = {2}
                  print(f"  No match for '{ep_dir.name}' -> Project 2 (default)")
              all_transcripts.append({
                  'path': transcript_file,
                  'dir': ep_dir.name,
                  'meta': meta,
                  'body': body,
                  'tags': tags,
                  'projects': projects,
              })

          print(f"Classified {len(all_transcripts)} transcripts\n")

          timestamp = datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M UTC')
          index_dir = Path('index')
          all_output_files = []

          for proj_num, proj_name in PROJECT_NAMES.items():
              prefix = PROJECT_FILE_PREFIX[proj_num]
              relevant = [t for t in all_transcripts if proj_num in t['projects']]

              print(f"Project {proj_num:02d}: {proj_name} ({len(relevant)} episodes)")

              # Build topic index header block
              proj_topics = [k for k, v in TOPIC_TO_PROJECT.items() if v == proj_num]
              multi_topics = [k for k, vl in MULTI_PROJECT_TOPICS.items() if proj_num in vl]
              all_proj_topics = sorted(set(proj_topics + multi_topics))

              header_block = f"## TOPIC INDEX\n\n"
              header_block += f"_This project covers {len(relevant)} episodes across {len(all_proj_topics)} topics._\n\n"
              for topic in all_proj_topics:
                  index_file = index_dir / f"{topic}.md"
                  if index_file.exists():
                      header_block += f"### {topic.replace('-', ' ').title()}\n\n"
                      header_block += index_file.read_text(encoding='utf-8', errors='replace')
                      header_block += "\n\n---\n\n"

              # Build transcript blocks
              transcript_blocks = [build_transcript_block(t) for t in relevant]

              # Write with chunking
              files = write_chunks(
                  output_dir, prefix, proj_name,
                  timestamp, header_block, transcript_blocks
              )
              all_output_files.extend(files)

          print(f"\nâœ… Done. Files written to byl-projects/:")
          for f in sorted(output_dir.glob('*.md')):
              size_mb = f.stat().st_size / (1024 * 1024)
              print(f"  {f.name}: {size_mb:.2f} MB")

          PYTHON_SCRIPT

          python classify_and_generate.py

      - name: Commit and push project files
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add byl-projects/
          git diff --staged --quiet || git commit -m "chore: update byl-projects/ with 200k token cap [auto]"
          git push

      - name: Notify via Slack
        run: |
          curl -X POST "${{ secrets.SLACK_WEBHOOK_URL }}" \
            -H 'Content-type: application/json' \
            --data "{
              \"text\": \"ðŸ“š *BYL Project Files Updated*\nAll project brain files regenerated (200k token cap, auto-split where needed).\nðŸ‘‰ https://github.com/${{ github.repository }}/tree/main/byl-projects\"
            }"
