name: Generate BYL Project Files with Full Transcripts

on:
  push:
    paths:
      - 'index/**'
      - 'episodes/**'
  workflow_dispatch:

jobs:
  generate:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install pyyaml

      - name: Classify and generate project files
        run: |
          cat << 'PYTHON_SCRIPT' > classify_and_generate.py
          import os
          import yaml
          import re
          from pathlib import Path
          from datetime import datetime, timezone

          # â”€â”€ Constants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          MAX_TOTAL_LINES_PER_PROJECT = 60_000   # Hard cap: total lines across ALL parts
          MAX_LINES_PER_FILE = 60_000            # Each individual part also capped at 60k

          # â”€â”€ Topic â†’ Project mapping â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          TOPIC_TO_PROJECT = {
            "product-strategy":        1, "product-market-fit":      1, "innovation":              1,
            "business-strategy":       1, "strategy":                1, "founder-mode":            1,
            "entrepreneurship":        1, "bootstrapping":           1, "network-effects":         1,
            "marketplaces":            1, "airbnb":                  1, "stripe":                  1,

            "product-management":      2, "product-development":     2, "product-leadership":      2,
            "prioritization":          2, "okrs":                    2, "decision-making":         2,
            "experimentation":         2, "ab-testing":              2, "agile":                   2,
            "focus":                   2, "google":                  2, "microsoft":               2,

            "growth-strategy":         3, "startup-growth":          3, "product-led-growth":      3,
            "retention":               3, "sales":                   3, "enterprise-sales":        3,
            "marketing":               3, "brand-building":          3, "branding":                3,
            "word-of-mouth":           3, "community-building":      3, "uber":                    3,
            "linkedin":                3,

            "ai":                      4, "machine-learning":        4, "chatgpt":                 4,
            "openai":                  4,

            "leadership":              5, "management":              5, "organizational-design":   5,
            "executive-coaching":      5, "company-culture":         5, "startup-culture":         5,
            "feedback":                5, "influence":               5, "power":                   5,
            "communication":           5, "storytelling":            5, "slack":                   5,

            "hiring":                  6, "recruiting":              6, "team-building":           6,
            "mentorship":              6, "executive-search":        6, "remote-work":             6,
            "work-life-balance":       6, "facebook":                6, "meta":                    6,

            "engineering":             7, "productivity":            7, "time-management":         7,
            "design":                  7, "user-experience":         7, "customer-experience":     7,
            "customer-research":       7,

            "career-development":      8, "career-growth":           8, "personal-development":    8,
            "personal-branding":       8, "personal-transformation": 8, "skill-building":          8,
            "networking":              8, "anxiety-management":      8, "mental-health":           8,
            "stress-management":       8, "psychology":              8, "neuroscience":            8,

            "analytics":               9, "data-analytics":          9,

            "venture-capital":        10, "media-relations":        10,
          }

          MULTI_PROJECT_TOPICS = {
            "experimentation":    [2, 4, 9],
            "ab-testing":         [2, 9],
            "machine-learning":   [4, 9],
            "okrs":               [2, 7],
            "agile":              [2, 7],
            "focus":              [2, 7],
            "slack":              [5, 7],
            "mentorship":         [6, 8],
            "entrepreneurship":   [1, 10],
            "business-strategy":  [1, 10],
            "bootstrapping":      [1, 10],
            "enterprise-sales":   [3, 10],
          }

          PROJECT_NAMES = {
            1:  "Product Strategy & Vision",
            2:  "Product Management & Development",
            3:  "Growth & Revenue",
            4:  "AI & Machine Learning",
            5:  "Leadership & Management",
            6:  "Hiring, Teams & People",
            7:  "Workflow, Productivity & Engineering",
            8:  "Career & Personal Growth",
            9:  "Data, Analytics & Experimentation",
            10: "Venture, Finance & Strategy",
          }

          PROJECT_FILE_PREFIX = {
            1:  "01-product-strategy-vision",
            2:  "02-product-management-development",
            3:  "03-growth-revenue",
            4:  "04-ai-machine-learning",
            5:  "05-leadership-management",
            6:  "06-hiring-teams-people",
            7:  "07-workflow-productivity-engineering",
            8:  "08-career-personal-growth",
            9:  "09-data-analytics-experimentation",
            10: "10-venture-finance-strategy",
          }

          # â”€â”€ Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

          def count_lines(text):
              return text.count('\n') + 1

          def normalize_tag(tag):
              return re.sub(r'[^a-z0-9]+', '-', tag.lower()).strip('-')

          def get_projects_for_tags(tags):
              projects = set()
              for tag in tags:
                  norm = normalize_tag(tag)
                  if norm in MULTI_PROJECT_TOPICS:
                      projects.update(MULTI_PROJECT_TOPICS[norm])
                  elif norm in TOPIC_TO_PROJECT:
                      projects.add(TOPIC_TO_PROJECT[norm])
              return projects

          def parse_transcript(filepath):
              try:
                  content = filepath.read_text(encoding='utf-8', errors='replace')
                  parts = content.split('---', 2)
                  if len(parts) >= 3:
                      try:
                          meta = yaml.safe_load(parts[1]) or {}
                      except:
                          meta = {}
                      body = parts[2].strip()
                  else:
                      meta = {}
                      body = content.strip()
                  return meta, body
              except Exception as e:
                  print(f"  WARNING: Could not parse {filepath}: {e}")
                  return {}, ""

          def get_tags_from_transcript(meta, body, guest_folder):
              tags = []
              for field in ['tags', 'keywords', 'topics', 'categories']:
                  val = meta.get(field)
                  if isinstance(val, list):
                      tags.extend([str(t) for t in val])
                  elif isinstance(val, str):
                      tags.extend([t.strip() for t in val.split(',')])
              folder_norm = normalize_tag(guest_folder)
              if folder_norm in TOPIC_TO_PROJECT:
                  tags.append(folder_norm)
              if not tags:
                  text_to_scan = (
                      str(meta.get('title', '')) + ' ' +
                      str(meta.get('description', '')) + ' ' +
                      body[:3000]
                  ).lower()
                  for topic in TOPIC_TO_PROJECT.keys():
                      if topic.replace('-', ' ') in text_to_scan:
                          tags.append(topic)
              return tags

          def build_transcript_block(t):
              meta  = t['meta']
              guest = meta.get('guest', t['dir'])
              title = meta.get('title', f"Episode: {t['dir']}")
              date  = meta.get('publish_date', 'Unknown')
              url   = meta.get('youtube_url', '')
              tags  = t['tags']
              block  = f"## {title}\n"
              block += f"**Guest:** {guest}  \n"
              block += f"**Published:** {date}  \n"
              if url:
                  block += f"**YouTube:** {url}  \n"
              if tags:
                  block += f"**Tags:** {', '.join(tags[:10])}  \n"
              block += "\n"
              block += t['body']
              block += "\n\n---\n\n"
              return block

          def build_header_block(proj_num, proj_name, timestamp, relevant_count, all_proj_topics, index_dir):
              header  = f"# BYL Brain: {proj_name}\n"
              header += f"_Auto-generated from Lenny's Podcast Transcripts Archive_\n"
              header += f"_Last updated: {timestamp}_\n"
              header += f"_Total episodes in this project: {relevant_count}_\n"
              header += f"_Hard cap: 60,000 lines total across all parts_\n\n"
              header += "---\n\n"
              header += "## TOPIC INDEX\n\n"
              for topic in all_proj_topics:
                  index_file = index_dir / f"{topic}.md"
                  if index_file.exists():
                      header += f"### {topic.replace('-', ' ').title()}\n\n"
                      header += index_file.read_text(encoding='utf-8', errors='replace')
                      header += "\n\n---\n\n"
              header += "## FULL TRANSCRIPTS\n\n---\n\n"
              return header

          def write_project(output_dir, prefix, proj_name, timestamp,
                            header_block, transcript_blocks):
              """
              Write project content into part files.
              RULE: sum of lines across ALL part files <= MAX_TOTAL_LINES_PER_PROJECT.
              Each individual file also <= MAX_LINES_PER_FILE.
              Transcripts are included in priority order until the budget is exhausted.
              """
              header_lines = count_lines(header_block)

              # Calculate total line budget for transcript content
              transcript_budget = MAX_TOTAL_LINES_PER_PROJECT - header_lines
              if transcript_budget <= 0:
                  print(f"  WARNING: Header alone exceeds 60k lines for {proj_name}. Writing header only.")
                  transcript_budget = 0

              # Select transcripts that fit within the budget
              selected_blocks = []
              used_lines = 0
              skipped = 0
              for block in transcript_blocks:
                  block_lines = count_lines(block)
                  if used_lines + block_lines <= transcript_budget:
                      selected_blocks.append(block)
                      used_lines += block_lines
                  else:
                      skipped += 1

              if skipped:
                  print(f"  NOTE: {skipped} episodes trimmed to stay within 60k line cap")

              total_content_lines = header_lines + used_lines
              print(f"  Total lines: {total_content_lines:,} / {MAX_TOTAL_LINES_PER_PROJECT:,} "
                    f"({len(selected_blocks)} episodes, {skipped} trimmed)")

              # Now split into part files, each <= MAX_LINES_PER_FILE
              parts = []
              current_lines = 0
              current_blocks = []

              for block in selected_blocks:
                  block_lines = count_lines(block)
                  # Check if adding this block exceeds per-file limit
                  if current_lines + block_lines > MAX_LINES_PER_FILE and current_blocks:
                      parts.append(current_blocks)
                      current_blocks = []
                      current_lines = 0
                  current_blocks.append(block)
                  current_lines += block_lines

              if current_blocks:
                  parts.append(current_blocks)

              if not parts:
                  parts = [[]]  # Write at least one file even if empty

              total_parts = len(parts)
              files_written = []

              for i, part_blocks in enumerate(parts, 1):
                  if total_parts == 1:
                      filename = f"{prefix}.md"
                  else:
                      filename = f"{prefix}-part{i}.md"

                  filepath = output_dir / filename

                  with open(filepath, 'w', encoding='utf-8') as f:
                      # Header goes in every part for standalone usability
                      if i == 1:
                          f.write(header_block)
                      else:
                          # Minimal header for continuation parts
                          f.write(f"# BYL Brain: {proj_name} (Part {i} of {total_parts})\n")
                          f.write(f"_Continuation â€” see Part 1 for topic index_\n")
                          f.write(f"_Last updated: {timestamp}_\n\n---\n\n")
                          f.write("## FULL TRANSCRIPTS (continued)\n\n---\n\n")

                      for block in part_blocks:
                          f.write(block)

                  file_lines = count_lines(filepath.read_text(encoding='utf-8', errors='replace'))
                  size_kb = filepath.stat().st_size / 1024
                  print(f"    -> {filename}: {file_lines:,} lines | {size_kb:.0f} KB")
                  files_written.append(filename)

              return files_written

          # â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

          episodes_dir = Path('episodes')
          output_dir   = Path('byl-projects')
          index_dir    = Path('index')
          output_dir.mkdir(exist_ok=True)

          # Clean old generated files
          for f in output_dir.glob('*.md'):
              f.unlink()

          # Parse and classify all transcripts
          all_transcripts = []
          episode_dirs = sorted([d for d in episodes_dir.iterdir() if d.is_dir()])
          print(f"Found {len(episode_dirs)} episode directories\n")

          for ep_dir in episode_dirs:
              transcript_file = ep_dir / 'transcript.md'
              if not transcript_file.exists():
                  continue
              meta, body = parse_transcript(transcript_file)
              tags = get_tags_from_transcript(meta, body, ep_dir.name)
              projects = get_projects_for_tags(tags)
              if not projects:
                  projects = {2}
              all_transcripts.append({
                  'path':     transcript_file,
                  'dir':      ep_dir.name,
                  'meta':     meta,
                  'body':     body,
                  'tags':     tags,
                  'projects': projects,
              })

          print(f"Classified {len(all_transcripts)} transcripts\n")
          timestamp = datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M UTC')

          # Generate each project
          for proj_num, proj_name in PROJECT_NAMES.items():
              prefix   = PROJECT_FILE_PREFIX[proj_num]
              relevant = [t for t in all_transcripts if proj_num in t['projects']]

              print(f"\nProject {proj_num:02d}: {proj_name} ({len(relevant)} episodes)")

              # Build topic list for this project
              proj_topics  = [k for k, v  in TOPIC_TO_PROJECT.items()    if v  == proj_num]
              multi_topics = [k for k, vl in MULTI_PROJECT_TOPICS.items() if proj_num in vl]
              all_proj_topics = sorted(set(proj_topics + multi_topics))

              header_block     = build_header_block(proj_num, proj_name, timestamp,
                                                    len(relevant), all_proj_topics, index_dir)
              transcript_blocks = [build_transcript_block(t) for t in relevant]

              write_project(output_dir, prefix, proj_name, timestamp,
                            header_block, transcript_blocks)

          # Final summary
          print(f"\n{'='*60}")
          print(f"FINAL OUTPUT â€” byl-projects/")
          print(f"{'='*60}")
          total_lines_all = 0
          for f in sorted(output_dir.glob('*.md')):
              lines = count_lines(f.read_text(encoding='utf-8', errors='replace'))
              size_kb = f.stat().st_size / 1024
              total_lines_all += lines
              status = "âœ…" if lines <= MAX_LINES_PER_FILE else "âŒ OVER LIMIT"
              print(f"  {status} {f.name}: {lines:,} lines | {size_kb:.0f} KB")
          print(f"\nTotal lines across all files: {total_lines_all:,}")

          PYTHON_SCRIPT

          python classify_and_generate.py

      - name: Commit and push project files
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add byl-projects/
          git diff --staged --quiet || git commit -m "chore: update byl-projects/ 60k line cap [auto]"
          git push

      - name: Notify via Slack
        run: |
          curl -X POST "${{ secrets.SLACK_WEBHOOK_URL }}" \
            -H 'Content-type: application/json' \
            --data "{
              \"text\": \"ðŸ“š *BYL Project Files Updated*\nAll 10 projects regenerated. Hard cap: 60,000 lines total per project.\nðŸ‘‰ https://github.com/${{ github.repository }}/tree/main/byl-projects\"
            }"
