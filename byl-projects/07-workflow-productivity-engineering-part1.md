# BYL Brain: Workflow, Productivity & Engineering
_Lenny's Podcast — 153 of 153 episodes included_
_Last updated: 2026-02-23 01:10 UTC | Claude.ai optimised: 4,500 lines/file, 8,000 lines total_

---

## TOPIC INDEX

**Agile**

# agile

Episodes discussing **agile**:

- [Melissa Perri](../episodes/melissa-perri/transcript.md)


**Customer Experience**

# customer experience

Episodes discussing **customer experience**:

- [April Dunford](../episodes/april-dunford/transcript.md)
- [Nilan Peiris](../episodes/nilan-peiris/transcript.md)


**Customer Research**

# customer research

Episodes discussing **customer research**:

- [Bob Moesta](../episodes/bob-moesta/transcript.md)
- [Gia Laudi](../episodes/gia-laudi/transcript.md)
- [Krithika Shankarraman](../episodes/krithika-shankarraman/transcript.md)
- [Lane Shackleton](../episodes/lane-shackleton/transcript.md)
- [Melissa Perri + Denise Tilles](../episodes/melissa-perri-denise-tilles/transcript.md)
- [Patrick Campbell](../episodes/patrick-campbell/transcript.md)
- [Shaun Clowes](../episodes/shaun-clowes/transcript.md)


**Design**

# design

Episodes discussing **design**:

- [Jessica Hische](../episodes/jessica-hische/transcript.md)


**Engineering**

# engineering

Episodes discussing **engineering**:

- [Alexander Embiricos](../episodes/alexander-embiricos/transcript.md)
- [Nicole Forsgren](../episodes/nicole-forsgren/transcript.md)
- [Scott Wu](../episodes/scott-wu/transcript.md)


**Focus**

# focus

Episodes discussing **focus**:

- [Jake Knapp + John Zeratsky](../episodes/jake-knapp-john-zeratsky/transcript.md)
- [Nir Eyal](../episodes/nir-eyal/transcript.md)


**Okrs**

# OKRs

Episodes discussing **OKRs**:

- [Annie Pearl](../episodes/annie-pearl/transcript.md)
- [Christina Wodtke](../episodes/christina-wodtke/transcript.md)
- [Itamar Gilad](../episodes/itamar-gilad/transcript.md)
- [Lane Shackleton](../episodes/lane-shackleton/transcript.md)
- [Matt LeMay](../episodes/matt-lemay/transcript.md)
- [Nickey Skarstad](../episodes/nickey-skarstad/transcript.md)
- [Ravi Mehta](../episodes/ravi-mehta/transcript.md)
- [Ray Cao](../episodes/ray-cao/transcript.md)
- [Varun Parmar](../episodes/varun-parmar/transcript.md)
- [Yamashata](../episodes/yamashata/transcript.md)


**Productivity**

# productivity

Episodes discussing **productivity**:

- [Jake Knapp + John Zeratsky](../episodes/jake-knapp-john-zeratsky/transcript.md)
- [Jessica Hische](../episodes/jessica-hische/transcript.md)
- [Jonny Miller](../episodes/jonny-miller/transcript.md)
- [Nicole Forsgren](../episodes/nicole-forsgren/transcript.md)
- [Nir Eyal](../episodes/nir-eyal/transcript.md)


**Slack**

# Slack

Episodes discussing **Slack**:

- [Annie Duke](../episodes/annie-duke/transcript.md)
- [Fareed Mosavat](../episodes/fareed-mosavat/transcript.md)
- [Interview Q Compilation](../episodes/interview-q-compilation/transcript.md)
- [Jules Walter](../episodes/jules-walter/transcript.md)
- [Kenneth Berger](../episodes/kenneth-berger/transcript.md)
- [Merci Grace](../episodes/merci-grace/transcript.md)
- [Tamar Yehoshua](../episodes/tamar-yehoshua/transcript.md)
- [Tanguy Crusson](../episodes/tanguy-crusson/transcript.md)


**Time Management**

# time management

Episodes discussing **time management**:

- [Jake Knapp + John Zeratsky](../episodes/jake-knapp-john-zeratsky/transcript.md)
- [Nir Eyal](../episodes/nir-eyal/transcript.md)


**User Experience**

# user experience

Episodes discussing **user experience**:

- [Anuj Rathi](../episodes/anuj-rathi/transcript.md)
- [Bob Baxley](../episodes/bob-baxley/transcript.md)
- [Dmitry Zlokazov](../episodes/dmitry-zlokazov/transcript.md)
- [Gustav Söderström](../episodes/gustav-söderström/transcript.md)
- [Karri Saarinen](../episodes/karri-saarinen/transcript.md)
- [Katie Dill](../episodes/katie-dill/transcript.md)
- [Krithika Shankarraman](../episodes/krithika-shankarraman/transcript.md)
- [Merci Grace](../episodes/merci-grace/transcript.md)


---

## TRANSCRIPTS

### Al Engineering 101 with Chip Huyen (Nvidia, Stanford, Netflix)
**Guest:** Chip Huyen | **Date:** 2025-10-23 | [YouTube](https://www.youtube.com/watch?v=qbvY0dQgSJ4)  

# OpenAI researcher on why soft skills are the future of work | Karina Nguyen

## Transcript

Chip Huyen (00:00:00):
A question that get asked a lot and a lot is, "How do we keep up to date with the latest AI news?" Why do you need to keep up to date with the latest AI news? If you talk to the users who understand what they want or they don't want, look into the feedback, then you can actually improve the application way, way, way more.

Lenny Rachitsky (00:00:15):
A lot of companies are building AI products. A lot of companies are not having a good time building AI products.

Chip Huyen (00:00:19):
We are in an ideal crisis. Now, we have all this really cool tools to do everything from scratch and have new design. It can have you write code. You can have new website. So in theory, we should see a lot more, but at the same time, people are somehow stuck. They don't know what to build.

Lenny Rachitsky (00:00:33):
All this AI hype, the data is actually showing most companies try it, doesn't do a lot. They stop. What do you think is the gap here?

Chip Huyen (00:00:38):
It's really hard to measure productivity. So, I do ask people to ask their managers, "Would you rather give everyone on the team very expensive coding agent subscriptions or you get an extra head count?" Almost every one, the managers will say head count. But if you ask VP level or someone who manage a lot of teams, they would say, "Want AI assistant." Because as managers, you are still growing, so for you having one HR head count is big. Whereas for executives, maybe you have more business metrics that you care about. So you actually think about what actually drive productivity metrics for you.

Lenny Rachitsky (00:01:11):
Today, my guest is Chip Huyen. Unlike a lot of people who share insights into building great AI products and where things are heading, Chip has built multiple successful AI products, platforms, tools. Chip was a core developer on NVIDIA's NeMo platform, an AI researcher at Netflix. She taught machine learning at Stanford. She's also a two-time founder and the author of two of the most popular books in the world of AI, including her most recent book called AI Engineering, which has been the most read book on the O'Reilly platform since its launch.

(00:01:41):
She's also gotten to work with a lot of enterprises on their AI strategies, and so she gets to see what's actually happening on the ground inside a lot of different companies. In our conversation, Chip explains a lot of the basics like, what exactly does pre-training and post-training look like? What is RAG? What is reinforcement learning? What is RLHF? We also get into everything she's learned about how to build great AI products, including what people think it takes and what it actually takes. We talk about the most common pitfalls that companies run into, where she's seeing the most productivity gains and so much more.

(00:02:12):
This episode is quite technical, more technical than most conversations I've had, and is meant for anyone looking for a more in-depth conversation about AI. If you enjoy this podcast, don't forget to subscribe and follow it in your favorite podcasting app or YouTube. And if you become an annual subscriber of my newsletter, you get a year free of 16 incredible products, including Devin, Lovable, Replit, Bolt, n8n, Linear, Superhuman, Descript, Wispr Flow, Gamma, Perplexity, Warp, Granola, Magic Patterns, Raycast, ChatPRD, and Mobbin. Head on over to lennysnewsletter.com and click product pass. With that, I bring you Chip Huyen after a short word from our sponsors.

(00:02:47):
This episode is brought to you by Dscout. Design teams today are expected to move fast but also to get it right. That's where Dscout comes in. Dscout is the all-in-one research platform built for modern product and design teams. Whether you're running usability tests, interviews, surveys, or in the wild field work, Dscout makes it easy to connect with real users and get real insights fast. You can even test your Figma prototypes directly inside the platform. No juggling tools, no chasing ghost participants, and with the industry's most trusted panel, plus AI-powered analysis, your team gets clarity and confidence to build better without slowing down.So if you're ready to streamline your research, speed of decisions, and design with impact, head to dscout.com to learn more. That's D-S-C-O-U-T.com, the answers you need to move confidently.

(00:03:39):
Did you know that I have a whole team that helps me with my podcast and with my newsletter? I want everyone on my team to be super happy and thrive in their roles. Justworks knows that your employees are more than just your employees. They're your people. My team is spread out across Colorado, Australia, Nepal, West Africa, and San Francisco. My life would be so incredibly complicated to hire people internationally, to pay people on time and in their local currencies, and to answer their HR questions 24/7. But with Justworks, it's super easy. Whether you're setting up your own automated payroll, offering premium benefits or hiring internationally, Justworks offers simple software and 24/7 human support from small business experts for you and your people. They do your human resources right so that you can do right by your people. Justworks, for your people. Chip, thank you so much for being here, and welcome to the podcast.

Chip Huyen (00:04:34):
Hi, Lenny. I've been a big fan of the podcast for a while, so I'm really excited to be here. Thank you for having me.

Lenny Rachitsky (00:04:40):
I want to start with this table/chart that you shared on LinkedIn a while ago that went super viral, and I think it went super viral because it hit a nerve with a lot of people. Let me just read this and we'll show this on YouTube for people that are watching. So it's this very simple table you shared of what people think will improve AI apps and what actually improves AI apps. What people think will improve AI apps, staying up to date with the latest AI news, adopting the newest agentic framework, agonizing about what vector databases to use, constantly evaluating what model is smarter, fine-tuning a model. And then you have what actually improves AI apps, talking to users, building more reliable platforms, preparing better data, optimizing end-to-end workflows, writing better prompts. Why do you think this hit such a nerve with people? If you had to boil it down, what do you think people are missing about building successful AI apps?

Chip Huyen (00:05:30):
[inaudible 00:05:30] question that get asked a lot and a lot is that, "How do we keep up to date with the latest AI news?" I'm like, "Why? Why do you need to keep up to date with the latest AI news?" I know it sound very counter-intuitive, but there's just so much news out there. A lot of people also ask me questions like, "How do I choose between two different technologies?" Maybe like recently, MCP versus agent-to-agent protocol? And it was like, "Which one is better or this or that?" I think it's a [inaudible 00:05:59] question you should ask them is like, "First, how much of the improvement could you get from optimal solutions versus non-optimal solutions?" Right? And sometimes they were like, "Actually, it's not much." Right?

(00:06:10):
I was like, "Okay, if it's not much improvement, then why do you want to spend so much time debating something that doesn't make that much difference to your performance?" Another question they ask is like, "If you adopted a new technology, how hard it could be to switch that out to another?" And sometimes they will like, "Oh, I think it could be a lot of work switching it out." And I'm just like, "Hmm, let's say here's a new technology. It hasn't been tested by a lot of people, and if you would adopt it, you would be stuck with it forever. Do you actually want to adopt it?" Maybe you want to think twice about over commit to new technologies that hasn't been better tested.

Lenny Rachitsky (00:06:49):
I love your just broader advice is just simple like, to build successful AI apps, talk to users, build better data, write better prompts, optimize the user experience, versus just like, what is the latest and greatest? What's the best model to use right now? What's happening in AI? Let me follow this thread of this idea of fine-tuning and basically post-training. There's all these terms that people hear in AI, and I think this is going to be a really good opportunity for people to learn what we're actually talking about, since you actually do these things, you build these things, you work with companies doing these things. There's a few terms I want to sprinkle in through the conversation, but let's start with this one. What's the simplest way for someone to understand? What is the difference between pre-training and post-training and then just how fine-tuning fits into that, just what fine-tuning actually is?

Chip Huyen (00:07:34):
Chip disclaimer, I don't have full visibility on what this big secretive frontier labs are doing. But right from what I heard, so I think it's like one is, supervised fine-tuning when you have demonstration data, and you have a bunch of experts, "Okay, here's a prompt, and here is what the answer should be like." You just train it to emulate what the human expert could be like. That's also what a lot of people would like, so open-source models are doing as they do it by distillation. So instead of having human experts to write really great answers to prompts, they get very popular, famous good models to generate a response to it and getting this train smaller models to emulate.

(00:08:23):
Sometimes you see people just like... So, that's because I really appreciate open source community by the way, but going from being able to train models that can emulate a existing good model. It's very different from being [inaudible 00:08:38] trained good models, like an output for existing good model. So, it's a big step there. Yeah, we have my supervised fine-tuning, and another thing that's very big, I'm not sure you have guests talking about it already, but reinforcement learning is everywhere.

Lenny Rachitsky (00:08:52):
Let's pause on that because I definitely want to spend time on that, and that's such cool topic that's merging more and more in my conversations. But just to even summarize the things you just shared, which I think is really, really important stuff. So, the idea here is a model, essentially this algorithm piece of code that someone writes and say the frontier models are feeding it just like the entire internet of content, and basically, it's trying to test itself on predicting across all that data the next word, essentially. Token is the correct way of thinking about it, but a simpler way to think about it is the next word in text. As it gets it wrong, it adjusts these things called weights, essentially. Just like, is that a simple way to think about it, even that's just very surface level?

Chip Huyen (00:09:35):
So, I think of language modeling as a way of encoding statistical information about language, right? So, let's say that we both speak English, so we get a sense of what is more statistically likely. If I say my favorite color is, then you would say, "Okay, that should be another color." The word blue would be much more likely to appear than the word like [inaudible 00:09:59], right? Because statistically, blue is more likely to [inaudible 00:10:02] my favorite color is. So, it's a way of encoding statistical information.

(00:10:07):
So when language modeling, when you train a large amount of data, you see a lot of languages, a lot of domains. So it can tell, okay, your basic size is standard. Then the user do the prompts and it could come with the next most likely token. So by the way, it's not a new idea actually. So it's the idea comes very, very old, from the 1951 papers like English entropy. I think it's by Claude Shannon, it's a great paper. And I think it reveals a story I really like is from... Did you read Sherlock Holmes by the way?

Lenny Rachitsky (00:10:39):
Yeah, I read a few Sherlock Holmes books. Yeah.

Chip Huyen (00:10:41):
Yeah. So this is story of when Sherlock Holmes says using this statistical information to help solve a case. So this is his story. There is somebody left a message with a lot of stick figures. So Sherlock Holmes was like, okay, he knows that in English, the most common letter is E. Then the most common stick figure must be E. And then he goes, he stopped like that, [inaudible 00:11:07]. So the code... So I think there's language. So in a way, it's simple language modeling, but instead of at a work level, he does this as character level and token is something in between, right? A token is not quite a word, but it's bigger than a character. So let's say we say token because it would help us reduce vocabulary because which character is smallest amount of vocabulary right now. So alphabet has 26 character, but words can have millions and millions, right? Whereas tokens, you can be able to get the sweet spot between the two.

(00:11:44):
So let's say that we have the new word, how to say it, like podcasting, right? Let's say it's a new word, but it can divide a podcast and ing. So people understand, okay, podcast, we know the meaning. We know that ing is a verb, gerund, whatever it is. So we even know the word podcasting so that's why the token comes in. But yeah, the pre-tuning is basically encoding statistical informations of language to have you predict what is most likely. I think that most likely is the most simple way of doing it because it's more building a distribution of, okay, so the next token could be more 90% of the the time it could be a color, 10% of the time could be something else. So it basically distribution so language could pick, depending on your sampling strategy. Do you want it to always pick the most likely token or do you want it to pick something more creative? So I think my sampling strategy, I think is something extremely important. It can have you boost a performance in a huge way and very, very underrated.

Lenny Rachitsky (00:12:49):
Okay, awesome. So essentially, a model is just code with this whole set of weights, essentially the statistical model that has learned to predict what comes next after certain words and phrases?

Chip Huyen (00:13:03):
Yeah.

Lenny Rachitsky (00:13:03):
And then post-training and fine-tuning, specifically, is doing that same thing. So pre-training you get GPT5. Fine-tuning is someone taking GPT5 and doing the same sort of thing, adjusting these weights a little bit for specific use cases on data that they find is necessary to do their very specific use case. Is that a simple way to think about it?

Chip Huyen (00:13:24):
Yeah, I think weights is functions, right? So let's say you have... Maybe it has a functions of maybe Lenny's height is maybe 1X plus something or 2X [inaudible 00:13:38] and plus something is the weights, right? So you change it until you fit the correct data, which is my height and your height. So you can think it's a weight, as just a weight, say function. So you train, adjust the weights so they can fit the data, which is the training data.

Lenny Rachitsky (00:13:54):
Awesome. Okay. So we're talking about pre-training, post-training, fine-tuning. Is there anything else here that's important to share about just what this is exactly? What people need to understand about these parts of training?

Chip Huyen (00:14:06):
So the vast majority of time, we don't touch on pre-training model. As users, we don't use it at all.

Lenny Rachitsky (00:14:12):
Right. It's already done for us.

Chip Huyen (00:14:13):
Yeah. So I think my [inaudible 00:14:15] is a bit of fun process when my friend's training model is they try to play with their pre-training model and they're horrendous. They're saying things like [inaudible 00:14:23] "Oh, my gosh." Yeah, it's crazy. So it's very interesting to look at how much of post-training can change the model behavior and I think that's where a lot of time, is a lot of people are spending energy on nowadays, their frontier lab, is on post-training. Because pre-training, I think... So pre-training have been used to increase the general capacity of capabilities of a model. And it needs a lot of data and model size to increase the model capabilities. And at some point, we are actually have kind of maxed out on the internet data. And people text data max out. I think a lot of people are doing with other data like audios and videos, and everyone's trying to think of what is the new source of data, but where like post-trading, but middle course of this is more of everyone have very similar pre-training data, is that post-training is where they make a big difference nowadays.

Lenny Rachitsky (00:15:21):
This is a good segue to, you talked about supervised learning versus unsupervised learning. I love, we're getting into this, by the way. This is super interesting. So you talk about labeled data. Basically, supervised learning is AI learning on data that somebody has already labeled and told it, here's correct versus incorrect. For example, this is spam versus not spam. This is a good short story. This is not a good short story. We've had the CEOs of a lot of these companies that do this for labs, Mercor and Scale, Handshake, there's Micro, there's a few others. So is that essentially what these companies are doing for labs, giving them labeled data, high-quality data to train on?

Chip Huyen (00:15:57):
It is in a way, but I think it's more like a product of big equations. So there are a lot more different components than that. So that's why I was talking about reinforcement learning. I'm not sure if your CEO [inaudible 00:16:09] interview bring up that term. So the idea is that once you [inaudible 00:16:14]... So let's say you have a model, give the model a prompt and it produce an output. You want to buy, once you reinforce, encourage the model to produce an output that is better. So now it comes to how do we know that the answer is good or bad? So usually, people relies on signals. So one way to get a first one good or bad is human feedback. They happen to be have two responses. You can, okay, this one one's better than the other. And we do that is because as humans, we tend to, it's very hard to give a concrete score, but it's easier to do comparisons.

(00:16:53):
If you ask me, okay, give this song a score, I'm not a musician and don't know how hard it is. It's like yeah, I don't know what, out of 10 I going to remove six. And if you ask me again a month from now and I completely forgotten, okay, maybe now seven, only four, I don't know. But then if you ask me, okay, here are two songs and which one would you prefer to play for the birthday party? I was like, "Okay, I can prefer this song." So comparisons a lot easier. So [inaudible 00:17:18] have a human, you have human feedback and then you use this human feedback to treat a reward model to tell which and then the reward model help you like, okay, it's a model that produce this response.

(00:17:28):
It's [inaudible 00:17:30] can score, is this good or bad? And you try to bias toward producing better model, the better responses. Another ways you can, instead of using a human, so you can use AI because the response and say good or bad, right? Or in fact the thing is that people are very big on nowadays, verifiable rewards, which it's natural. So basically, they give it a math problem and then math solutions is a model app a solution. Okay, it's expected response should be 42 and if it doesn't provide 42, then it's wrong. Now it's not a good response. So yes, a lot of time, people using this human laborer, human laborers should produce, how to say, expert questions and I say expected answers and in the ways that [inaudible 00:18:16] systems that verifiable so that the models can be trained on. Yeah.

Lenny Rachitsky (00:18:19):
Okay, I'm really glad you went there. This is essentially RLHF reinforcement learning with human feedback, which is exactly what I wanted to also talk about, right?

Chip Huyen (00:18:29):
Yeah. So I think it's general, it's a way of learning. It's training is [inaudible 00:18:33] learning and whether it learn from human feedback or AI feedback or verifiable rewards, I think I say it's just different way of collating signals.

Lenny Rachitsky (00:18:44):
Awesome. Yeah. We had the CEO of Anthropic on the podcast and he talked about their version of RLHF, which is AI driven reinforcement learning. I love the way you phrased it where basically you want to help the model, you want to reinforce correct behavior and correct answers, and this is the method to do it, whether it's say an engineer seeing an output from a model being like, "No, here's how I would code it differently." And it's training a different model that the original model works with to tell it, am I correct or not correct? Is that right, roughly?

Chip Huyen (00:19:15):
Yeah.

Lenny Rachitsky (00:19:16):
Okay.

Chip Huyen (00:19:16):
I think that's a way of looking into it. I think that's a space is so exciting nowadays because there are so many domain expert task that the model developers want models to do well on, right? Let's say you're accountant. Maybe you want to use a model to have accounting task and need a lot of accounting data examples from accountant. So you need to hire a lot of them, should I do it or everyone [inaudible 00:19:41] physics problems, everyone should do, I don't know, legal questions and stuff or engineering questions or somebody was telling me they want to do, using coding to source scientific problems and not just coding to build product, which is another different whole realm of things. And I also using very specific toolings. I'm not sure what apps you use, but maybe like a [inaudible 00:20:04] app or QuickBooks or Google Excel. They have very specific tools, specific expertise. So you want the model [inaudible 00:20:13] learn.

(00:20:13):
So they need a lot of humans expert in this area should create data to train them and it's a massive thing people because everyone wants a lot of data and wants [inaudible 00:20:25] unlimited budget. But whether, I think this is also a little bit of low-key, interesting economics. I'm not sure you've talked to the guests about, I thought it's very interesting [inaudible 00:20:35] think about because it's very lopsided, right? Because they're only a very small numbers of frontier labs and they want a lot of data and there's a massive amount of startups or company providing related data. So you can see these companies like this startup doing data labeling. They have maybe some massive AR, but if you ask them, "Okay, so how many customers you have?" And they could be very small numbers, I'm not sure. I'm not sure you... I saw you smiling.

Lenny Rachitsky (00:21:03):
Yeah, yeah, yeah, we chatted about that.

Chip Huyen (00:21:05):
Yeah, so I'm a little bit like [inaudible 00:21:08] uneasy. I have a company's growing crazy, but it's heavily dependent on two or three companies. And at the same time, if I was this company, frontier labs, what could be the right economical things for me to do? Now I want a lot of startups. I want to have a lot of providers so I can pick and choose, and as this providers can also to compete each other to lower the price and it's so dependent on [inaudible 00:21:34] regardless. So I feel like, yeah, so this whole economics is very interesting to me and I'm curious to see and how it plays out.

Lenny Rachitsky (00:21:42):
What I'm hearing is you're bearish on the future of these data labeling companies because as you said, they don't have a lot of leverage over pricing because they have so few customers and there's so many people getting into the space. So basically, even though there's some of the fastest growing companies in the world, you're feeling like there's a challenge up ahead.

Chip Huyen (00:22:00):
I'm not sure if I'm bearish on it. I think I'm curious because I think things has a way of work out in ways that I don't expect. So I think that maybe these companies, they have a lot of data, maybe they wouldn't be able to use that to have some insight that helps them stay ahead of the curve. So I don't know.

Lenny Rachitsky (00:22:22):
A very fair answer. Okay, while we're on this topic, I want to chat about evals, which is a very recurring topic in this podcast. This is the other piece of data content these companies share that AI labs really need. Can you just talk about what an eval is, the simplest way to understand it and then how this helps models get smarter?

Chip Huyen (00:22:41):
So I think people approach eval, I think they're two very different problems. One is a app builder and can I say have an app that do maybe a chatbot? Very simple answer first thing that came to my mind and I want you to know if chatbot is good or bad. So it needs to come away with evaluate the chatbot. Another thing is, I think of this as a task-specific eval design. So let's say I'm a model developer and I want to make my model better at code writing. And it was like, "Okay, but how do I even measure code writing?"

(00:23:19):
So I need someone to understand code writing and think about what makes a story good and then design the whole dataset and then criteria to evaluate code writing. So yeah, I think there's that. I think it's more like eval design that is very interesting [inaudible 00:23:39] work criteria, [inaudible 00:23:42] work guidelines, how to do it and then also train people how to do it effectively. So I guess, [inaudible 00:23:49], I think eval is really, really fun because it's extremely creative. I was looking at different eval people built and it was like, "Wow." It's not dry at all. It's just super, super, super fun.

Lenny Rachitsky (00:24:01):
We had a whole podcast on evals with Hamel and Shreya. That's exactly what they talked about is just, it's actually really fun to create evals for companies, especially. So let's still dig into that one a little bit more. There's this kind of debate online that, I don't know how big of a deal this debate is, but it feels like people spend a lot of time thinking about this, this idea of, do we need evals for AI products? Some of the best companies say they don't really do evals, they just go on vibes. They're just like, "Is this working well? Can I feel it or not?" What's your take on just the importance of building evals and the skill of evals for AI apps, not the model companies?

Chip Huyen (00:24:39):
You don't have to be absolutely perfect, I think, to win. You just need to be good enough and being consistent about it. Okay, this is not a philosophy I follow, but I have worked with enough companies to see that play out. So when I say, why a company don't eval? Let's say you are an executive and you want to have a new use case. So here's a use case you started out, built and it's like it works well. The customers are somewhat happy. You don't have the exact metric for it.

(00:25:05):
So the traffic keeps increasing, people seem happy, people keep buying stuff and now here's our engineer coming like, "Okay, we need eval for it." And it was like, "Okay, how much effort do we need to go into eval?" And they were like, "Okay, maybe two engineers, this much, this much." And it could maybe would improve that and it was like, "Okay, so how much expected gain can I get from it?" And the engineer would be like, "Oh, maybe you can improve it from 80% to 82%, 85%."

(00:25:33):
And it was like, "Okay, but [inaudible 00:25:35] that two engineers and we going to launch a new feature, then it could give me so much more improvement." So I think it's one of them is eval. Sometimes people think of eval as like okay, this is good enough, just don't touch it. If you do spend a lot of energy on eval, it would only incremental improvement where it spends the energy on another use case and maybe [inaudible 00:25:55] good enough that you can vibe check it.

(00:25:57):
So I do think maybe that's a debate is about. I do think that a lot of time people just get things to the place where it's like, okay, good enough, people run. But in the end, but of course there's a lot of risk associated with it because if you don't have a clear metric, you have a good visibility to [inaudible 00:26:17] applications or models performing it might do something very dumb or it can cause you, I know something crazy can happen. So yeah, so I do think eval is very, very important if you have, if you operate a scale and where failures can have catastrophic consequences.

(00:26:38):
Then you do need to be very tyrannical about what you put in front of the users, understand different failure modes, what could go wrong and also maybe in a space when that it's a feature, the product is a competitive advantage. You want to be the best at it. So you want to have a very strong understanding of where you are and where you are with the competitors. But it's just something that's more a low-key, okay, this is like something is like, okay, that's not the core but it helps with our users.

(00:27:04):
Then maybe you don't need to be so obsessed or theoretical about it. It's like, okay, that's good enough for now and if it fails, then it fails. Okay, I know it's so terrifying. But yeah, I think it's all about the question of return investment. I'm a big fan of eval, I love reading eval. And I says, I understand why some people would choose to not focus on eval right away and choose bringing on new functionalities instead.

Lenny Rachitsky (00:27:32):
Awesome. That is a really pragmatic answer. What I'm hearing is evals are great, very important, especially if you're operating at scale, but pick your battles. You don't need to write evals for every little feature. Something that Hamel and Shreya shared is that people need just, I don't know, five or seven evals for the most important elements of their product. Is that what you see or do you see a lot more in production that people build and need?

Chip Huyen (00:27:54):
I don't think of just a fixed number on the evals. What was the goal of eval? The goal of eval is to guide the product development. So you see eval, because I think I'm a big fan of eval, is that it helps you uncover opportunities where the progress are doing well. So sometimes, we've seen a very obvious [inaudible 00:28:15] where you look at the eval and we realize it's like, okay, it performed really poorly on this specific segment of users and then we look into it's like, okay, what's wrong with it? And it turns out, it's like we just don't have a good messaging to it. So people should just focus on the things that we're doing poorly, can improve significantly. Yeah, so I kind of like the number of eval is really depends. We have seen product with hundreds of different metrics.

Lenny Rachitsky (00:28:40):
Oh, wow.

Chip Huyen (00:28:41):
People going crazy, this is because that product is general, have different names, have one eval for, I don't know, verbosity, have one eval for user sensitive data and another is for length but has a number of, okay, let's just give a good example, concrete example, like deep research. So you have the application, you have views and model to do deep research for you. Okay, have a prompt. Let me say, okay, do me a comprehensive research on only Lenny's Podcast and help me propose, show me report on what kind of topics he's interested in, what kind of videos could get the most views or what topics that he's missing on that he should be covering, right? Have that prompt. Then how do you evaluate the result? I don't think there's one metrics that would help. Maybe it's like maybe you have a hundred, I think somebody has a benchmark and is get a hundred expert, write a bunch of prompts and they go through, on the answers on AI and do it. And it's extremely costly and slow.

(00:29:46):
But [inaudible 00:29:47] might have something else. First of all, one way I was thinking about it, I was talking to a friend about it and one way it's like, how would you produce the result of the summary? At first you need to, what you do, gather informations and to gather informations you need to do a lot of search queries. You gather, grab the search results and then some of the search results you aggregate and then maybe say, okay, I'm still missing on this. You have to go another route and on another route, [inaudible 00:30:17] have the summary. So every step of the way, you need evaluations. You don't [inaudible 00:30:21] end-to-end. Maybe it was a search query in my first thing about, okay, now I write five search queries. I might look into how good are the search queries? Do they as they similar to each other because you need five search queries are very similar? Okay, Lenny Podcast, Lenny Podcast last month, Lenny Podcast two months ago.

(00:30:39):
It's not very exciting. But if the query is a podcast, the keywords are more diverse and then look at the results of the search query and then say you enter the search query. Lenny Podcast data labeling and they come up with 10 pages, 10 results. And then you come up with like, oh, Lenny Podcast on, I don't know, frontier labs, and you have 10 results. [inaudible 00:31:06] different webpages. Okay, how much of them overlapping... Are we doing both the breadth, getting a lot of page, but also, do we have depth and also do you have relevance because if we come up with a search query, it's completely irrelevant to the original prompt. So I feel like every aspect of it, it would need a way of evaluating. So I don't think it's how many eval should I get, but how many eval do I need to get a good coverage, a high confidence in my application's performance and also to help me understand where it is not performing well so that I can fix it.

Lenny Rachitsky (00:31:43):
Awesome. And I'm hearing also just especially for the very core use case, the most common path people take in your product is where you want to focus.

Chip Huyen (00:31:51):
Yeah, yeah.

Lenny Rachitsky (00:31:54):
Okay. There's one more term I want to cover and I want to go a somewhat different direction. RAG? People see this term a lot, R-A-G. What does it mean?

Chip Huyen (00:32:04):
So RAG stands for Retrieval-Augmented Generations [inaudible 00:32:08] not a specific true generative AI. So the idea is just for a lot of questions, we need context to answer. So I think it came pretty, I think it's from the paper 2017. So someone was like, so they realized it's for a bunch of benchmark. When the question answering benchmarks, they realized it's like, okay, if we give the model informations about the questions, the next answer can be much, much better. So what they do with that is try to retrieve information from Wikipedia. So for question [inaudible 00:32:39], just retrieve that and then put it into the context and answer. It does much better. So I feel like it sounds like a no-brainer, right? I mean, obviously. So I think that's what RAG is, as a simplest sense, it's just providing the model with a relevant context so that it can answer the questions. And it's where things get really more interesting because traditionally, when it started out, RAG is mostly text.

(00:33:03):
So we talk about a lot of way of how to prepare data so that the model can retrieve effectively. Let's say that not everything is a Wikipedia page. A Wikipedia page is pretty contained and you know, okay, everything about it is about a topic. But a lot of time, you have documents of like [inaudible 00:33:19] and they have a weird way of structures of documents. Let's say that you had documents about Lenny Podcast and in the future, in the beginning a document it's like, from now on, podcast wouldn't refer to Lenny's Podcast. So let's say somebody in the future is like, "Okay, tell me about Lenny. Lenny's work." And because as a [inaudible 00:33:40] document does not have the term Lenny, you just don't know, you might not retrieve it. And if the document is long enough that it's chunked into a different part, so the second part doesn't have the word Lenny, so you cannot reach it. So you have to find a way to process data. So that makes sure it's like... It can retrieve, the information is just relevant to the query even though it might not immediately obvious that it's related.

(00:34:02):
So people come up with only thing of, I think, contextual visual, like giving X chunk of the data, the relevant, maybe in a summary metadata so that it knows or some people use as hypothetical questions. It's very interesting for even the chunk of documents, I must generate a bunch of questions that the chunks can help answer so that when I have a query, it's like okay, does it match any of the hypothetical questions? It can fetch it. So it's very interesting approach. Okay, so maybe before I go to the next thing, I just want to say this data preparations for RAG is extremely important. And I would say this in a lot of the companies that I have seen, that's the biggest performance, in their RAG solutions coming from better data preparations, not agonizing over what [inaudible 00:34:51] databases to use because [inaudible 00:34:53] database, of course is very important to care about things like latency or if you have very specific access patterns like read-heavy or write-heavy, of course, it's like it matters. But in term of pure quality answers, I think the data preparation is just [inaudible 00:35:07].

Lenny Rachitsky (00:35:06):
When you say data preparation, what's an example to make that real and concrete for us to understand?

Chip Huyen (00:35:16):
So one way is mentioned as in you have chunks of data. So we have think about how big of each chunk should be. Because if it's sort of think about it's a context you want to maximize, maybe you can, it's very simple example. You want to retrieve a thousand words. So if a data chunk is long, then it's more likely to contain more relevant metadata so it can retrieve more. But if it's too long then you have a thousand word. And so chunk is like a thousand words, you can reach one chunk. So it's not very useful. But if it's too short, then you can retrieve more relevant information also. It can retrieve a wider range of documents and chunks, but at the same time each chunk is too small to contain relevant information.

(00:36:02):
So we have very nice chunk design, how big each chunk should be. You add contextual informations like summary, metadata, hypothetical questions. Somebody was telling me just a very big performance they got is that from rewriting their data in the question-answering format. Instead of having... So they have a podcast instead of just chunking the podcast, you just reframe, rewrite it into here's a question, here's answers and produce a lot of them. It can use AI for that as well. So that's one example of data processing. A lot of example I see is for people helping, using AI to help specific [inaudible 00:36:40] use and documentations. And we write documentation. Usually a lot of documentation today is written for human reading and AI reading is different because it's different because humans, we have common sense and we kind of know what it is. So one things are, even for human experts, they have the context that AI doesn't quite have.

(00:36:59):
So somebody told me that what's a big change they have is let's say, that you have a function. The documentation for this, maybe the library. As a library said okay, the output of this one is maybe talking for, I don't know, some crazy term, maybe some temperature or something on the graph. It should be like one zero or minus one. And as a human expert maybe understand the scale, what one in the scale mean, but for AI, just really doesn't understand what that means. So actually, have another annotation layer for AI. It's like, okay, good temperatures equal one means like that. It's not like it's a actual temperature. It's associated with the scale over there. So just saving all this data processing to make it easier for AI to retrieve the relevant information to answer the questions.

Lenny Rachitsky (00:37:45):
This episode is brought to you by Persona, the verified identity platform, helping organizations onboard users fight fraud and build trust. We talk a lot on this podcast about the amazing advances in AI, but this can be a double-edged sword. For every wow moment, there are fraudsters using the same tech to wreak havoc, laundering money, taking over employee identities and impersonating businesses. Persona helps combat these threats with automated user business and employee verification. Whether you're looking to catch candidate fraud, meet age restrictions or keep your platform safe, Persona helps you verify users in a way that's tailored to your specific needs. Best of all, Persona makes it easy to know who you're dealing with without adding friction for good users. This is why leading platforms like Etsy, LinkedIn, Square and Lyft trust Persona to secure their platform. Persona is also offering my listeners 500 free services per month for one full year. Just head to withpersona.com/lenny to get started. That's withpersona.com/lenny. Thanks again to Persona for sponsoring this episode.

(00:38:51):
Awesome. Okay. So you've talked a bit about how you work with companies on these sorts of things, on their AI strategies, on their AI products, how they build, which tools they build, all these things. I want to spend a little time here because a lot of companies are building AI products. A lot of companies are not having a good time building AI products. Let me ask a few questions along these lines of what you've learned working with companies that are doing this well. One is just, I guess, in terms of AI tool adoption and adoption in general within companies, there's all this talk recently of just all this AI hype. The data is actually showing most companies try it. Doesn't do a lot, they stop. And so there's all this just maybe this isn't going anywhere. So in terms of just adoption of tools in AI within companies, what are you seeing there?

Chip Huyen (00:39:32):
For GenAI in company, I think there are two types of GenAI toolings that have been, I've seen ones is to internal productivity, like have coding tools, Slack chatbot, internal knowledge. A lot of big enterprises have some a wrapper around models, so with access to maybe some different type of a RAG solution. I think we talk about data or kind of like text-based RAG. We haven't talked about agentic RAG or I haven't talked about multi-modal RAG yet. But this, yes, it's a whole very exciting area around that. So basically, it should allow the employee to access internal document. Somebody ask, okay, I'm having a baby. What could be the maternal or paternal policy or am I having these operations with the health benefit cover that or I want you to interview, I want to refer my friend. What will be the process for that? So a lot of this having chatbot, internal chatbot to help with internal operations.

(00:40:35):
And another things, another category is more customer facing or partner facing. So product customers support chatbot is a big one. If you're a hotel chain, you might have a booking chatbot, which is somehow massive. A lot of booking chatbot because I guess it's... I do have this theory of a lot of applications companies pursue because they can't measure the concrete outcome. And I feel like booking or a sales chatbot, it's very clear. There was a conversion rate right now with that chatbot with human operators and what could be conversion rate with a chatbot and certain, somehow I think it's very clear outcomes and companies are easier to buy into these solutions. So a lot of companies have that customer facing chatbot.

(00:41:20):
So that is another category of tool and I think that for customers or external facing tools, because people are driven to choose applications with clear outcomes. So the questions of adopting them is really based on whether they see the outcome or not. Of course, it's not perfect because sometimes the outcome can be bad not because the idea or the application's idea [inaudible 00:41:52] is bad. It's just because the process of building it is not that great. Yeah. So it's tricky. For the internal adoptions of toolings or internal productivities, that's where it gets tricky. I would say a lot of companies [inaudible 00:42:08] think of AI strategy. I think of AI strategies usually have two key aspects. It's like use cases and the second is talent. You might have great data for great use cases, but you don't have talents and you cannot do it.

(00:42:23):
So a lot of time at the beginning with GenAI and sometimes I'm really admire a lot of companies for that, it's just like [inaudible 00:42:28] was like, okay, we need our employees to be very GenAI aware, very AI literate. So what I do is I start maybe adopting a bunch of tools for the team to use. They have a lot of up-skilling workshops, they encourage learning and then it's a really, really good thing. And it's also willing to spend a lot of money into adopting, giving people chargeability, subscriptions, purchase subscriptions, [inaudible 00:42:56] subscriptions to get the employees to be more AI literate. And that's the thing is a lot of... There's a [inaudible 00:43:05] may say, okay, we spend a ton of money on this tooling, but then we don't see because you can see the usage, but people don't seem to use them as much and what is the issue. So yeah, so I think that is tricky. Yeah.

Lenny Rachitsky (00:43:20):
What do you think is the issue? Is it just they don't know how to use them? What do you think is the gap here? Do you think we'll get to a place of just like, wow, work is completely different because of AI for a lot of companies?

Chip Huyen (00:43:32):
The main thing is it's really hard to measure productivity again. So I talk to a lot of people on their website. First of all, [inaudible 00:43:40] is coding. A lot of companies not using coding agents or coding [inaudible 00:43:45] coding. And I was asking, I was like, "Do you think that it helps with your productivity?" And a lot of times, the questions are very [inaudible 00:43:56] okay, I feel like it's [inaudible 00:43:59] better. And I said, okay, because we have more PRs, we see more code and then immediate [inaudible 00:44:04]. Okay, but of course, code, number of live code is not a good metric for that. So it's really, really tricky and it's something funny. So I do ask people to ask their managers because I work with usually VP level, so they have multiple teams under them. So I asked them, okay, do you ask some managers, okay, would you rather have access...

(00:44:28):
Would you rather give everyone on the team very expensive coding agent subscriptions or you get an extra headcount? Let's say maybe and almost everyone could say the managers could say headcount. But if you ask VP level or someone who manage a lot of teams, they would say just like [inaudible 00:44:48] good one, AI, a system as tools. And the reason is that we could say okay, because as manager is right, because you are still growing. You're not as a level when you manage hundreds of thousands of people. So for you, having one HR headcount is big. So you want that not for productivity reasons, but because you just want to have more people working for you. Whereas for executives, you care more about, maybe you have more business metrics that you care about. So you actually think about what actually drive productivity metrics for you. So it is tricky and I think that the question of productivity. I'm not sure it's fundamentally is the [inaudible 00:45:32] more productive, but it's just like we don't have a good way of measuring productivity improvement.

(00:45:37):
Another thing is also very [inaudible 00:45:40]. And I think it's like people do tell me that they notice different buckets of employees, different reactions to AI assist tools. First of all, I keep going back to coding because coding is big and it's easier to reason somehow. So it says I have different reports. One team would tell me that... One of people tell me, okay, amongst on his engineers, he thinks senior engineers would get the most output, would be more productive because it's like, okay, so that person's very interesting. So he actually divided his team to three buckets, but he didn't tell them, obviously. He was like, okay, here's more currently best performing, average performing and lowest performing. And then there's a randomized trial. So they give half of each group access to Cursor. And then [inaudible 00:46:31] noticed over time it was like, okay, something funny. The group that get the biggest performance boost, in his opinion, he was very close to his team.

(00:46:39):
The biggest performance boost [inaudible 00:46:41] the senior engineer, the highest performing. So the highest performing engineer get the biggest boost out of it. And then the second group is the average performing. So his opinion is like, okay, the highest performing engineers is also normal practice. They also know how to solve problems. So they have some solved problem better. Whereas the people who have the lowest performing, they only don't care much about work. So it's easier to just go on autopilot, get it to generate that code and just do it or just don't know how to do it. Another company, however, they tell me just actually, senior engineers are the one most resistant to using AI as this tooling because they said it's like, okay, but AI, because they are more opinionated and they have very high standard. It was like, okay, but AI code, [inaudible 00:47:30] code just sucks. So just very, very resistant in using it. So I don't know, I haven't quite been able to reconcile very different reports on that yet.

Lenny Rachitsky (00:47:39):
This is so interesting. So just to make sure I'm hearing what the story, so there's a company you work with, that did a three bucket test with their engineering team where they created three sorts of groups, the highest performing engineers, mid-performing engineers, lowest performing engineers, and gave some of them, so they gave some of them access to say, Cursor. Was it Cursor or what did they give them access to? It was Cursor, right?

Chip Huyen (00:48:03):
I think it was Cursor.

Lenny Rachitsky (00:48:04):
Okay, cool. And so within-

Chip Huyen (00:48:05):
I didn't work with them. This is more like a friend company.

Lenny Rachitsky (00:48:08):
Okay. It's a friend's company.

Chip Huyen (00:48:09):
Yeah.

Lenny Rachitsky (00:48:09):
So did they give half of the higher performing engineers Cursor and half not or how did they do the split there?

Chip Huyen (00:48:14):
Yeah, so they give half of the entire company but half of each bucket. Yeah.

Lenny Rachitsky (00:48:18):
Whoa.

Chip Huyen (00:48:19):
And then they observe the difference in productivity.

Lenny Rachitsky (00:48:21):
I see. So how do they even do that? They're just like, "Okay, you get cursor, you don't get cursor." How did they do that? That's so interesting.

Chip Huyen (00:48:31):
Yeah, I didn't get into the mechanics of it, but I was like, "I respect you for doing a randomized trial on that."

Lenny Rachitsky (00:48:33):
That is so cool.

Chip Huyen (00:48:33):
Yeah. Yeah.

Lenny Rachitsky (00:48:34):
Okay. Wow. How large was this engineering team? Was it like hundreds of people?

Chip Huyen (00:48:38):
It's not that large. It's about maybe 30 to maybe 40. Yeah.

_[258 additional lines trimmed for context budget]_

---

### Why AI evals are the hottest new skill for product builders | Hamel Husain & Shreya Shankar
**Guest:** Hamel+Shreya | **Date:** 2025-09-25 | [YouTube](https://www.youtube.com/watch?v=BsWxPI9UM4c)  

# Zigging vs. zagging: How HubSpot built a $30B company | Dharmesh Shah (co-founder/CTO)

## Transcript

Lenny Rachitsky (00:00:00):
To build great AI products, you need to be really good at building evals. It's the highest ROI activity you can engage in.

Hamel Husain (00:00:05):
This process is a lot of fun. Everyone that does this immediately gets addicted to it. When you're building an AI application, you just learn a lot.

Lenny Rachitsky (00:00:12):
What's cool about this is you don't need to do this many, many times. For most products, you do this process once and then you build on it.

Shreya Shankar (00:00:18):
The goal is not to do evals perfectly, it's to actionably improve your product.

Lenny Rachitsky (00:00:23):
I did not realize how much controversy and drama there is around evals. There's a lot of people with very strong opinions.

Shreya Shankar (00:00:28):
People have been burned by evals in the past. People have done evals badly, so then they didn't trust it anymore, and then they're like, "Oh, I'm anti evals."

Lenny Rachitsky (00:00:36):
What are a couple of the most common misconceptions people have with evals?

Hamel Husain (00:00:39):
The top one is, "We live in the age of AI. Can't the AI just eval it?" But it doesn't work.

Lenny Rachitsky (00:00:45):
A term that you used in your posts that I love is this idea of a benevolent dictator.

Hamel Husain (00:00:49):
When you're doing this open coding, a lot of teams get bogged down in having a committee do this. For a lot of situations, that's wholly unnecessary. You don't want to make this process so expensive that you can't do it. You can appoint one person whose taste that you trust. It should be the person with domain expertise. Oftentimes, it is the product manager.

Lenny Rachitsky (00:01:09):
Today, my guests are Hamel Husain and Shreya Shankar. One of the most trending topics on this podcast over the past year has been the rise of evals. Both the chief product officers of Anthropic and OpenAI shared that evals are becoming the most important new skill for product builders. And since then, this has been a recurring theme across many of the top AI builders I've had on. Two years ago, I had never heard the term evals. Now it's coming up constantly. When was the last time that a new skill emerged that product builders had to get good at to be successful?

(00:01:41):
Hamel and Shreya have played a major role in shifting evals from being an obscure, mysterious subject to one of the most necessary skills for AI product builders. They teach the definitive online course on evals, which happens to be the number one course on Maven. They've now taught over 2,000 PMs and engineers across 500 companies, including large swaths of the OpenAI and Anthropic teams along with every other major AI lab.

(00:02:07):
In this conversation, we do a lot of show versus tell. We walk through the process of developing an effective eval, explain what the heck evals are and what they look like, address many of the major misconceptions with evals, give you the first few steps you can take to start building evals for your product, and also share just a ton of best practices that Hamel and Shreya have developed over the past few years. This episode is the deepest yet most understandable primer you'll find on the world of evals. And honestly, it got me excited to write evals, even though I have nothing to write evals for. I think you'll feel the same way as you watch this.

(00:02:41):
If this conversation gets you excited, definitely check out Hamel and Shreya's course on Maven. We'll link to it in the show notes. If you use the code LENNYSLIST when you purchase the course, you'll get 35% off the price of the course. With that, I bring you Hamel Husain and Shreya Shankar.

(00:02:58):
This episode is brought to you by Fin, the number one AI agent for customer service. If your customer support tickets are piling up, then you need Fin. Fin is the highest-performing AI agent on the market with a 65% average resolution rate. Fin resolves even the most complex customer queries. No other AI agent performs better. In head-head bake-offs with competitors, Fin wins every time. Yes, switching to a new tool can be scary, but Fin works on any help desk with no migration needed, which means you don't have to overhaul your current system or deal with delays in service for your customers.

(00:03:31):
And Fin is trusted by over 5,000 customer service leaders and top AI companies like Anthropic and Synthesia. And because Fin is powered by the Fin AI engine, which is a continuously improving system that allows you to analyze, train, test, and deploy with ease, Fin can continuously improve your results too. So if you're ready to transform your customer service and scale your support, give Fin a try for only 99 cents per resolution. Plus, Fin comes with a 90-day money-back guarantee. Find out how Fin can work for your team at fin.ai/lenny. That's fin.ai/lenny.

(00:04:05):
This episode is brought to you by Dscout. Design teams today are expected to move fast, but also to get it right. That's where Dscout comes in. Dscout is the all-in-one research platform built for modern product and design teams. Whether you're running usability tests, interviews, surveys, or in-the-wild fieldwork, Dscout makes it easy to connect with real users and get real insights fast. You can even test your Figma prototypes directly inside the platform. No juggling tools, no chasing ghost participants. And with the industry's most trusted panel plus AI-powered analysis, your team gets clarity and confidence to build better without slowing down. So if you're ready to streamline your research, speed up decisions, and design with impact, head to dscout.com to learn more. That's dscout.com. The answers you need to move confidently. Hamel and Shreya, thank you so much for being here, and welcome to the podcast.

Hamel Husain (00:05:04):
Thank you for having us.

Shreya Shankar (00:05:05):
Yeah, super excited.

Lenny Rachitsky (00:05:07):
I'm even more excited. Okay, so a couple years ago, I had never heard the term evals. Now it's one of the most trending topics on my podcast, essentially, that to build great AI products, you need to be really good at building evals. Also, it turns out some of the fastest-growing companies in the world are basically building and selling and creating evals for AI labs. I just had the CEO of Mercor on the podcast. So there's something really big happening here. I want to use this conversation to basically help people understand this space deeply, but let's start with the basics. Just what the heck are evals? For folks that have no idea what we're talking about, give us just a quick understanding of what an eval is, and let's start with Hamel.

Hamel Husain (00:05:49):
Sure. Evals is a way to systematically measure and improve an AI application, and it really doesn't have to be scary or unapproachable at all. It really is, at its core, data analytics on your LLM application and a systematic way of looking at that data, and where necessary, creating metrics around things so you can measure what's happening, and then so you can iterate and do experiments and improve.

Lenny Rachitsky (00:06:22):
So that's a really good broad way of thinking about it. If you go one level deeper just to give people a very, even more concrete way of imagining and visualizing what we're talking about, even if you have a example to show would be even better, what's an even deeper way of understanding what an eval is?

Hamel Husain (00:06:36):
Let's say you have a real estate assistant application and it's not working the way you want. It's not writing emails to customers the way you want, or it's not calling the right tools, or any number of errors. And before evals, you would be left with guessing. You would maybe fix a prompt and hope that you're not breaking anything else with that prompt, and you might rely on vibe checks, which is totally fine.

(00:07:11):
And vibe checks are good and you should do vibe checks initially, but it can become very unmanageable very fast because as your application grows, it's really hard to rely on vibe checks. You just feel lost. And so evals help you create metrics that you can use to measure how your application is doing and kind of give you a way to improve your application with confidence. That you have a feedback signal in which to iterate against.

Lenny Rachitsky (00:07:44):
So just to make very real, so imagining this real estate agent, maybe they're helping you book a listing or go see an open house. The idea here is you have this agent talking to people, it's answering questions, pointing them to things. As a builder of that agent, how do you know if it's giving them good advice, good answers? Is it telling them things that are completely wrong?

(00:08:04):
So the idea of evals, essentially, is to build a set of tests that tell you, how often is this agent doing something wrong that you don't want it to do? And there's a bunch of ways you could define wrong. It could be just making up stuff. It could be just answering in a really strange way. The way I think about evals, and tell me if this is wrong, just simply is like unit tests for code. You're smiling. You're like, "No, you idiot."

Shreya Shankar (00:08:29):
No, that's not what I was thinking.

Lenny Rachitsky (00:08:31):
Okay. Okay, okay, tell me. Tell me, how does that feel as a metaphor?

Shreya Shankar (00:08:35):
Okay. I like what you said first, which is we had a very broad definition. Evals is a big spectrum of ways to measure application quality. Now, unit tests are one way of doing this. Maybe there are some non-negotiable functionalities that you want your AI assistant to have, and unit tests are going to be able to check that. Now, maybe you also, because these AI assistants are doing such open-ended tasks, you kind of also want to measure how good are they at very vague or ambiguous things like responding to new types of user requests or figuring out if there's new distributions of data like new users are coming and using your real estate agent that you didn't even know would use your product. And then all of a sudden, you think, "Oh, there's a different way you want to kind of accommodate this new group of people."

(00:09:24):
So evals could also be a way of looking at your data regularly to find these new cohorts of people. Evals could also be like metrics that you just want to track over time, like you want to track people saying, "Yes. Thumbs up. I liked your message." You want very, very basic things that are not necessarily AI-related but can go back into this flywheel of improving your product. So I would say, overall, unit tests are a very small part of that very big puzzle.

Lenny Rachitsky (00:09:56):
Awesome. You guys actually brought an example of an eval just to show us exactly what the hell we're talking about. We're talking in these big ideas. So how about let's pull one up and show people, "Here's what an eval is."

Hamel Husain (00:10:06):
Yeah, let me just set the stage for it a little bit. So to echo what Shreya said, it's really important that we don't think of evals as just tests. There's a common trap that a lot of people fall into because they jump straight to the test like, "Let me write some tests," and usually that's not what you want to do. You should start with some kind of data analysis to ground what you should even test, and that's a little bit different than software engineering where you have a lot more expectations of how the system is going to work. With LLMs, it's a lot more surface area. It's very stochastic, so you kind of have a different flavor here.

(00:10:47):
And so the example I'm going to show you today, it's actually a real estate example. It's a different kind of real estate example. It's from a company called Nurture Boss. I can share my screen to show you their website just to help you understand this use case a little bit, so let me share my screen. So this is a company that I worked with. It's called Nurture Boss, and it is a AI assistant for property managers who are managing apartments, and it helps with various tasks such as inbound leads, customer service, booking appointments, so on and so forth. It's like all the different sort of operations you might be doing as a property manager, it helps you with that. And so you can see kind of what they do. It's a very good example because it has a lot of the complexities of a modern AI application.

(00:11:40):
So there's lots of different channels that you can interact through the AI with like chat, text, voice, but also, there's tool calls, lots of tool calls for booking appointments, getting information about availability, so on and so forth. There's also RAG retrieval, getting information about customers and properties and things like that. So it's pretty fully fleshed in terms of an AI application. And so they have been really generous with me in allowing me to use their data as a teaching example. And so we have anonymized it, but what I'm going to walk through today is, okay, let's do the first part of how we would start to build evals for Nurture Boss. Why would we even want to do that?

(00:12:36):
So let's go through the very beginning stage, what we call error analysis, which is, let's look at the data of their application and first start with what's going wrong. So I'm going to jump to that next, and I'm going to open an observability tool. And you can use whatever you want here. I just happen to have this data loaded in a tool called Braintrust, but you can load it in anything. We don't have a favorite tool or anything in the blog post that we wrote with you. We had the same example but in Phoenix Arize, and I think Aman, on your blog post, used Phoenix Arize as well. And there's also LangSmith. So these are kind of like different tools that you can use.

(00:13:29):
So what you see here on the screen, this is logs from the application, and let me just show you how it looks. So what you see here is, and let me make it full screen, this is one particular interaction that a customer had with the Nurture Boss application, and what it is is a detailed log of everything that happened. So it's called a trace, and it's just the engineering term for logs of a sequence of events. The concept of a trace has been around for a really long time, but it's especially really important when it comes to AI applications.

(00:14:12):
And so we have all the different components and pieces and information that the AI needs to do its job, and we are logged all of it and we're looking at a view of that. And so you see here a system prompt. The system prompt says, "You are an AI assistant working as a leasing team member at Retreat at Acme Apartments." Remember, I said this is anonymized, so that's why the name is Acme Apartments. "Your primary role is to respond to text messages from both current residents and prospective residents. Your goal is to provide accurate, helpful information," yada, yada, yada. And then there's a lot of detail around guidelines of how we want this thing to behave.

Lenny Rachitsky (00:14:56):
Is this their actual system prompt, by the way, for this company?

Hamel Husain (00:14:58):
It is. Yes, it is.

Lenny Rachitsky (00:14:58):
Amazing. That's so cool.

Hamel Husain (00:14:59):
It's a real system prompt.

Lenny Rachitsky (00:15:01):
That's amazing because it's rare you see a actual company product's system prompt. That's like their crown jewels a lot of times, so this is actually very cool on its own.

Hamel Husain (00:15:08):
Yeah. Yeah, it's really cool. And you see all of these different sort of features that are different use cases, so things about tour scheduling, handling applications, guidance on how to talk to different personas, so on and so forth. And you can see the user just kind of jumps in here and asks, "Okay, do you have a one-bedroom with study available? I saw it on virtual tours." And then you can see that the LLM calls some tools. It calls this get individual's information tool, and it pulls back that person's information. And then it gets the community's availability. So it's querying a database with the availability for that apartment complex.

(00:16:01):
And then finally, the AI responds, "Hey, we have several one-bedroom apartments available, but none specifically listed with a study. Here are a few options."

(00:16:12):
And then it says, "Can you let me know when one with a study is available?"

(00:16:16):
And then it says, "I currently don't have specific information on the availability of a one-bedroom apartment."

(00:16:23):
User says, "Thank you."

(00:16:25):
And the AI says, "You're welcome. If you have any more questions, feel free to reach out." Now, this is an example of a trace, and we're looking at one specific data point. And so one thing that's really important to do when you're doing data analysis of your LLM application is to look at data. Now, you might wonder, "There's a lot of these logs. It's kind of messy. There's a lot of things going on here. How in the hell are you supposed to look at this data? Do you want to just drown in this data? How do you even analyze this data?"

(00:17:07):
So it turns out there is a way to do it that is completely manageable, and it's not something that we invented. It's been around in machine learning and data science for a really long time, and it's called error analysis. And what you do is, the first step in conquering data like this is just to write notes. Okay? So you got to put your product hat on, which is why we're talking to you, because product people have to be in the room and they have to be involved in sort of doing this. Usually a developer is not suited to do this, especially if it's not a coding application.

Lenny Rachitsky (00:17:47):
And just to mirror back, why I think you're saying that is because this is the user experience of your product. People talking to this agent is the entire product essentially, and so it makes sense for the product person to be super involved in this.

Hamel Husain (00:17:59):
Yeah. So let's reflect on this conversation. Okay, a user asked about availability. The AI said, "Oh, we don't really have that. Have a nice day." Now, for a product that is helping you with lead management, is that good? Do you feel like this is the way we want it to go?

Lenny Rachitsky (00:18:30):
Not ideal.

Hamel Husain (00:18:32):
Yes, not ideal, and I'm glad you said that. A lot of people would say, "Oh, it's great. The AI did the right thing. It looked, it said, 'We didn't have available,' and it's not available." But with your product hat on, you know that's not correct. And so what you would do is you would just write a quick note here. You would say, "Okay." You might pop in here, and you can write a note. So every observability application has ability to write notes, and you wouldn't try to figure out if something is wrong. In this case, it's kind of not doing the right thing, but you just write a quick note, "Should have handed off to a human."

Lenny Rachitsky (00:19:19):
And as we watch this happening, it's like you mention this and you'll explain more. You're doing this, this feels very manual and unscalable, but as you said, this is just one step of the process and there's a system to this. That was just the first one.

Hamel Husain (00:19:30):
Yeah, and you don't have to do it for all of your data. You sample your data and just take a look, and it's surprising how much you learn when you do this. Everyone that does this immediately gets addicted to it and they say, "This is the greatest thing that you can do when you're building an AI application." You just learn a lot and you're like, "Hmm, this is not how I want it to work. Okay." And so that's just an example.

(00:19:58):
So you write this note, and then we can go on to the next trace. So this is the next trace. I just pushed a hot key on my keyboard. Let me go back to looking at it.

Lenny Rachitsky (00:20:09):
And these tools make it easy to go through a bunch and add these notes quickly.

Hamel Husain (00:20:13):
Yes. And so this is another one. Similar system prompt. We don't need to go through all of it again. We'll just jump right into the user question. "Okay, I've been texting you all day." Isn't that funny? And the user says, "Please." Okay, yeah, this one is just like an error in the application where this is a text message application, sorry, the channel through which the customer is communicating is through text message, and you're just getting really garbled. And you can see here that it kind of doesn't make sense. The words are being cut off like, "In the meantime," and then the system doesn't know how to respond, because you know how people text message, they write short phrases. They split their sentence across four or five different turns. So in this case-

Lenny Rachitsky (00:21:16):
Yeah, so what do you do with something like that?

Hamel Husain (00:21:18):
Yeah, so this is a different kind of error.

Lenny Rachitsky (00:21:19):
Mm.

Hamel Husain (00:21:19):
This is more of, "Hey, we're not handling this interaction correctly. This is more of a technical problem," rather than, "Hey, the AI is not doing exactly what we want." So we would write that down too.

Lenny Rachitsky (00:21:20):
Which is still really cool.

Hamel Husain (00:21:20):
Yeah.

Lenny Rachitsky (00:21:31):
It's amazing you're catching that, too, here. Otherwise, you'd have no idea this was happening.

Hamel Husain (00:21:35):
Yeah, you might not know this is happening, right? And so you would just say, "Okay." You would write a note like, "Oh, conversation flow is janky because of text message."

Lenny Rachitsky (00:21:51):
And I like that, I like that you're using the word janky. It shows you just how informal this can be at this stage.

Hamel Husain (00:21:56):
Yeah, it's supposed to be chill. Just don't overthink it. And there's a way to do this. So the question always comes up, how do you do this? Do you try to find all the different problems in this trace? What do you write a note about? And the answer is, just write down the first thing that you see that's wrong, the most upstream error. Don't worry about all the errors, just capture the first thing that you see that's wrong, and stop, and move on. And you can get really good at this. The first two or three can be very painful, but you can do a bunch of them really fast.

(00:22:38):
So here's another one, and let's skip the system prompt again. And the user asks, "Hey, I'm looking for a two- to three-bedroom with either one or two baths. Do you provide virtual tours?"

(00:22:51):
And a bunch of tools are called and it says, "Hi Sarah. Currently, we have three-bedroom, two-and-a-half-bathroom apartment available for $2,175. Unfortunately, we don't have any two-bedroom options at the moment. We do offer virtual tours. You can schedule a tour," blah, blah. It just so happens that there is no virtual tour, right?

Lenny Rachitsky (00:23:16):
Mm-hmm. Nice.

Hamel Husain (00:23:16):
So it is hallucinating something that doesn't exist. Then you kind of have to bring your context as an engineer, or even product content, and say, "Hey, this is kind of weird. We shouldn't be telling a person about virtual tour when it's not offered."

(00:23:32):
So you would say, "Okay, offered virtual tour," and you just write the note. So you can see there's a diversity of different kinds of errors that we're seeing, and we're actually learning a lot about your application in a very short amount of time.

Shreya Shankar (00:23:55):
One common question that we get from people at this stage is, "Okay, I understand what's going on. Can I ask an LLM to do this process for me?"

Lenny Rachitsky (00:24:04):
Mm, great question.

Shreya Shankar (00:24:04):
And I loved Hamel's most recent example because what we usually find when we try to ask an LLM to do this error analysis is it just says the trace looks good because it doesn't have the context needed to understand whether something might be bad product smell or not. For example, the hallucination about scheduling the tour, right? I can guarantee you, I would bet money on this, if I put that into chat GPT and asked, "Is there an error?" it would say, "No, did a great job."

(00:24:34):
But Hamel had the context of knowing, "Oh, we don't actually have this virtual tour functionality," right? So I think, in these cases, it's so important to make sure you are manually doing this yourself. And we can talk a little bit more about when to use LLMs in the process later, but number one pitfall right here is people are like, "Let me automate this with an LLM."

Lenny Rachitsky (00:24:55):
Do you think we'll get to a place where an agent can do this, where it has that context?

Shreya Shankar (00:24:58):
Oh, no. No, no, no. Sorry. There are parts of error analysis that an LLM is suited for, which we could talk about later in this podcast. But right now, in this stage of free form, note-taking is not the place for an LLM.

Lenny Rachitsky (00:25:13):
Got it. And this is something you call open coding, this step?

Shreya Shankar (00:25:14):
Yes, absolutely.

Lenny Rachitsky (00:25:17):
Cool. Another term that you used in your posts that I love and that fits into this step is this idea of a benevolent dictator. Maybe just talk about what that is, and maybe, Shreya, cover that.

Shreya Shankar (00:25:27):
Yeah, so Hamel actually came up with this term.

Lenny Rachitsky (00:25:29):
Okay, maybe Hamel cover that, actually.

Hamel Husain (00:25:33):
No problem. And we'll actually show the LLM automation in this example, because we're going to take this example, we're going to go all the way through.

Lenny Rachitsky (00:25:40):
Amazing.

Hamel Husain (00:25:41):
And so benevolent dictator is just a catchy term for the fact that when you're doing this open coding, a lot of teams get bogged down in having a committee do this. And for a lot of situations, that's wholly unnecessary. People get really uncomfortable with, "Okay, we want everybody on board. We want everybody involved," so on and so forth. You need to cut through the noise. And a lot of organizations, if you look really deeply, especially small, medium-sized companies, you can appoint one person whose tastes that you trust. And you can do this with a small number of people and often one person, and it's really important to make this tractable. You don't want to make this process so expensive that you can't do it. You're going to lose out.

(00:26:36):
So that's the idea behind benevolent dictator, is, "Hey, you need to simplify this across as many dimensions as you can." Another thing that we'll talk about later is when it goes to building an LLM as a judge, you need a binary score. You don't want to think about, "Is this like a 1, 2, 3, 4, 5?" Like, assign a score to it. You can't. That's going to slow it down.

Lenny Rachitsky (00:26:59):
Just to make sure this benevolent dictator point is really clear, basically, this is the person that-

Lenny Rachitsky (00:27:00):
Make sure this benevolent dictator point is really clear. Basically, this is the person that does this note-taking, and ideally they're the expert on the stuff. So if it's law stuff, maybe there's a legal person that owns this, it could be a product manager. Give us advice on who this person should be?

Hamel Husain (00:27:16):
Yeah. It should be the person with domain expertise. So in this case, it would be the person who understands the business of leasing, apartment leasing, and has context to understand if this makes sense. It's always a domain expert, like you said. Okay. For legal, it would be a law person. For mental health, it would be the mental health expert, whether that's a psychiatrist or someone else.

Lenny Rachitsky (00:27:41):
Cool.

Hamel Husain (00:27:42):
Though oftentimes, it is the product manager.

Lenny Rachitsky (00:27:44):
Cool. So the advice here is pick that person. It may not feel so super fair that they're the one in charge and they're the dictator, but they're benevolent. It's going to be okay.

Hamel Husain (00:27:52):
Yeah. It's going to be okay. It's not perfection. You're just trying to make progress and get signal quickly so you have an idea of what to work on because it can become infinitely expensive if you're not careful.

Lenny Rachitsky (00:28:07):
Yeah. Okay, cool. Let's go back to your examples.

Hamel Husain (00:28:09):
Yeah, no problem. So this is another example where we have someone saying, "Okay. Do you have any specials?" And the assistant or the AI responds, "Hey, we have a 5% military discount." User responds, and it switches the subject, "Can you tell me how many floors there are? Do you have any one-bedrooms available or one-bedrooms on the first floor?" And the AI responds, "Yeah, okay. We have several one-bedroom apartments available." And then the user wants to confirm, "Any of those on the first floor and how much are the one-bedrooms?" And then also, it's a current resident, so they're also asking, "I need a maintenance request."

(00:28:56):
You could see the messiness of the real world in here, and the assistant just calls a tool that says transfer call, but it doesn't say anything. It just abruptly does transfer call, so it's pretty jank, I would say. It's just not-

Lenny Rachitsky (00:29:13):
Another jank.

_[843 additional lines trimmed for context budget]_

---

### The art of product management | Shreyas Doshi (Stripe, Twitter, Google, Yahoo)
**Guest:** Shreyas Doshi Live | **Date:** 2022-08-25 | [YouTube](https://www.youtube.com/watch?v=YP_QghPLG-8)  

# The art of product management | Shreyas Doshi (Stripe, Twitter, Google, Yahoo)

## Transcript

Lenny Rachitsky (00:02):
Today, I am super excited to bring you a very special episode with Shreyas Doshi, recorded live at the Lenny and Friends Summit in front of 1,000 people in San Francisco. This is Shreyas' second time on the podcast. His first visit is the third most popular episode of all time of this podcast, and I love that Shreyas was game to try this.

(00:20):
In our conversation, Shreyas shares three questions plus a bonus question that he wished he'd asked himself sooner in his career. We talk about why product leaders are so busy, why the job is so frustrating, why it is so central to build good taste, and also why you're probably not listening as well as you should be. This was so much fun, a huge thank you to Shreyas for doing this.

(00:41):
If you enjoy this podcast, don't forget to subscribe and follow it in your favorite podcasting app or YouTube. It's the best way to avoid missing future episodes, and helps the podcast tremendously. With that, I bring you Shreyas Doshi. Shreyas, thank you so much for being here, and welcome to the podcast.

Shreyas Doshi (01:01):
Thanks, Lenny, for having me. This is amazing.

Lenny Rachitsky (01:04):
I was going to ask, we recorded our first episode, I think two years ago, and I was in a tiny room in my house. I don't know where you were, but it was very not like this. Thoughts on the setup of this episode?

Shreyas Doshi (01:18):
So first, the Lenny Empire keeps growing, which is amazing to see. And second, as I was coming up here, somebody told me this used to be a car dealership, and I actually realized I purchased my car here.

Lenny Rachitsky (01:34):
What?

Shreyas Doshi (01:36):
So crazy, only in SF.

Lenny Rachitsky (01:38):
What kind of car was this? Say more.

Shreyas Doshi (01:40):
It was a Honda CR-V.

Lenny Rachitsky (01:42):
Okay, wow. I am told this venue was also used for... Jimi Hendrix performed here, and Aretha Franklin performed here. So it's like Jimi Hendrix, Aretha Franklin, Shreyas.

Shreyas Doshi (01:56):
There we go, that's going up on my Twitter bio soon.

Lenny Rachitsky (02:02):
Okay, so usually, when we talk, you're full of ideas and you're full of answers. When we were preparing for this, you told me, "I have questions, I have questions I want to ask."

Shreyas Doshi (02:16):
You know, reflecting on my career as a PM leader over the years, there are some questions I wish I had asked myself sooner, but I did not, and I had the great luck of having a life, a PM life full of suffering, and I have zero complaints about it. But as I look back, I feel like there are some questions that even if I asked myself some questions, those questions, I wasn't honest to myself about the answers. So that's what I thought I'd do, is share the questions that I wish I'd asked myself sooner.

Lenny Rachitsky (03:05):
Awesome.

Shreyas Doshi (03:06):
Yeah.

Lenny Rachitsky (03:06):
Love that, okay. This episode is brought to you by WorkOS. If you're building a SaaS app, at some point your customers will start asking for enterprise features, like SAML authentication and SCIM provisioning. That's where WorkOS comes in, making it fast and painless to add enterprise features to your app. Their APIs are easy to understand so that you can ship quickly and get back to building other features. Today, hundreds of companies are already powered by WorkOS, including ones you probably know like, Vercel, Webflow, and Loom.

(03:41):
WorkOS also recently acquired Warrant, the Fine Grained Authorization service. Warrant's product is based on a groundbreaking authorization system called Zanzibar, which was originally designed for Google to power Google Docs and YouTube. This enables fast authorization checks at enormous scale, while maintaining a flexible model that can be adapted to even the most complex use cases. If you're currently looking to build role-based access control or other enterprise features, like single sign-on, SCIM, or user management, you should consider WorkOS. It's a drop-in replacement for Auth0 and supports up to 1 million monthly active users for free. Check it out at workos.com to learn more. That's workos.com.

(04:29):
This episode is brought to you by Paragon, the developer platform for building native, customer-facing integrations with third-party apps. Are native integrations on your product roadmap? Whether it's to ingest context from your user's external data and documents, or to sync data and automate tasks across your users' other apps, integrations are mission-critical for B2B software products today.

(04:50):
But building these integrations in-house cost an average of three months of engineering, according to the 2024 State of Integration survey, which results in difficult roadmap trade-offs. This is why engineering teams at Coffee AI, AI21, and over 100 other B2B SaaS companies use Paragon, so they can focus their efforts on core product features, not integrations. The result, they've shipped integrations seven times faster, all while avoiding the never-ending maintenance that comes with rolling your own integrations. Visit useparagon.com/lenny to see how Paragon can help you accelerate your product's integration roadmap today and get $1,000 in credit on their Pro and Enterprise plans. That's useparagon.com.

(05:37):
What's the first question?

Shreyas Doshi (05:39):
Right, so let's see, the first question is why am I so busy? Why am I so busy? And the background is that I have spent most of my career just being completely stressed out, just absolutely stressed out every day. And there were many reasons for it, but one of the core reasons was I was always super busy, and there was always work I felt like I couldn't do that I wanted to do, and so I would go home at the end of the day, and even if I had worked hard, I'd just feel dissatisfied.

(06:27):
And so that was a constant fixture of my life as a PM, PM leader, and it's only... So I did product work for about 20 years before I started this new chapter of my career. And I think I only fixed it in the last three or four years of my career as a PM leader. But that means that there were about 16 or 17 years where I was just incredibly busy, and because I was incredibly busy, I was extremely stressed, and even though I was doing a good job, I was not feeling very good inside. And then, that showed up in my body, like all sorts of pains and aches I realized were actually not physical pains and aches. They were pains and aches from the stress.

Lenny Rachitsky (07:15):
It's like health issues that you had?

Shreyas Doshi (07:16):
Yeah, yeah, minor stuff. I mean, relatively minor stuff, but playing tennis, and you pull your back muscle, and now you are horizontal for three days, doesn't feel good.

Lenny Rachitsky (07:31):
Who here is very, very busy, and is just way too busy? Raise your hand.

Shreyas Doshi (07:36):
That's it?

Lenny Rachitsky (07:36):
Yeah, I know.

Shreyas Doshi (07:37):
Whoa.

Lenny Rachitsky (07:39):
Everyone's like [inaudible 00:07:39] everybody. Yeah, people were like, "Yeah, yeah. I don't have to raise my hand, I'm busy." Yeah. Okay, keep going.

Shreyas Doshi (07:50):
Yeah, and so here's the thing, when we talk about being busy, and managing your time, energy, all of that, I mean, this is a group of senior product people, so you all know that. Take tips and techniques, like maintain a to-do list. I found the LNO framework very useful for me, which I've shared before. I used to like working out of a calendar, those types of things. And I think you're all familiar with those things, but what I wanted to call out is that, at some point in our product career, we reach something, we reach an immovable force that will just overwhelm us, no matter what we do.

(08:39):
And that force is a scope. Okay, so as we grow in our product career, our scope grows, and we kind of like that, which is all great, but at some point, if you haven't already gotten there, many of you have, but for those of you who haven't, you will get there, where your scope will be so large, that no matter what you do in terms of efficiency, whatever framework you use for prioritization, whatever framework or tool you use to manage your to-do list, whatever tools and techniques you use, whatever prioritization you do, your scope is so large that you are still going to be incredibly busy.

(09:21):
And so that's what I faced, like I was saying for the first about 16, 17 years of working on products, and only in the last 3 or 4 years was I able to kind of find some answers on how to deal with that scope. And so perhaps we can talk about that, what do you think?

Lenny Rachitsky (09:43):
Yeah, so you're basically saying there's all these productivity tricks, ways to do more faster. And no matter how many of these tools you've got, you are just going to take on more and more work, and they'll peter out? I'll say many of my most popular newsletters posts are, "Here's productivity tricks and tips." And so people are always looking for these. And I'm curious to hear where you go with this, of just like that is not the answer long term, there's a different approach?

Shreyas Doshi (10:08):
Yes, and so the challenge is the following. How many of you are going through some kind of annual planning right now or you're planning on going through annual planning? Please. Everybody loves annual planning, great. So let's take annual planning. If you are a high-level manager, leader within a company, what does your month look like? Or in some cases, unfortunately, what do your two or three months look like when you are going through annual planning? It's all these kind of spreadsheets to fill out, and meetings to have, and dependencies, and priorities, and stakeholders to meet, and so on. And so I noticed, for instance, that at some point, that was making me really busy, and then that was making me feel guilty now, because I had my team to look after and to support, and then I had product decisions to make and various other things, and I've gone on some planning retreat or whatever.

(11:15):
And there you go, you last four, five, six, weeks, does that sound familiar to folks? Yeah? Okay, so I noticed that I needed to change that at some point, and actually found a solution, again, late in my career, but I found a solution, because I asked myself this question, which is, "Why am I so busy? I'm doing all the efficiency things, I'm managing my to-do list like a champ. I have my calendar set up just right. I have my routine set up just right. I'm working out so that I'm engaged at work, I'm productive, I'm doing all of that. Why am I so busy? Oh, it's planning season, and that is supposed to take up four to six weeks." And this was at Stripe when I encountered this. So that is supposed to take up four to six weeks.

(12:03):
Well, I realized that you don't have to do that. And so at some point in my time at Stripe, what I realized is the following, we go through a whole all sorts of just rituals around planning for four to six weeks. Then, we emerge and we share our plan with our executives. You know the questions they're going to ask, like, "If money were no concern, what would you do? What is your ambitious plan? So if we gave you five more engineers, what would you do? What other things would you include in the roadmap?" Right? The standard stuff. And so you emerge, you do your presentation, and then you publish the plan, and then you start the new year with a lot of enthusiasm and a lot of excitement. And January goes fine, until you get three customer escalations for features that were not in your plan. And so now, you try to figure out how you're going to revise resources.

(13:03):
You go talk to some dependency team that's going to sort of support these new features from these customer escalations. And you go through that process, and you revise your plan again. And then, usually by the time it's last week of February, everybody's forgotten the actual plan. And now, we are executing off of some other list somewhere. And by the way, when you mention this at times, politely of course, you might mention like, "You know, I'm noticing we are not actually really using the plan that we spent four to six weeks minimum doing." And then, some smart person in the room chimes in with, "Plans are useless, but planning is everything."

(13:52):
I don't know, Eisenhower, somebody else, I don't know who said this, "Plans are useless, but planning is everything." Nobody knows what it means. Nobody knows what that means, but everybody appreciates, "Ah, plans are useless, but planning is everything." Right? So I went through a few years of this. And then I go, "You know what? I'm going to bend some rules here." And so what I realized, Lenny, is you don't have to go through these four to six weeks, and it was an accident. Basically, what happened is around that time, the product I was working on, Stripe Connect, it's like a major product for Stripe, major, major business for Stripe. And I had put together a product strategy, like a real product strategy for this product.

(14:44):
And so this must have been earlier in the year. And so now, planning season came along. And the interesting I found is that because I had a real product strategy, not one of those fake ones, a real product strategy that I had gotten alignment on with everybody, my planning for this major product for Stripe took me like three days. Right, so while a lot of my peers, unfortunately, for their own products, were in this four-to-six week cycle of planning, and meeting, and blah, blah, blah, I just put it all together in three days, and whatever artifacts were needed, I put them together.

(15:27):
I did not fill out some templates. That's where it's about bending the rules, because if a template doesn't make sense, why should I fill it out? There's no need to fill it out. And so that's when I realized that actually, if you have a real product strategy, a real one that everybody is aligned with, that you have got pre-alignment on, then a lot of this nonsense we tend to do with annual planning actually goes away. Now, you still have to do some resource allocation and all of that, but even there, you don't need that false precision. How many of you have gotten into arguments about, "So should it be eight engineers for this team in 2025, or nine engineers for this team in 2025?"

(16:11):
Like, who cares? We all know that even those numbers that we set up, we don't actually follow through on them, as 2025 happens. So that's just an example of where we spend a lot of time on things that we think are strategic, that we think are important, but actually, we ought to spend that time on other much higher leverage things. Right, now it does require some upfront work, in this case upfront work on a clear product strategy that everybody understands, that everybody's aligned on. But frankly, if you have that, planning should be a breeze.

Lenny Rachitsky (16:49):
So what would be your kind of tactical tip for folks that want to do this better? I know there's probably a billion examples of these sorts of things you shared, so planning is an example. Folks that want to be less busy, maybe on that one is it give yourself very little time, and focus on strategy, and let that be the plan, basically, versus every single person and their roadmap for the next six months. What's the piece of advice you'd share there? And then, I want to move on to the next question, because I want to make sure we get through all these questions.

Shreyas Doshi (17:18):
Yes, there's definitely a specific tip, which is if you do have a strategy that will make a lot of your prioritization problems go away, it will make a lot of planning problems go away. And even if you do have some escalation from sales, which you will, or from support, or somewhere else, you now have at least a more rigorous framework to figure out what to do with that escalation. So there's definitely that.

(17:48):
But I think other thing I want to share is that, and this was my other realization as I asked the question, "Why am I so busy?" is I realized that I am so busy because I'm not making good product decisions. Okay, now, you have to understand, by this time I'm like 15 years into building products and whatever, 11 years into being a product manager. And so I think I'm pretty good. That's my kind of self-image. But then, again, if I'm being honest to myself, I'm not making as good product decisions as I can. So can I share an example of that?

Lenny Rachitsky (18:32):
Please.

Shreyas Doshi (18:34):
So what I noticed is that you have a meeting about some product feature that somehow is requested or is really important, whatever the case might be. And so you have a meeting with some stakeholders and your engineering team, designers, et cetera. And then, you're trying to decide, "Should we build this or not?" And somebody says, "You know what? Why are we doing a meeting for this?" I read somewhere or I heard Bezos say that two-way doors, it's a two-way door. You quickly make a decision. Like, just quickly make a decision and move on. This is a two-way door. And so you say, "Yeah, that's right." And any time we hear something like that, two-way door, you're like, "Oh, that person's really smart, so I want to be like them."

(19:32):
So I noticed that myself and my team, we were making these kinds of decisions without very clearly thinking through customer motivation, very clearly thinking through differentiation, very clearly thinking through a distribution approach for whatever this feature is. And while it sounds like, "Oh, of course you should be doing this," I guarantee you this is not how most product teams work. They're talking about, "Well, is Bob the engineer going to be free? And when are they free? And if they are free, then let's build the feature." That's kind of how a lot of product decisions happen.

(20:11):
The challenge here with this kind of approach, and again, this is what happens in practice, I'm not talking about whatever theory you read, this is what happens in practice. So when you follow this approach and you assume that, "Oh, this is a two-way door, we can kill the feature," in reality, it doesn't work out that way, because here's what happens in reality. So in reality, you commit to the feature, and it's going to take five, six weeks to do it, and then a couple more weeks to make sure, to ramp it up, et cetera, right? And so now the feature is out, and now you have your Q1 QBR, right? Say two months from now you have your Q1 QBR, and you're going to present your business review, whatever. You're going to present what you did, "What did you do last quarter? How are your ships performing from last quarter?"

(21:03):
And so now, it's time to talk about this feature at the QBR, because you have to share that/ Now as you start talking about this feature, the CEO will ask, "So yeah, we launched the feature. I'm very glad we launched this feature. How is it doing?" And you want to be able to say, you are the PM leader, you want to be able to say something smart, and something that makes you look competent. But the challenge is the feature hasn't had much adoption. So I'm not going to ask anybody to raise hands, but I think most PMs are familiar with this conundrum. And of course, we are verbally very agile as product leaders. So what we say is we don't have data, so we use favorable anecdotes.

(21:53):
And so we say, "Yeah, we launched the feature, and you know what? This customer from this company really loves the feature." And we put in an anecdote, it's like, "Life-changing feature." It doesn't matter that they're the only person using it. That doesn't matter. "Life-changing feature," right? We use data when it favors us, we use anecdotes when it favors us. So anyway, so we present that. Now. we do have the sales counterpart in the room too, our sales counterpart, and they say, "You know what, though? We are still not winning many deals because of this feature." And so of course the CEO asks, "So what's up? Why aren't we winning deals, even though we have the feature?"

(22:30):
So the people on the customer side usually we respond, "Well, I'm glad we have the feature, but it's not full featured yet. We need all these other bells and whistles to meet the table stakes." So now, what happens? Somebody uttered the word, "Meet the table stakes." Now, it's over for you, because now, the only response you can give is, "Oh, yeah, that's already part of the plan." And now, you put your engineering leader on the spot and you say, "Alice, isn't it? Haven't we allocated engineers to it already?" And so now, Alice has to come up with some response, which is like, "Yeah, yeah, yeah, Carol and David are going to work on it. It's slotted for one of these sprints."

(23:19):
And so now, you exit the QBR, you high-five each other. Well, "Good job, team, great job," et cetera, et cetera. But now, you have signed up for even more work for a feature you should not have built in the first place. That's why we're busy. And through a product leader's life, what happens is we just accumulate all of this debt, feature after feature. So I guess what I'm saying, Lenny, is one of my other tactical tips would be sometimes it is useful to pause for two minutes, or two days, or two weeks before making that decision, right? Because frankly, most doors that look like two-way doors are actually one-way doors. They are two-way doors at Bezos' level, but as a PM leader, for you, they are a one-way door, and that's making you busy.

Lenny Rachitsky (24:18):
Wow. I feel like you're a stand-up comedian/product manager. That was incredible.

(24:28):
This episode is brought to you by Vanta. When it comes to ensuring your company has top-notch security practices, things get complicated fast. Now, you can assess risk, secure the trust of your customers, and automate compliance for SOC 2, ISO 27001, HIPAA, and more with a single platform, Vanta. Vanta's market-leading trust management platform helps you continuously monitor compliance, alongside reporting and tracking risk. Plus, you can save hours by completing security questionnaires with Vanta AI.

(25:02):
Join thousands of global companies that use Vanta to automate evidence collection, unify risk management, and streamline security reviews. Get $1,000 off Vanta when you go to vanta.com/lenny. That's vanta.com/lenny.

(25:21):
I know it's Spotify, I heard one of their core values is, "Talk is cheap," but it's the virtue version of that. It's like, they actually prefer to talk more, and I think that's exactly what you're saying. Basically, spend more time on these things that seemingly seem just small little ideas and experiments.

Shreyas Doshi (25:37):
Yep, thinking is cheap, so you should do more thinking, not less.

Lenny Rachitsky (25:41):
Amazing. Shreyas, what's your second question?

Shreyas Doshi (25:45):
Yes. So my second question, I have to get the words right, do I actually have good taste? Do I actually have good taste is my second question. And for me, I asked this question after, again, all of these things... By the way, everything I say, I have been that guy, I've made that mistake. So that's why I just have to admit to myself that, yes, I have made these mistakes. And one of the mistakes I made, this was when I was at Google, and I was relatively new to product, about less than five years. And at Google, there's some parts of Google where you would be told as an early career PM that like, "We don't do strategy here. Strategy is for MBAs, okay? We are all about execution, okay?"

(26:47):
So I'm in this environment, I'm naïve, and I look around me and I'm like, "Google is the most successful company on the planet," at the time, and they are saying this, and I'm hearing this consistently, so it must be right. It must be right. And so I start saying it. I start saying, "Oh, yeah, execution is everything, and we don't do strategy around here." And I even remember there were not that many PMs, but there was a PM at Google who was kind of like the same level as me, but he just had much more wisdom than me. And he was trying to nudge me into... Like, I was managing a product. And he's like, "Shreyas, what is your strategy here?" And I was like, I told him the same thing. Like, "Oh, no, no, what are you talking strategy? We don't need strategy. We just need to get shit done." That was the thing.

(27:42):
And so I kept repeating that mantra until I got to Twitter. So this is Twitter right after their IPO. And I saw Twitter had an incredible asset, which is the product and the network effects. It had other incredible assets, including the brand. It had other assets that were great, including the talent. And yet, this company was struggling, the product was struggling. And even if it wasn't struggling, it was making a lot of money. But the point is it was not meeting its potential. So that's when I realized, and it wasn't like some sudden realization, it took me six to nine months of being at Twitter. This is circa 2014. That's when I realized that, "Oh, my gosh, Twitter's biggest problem is a product strategy problem. The reason they're struggling is they don't have a real product strategy."

(28:38):
Now, of course, attempts were made to create a product strategy, but it wasn't a real compelling, cohesive product strategy. So that's when I realized the folly of like, "Oh, wait a minute..." I was at Google six years. I spent most of those six years saying like, "Ah, strategy's useless. There's no point to strategy. Execution is where it's at. "I'm like, "No, actually, I was wrong." And that got further solidified as I went to Stripe, and I was kind of now growing earlier stage products and trying to make them highly, highly successful. I saw an even greater value and importance of having a clear strategy. And so that made me realize, basically...

(29:29):
You know, we talk about taste, we all talk about taste, and it's about the beautiful pixels, and the perfect product, and the whatever else, the Steve Jobs-esque passion, and all of that, whatever it is. And yes, taste is about that, but I think there is something that we as product leaders, and certainly I did, needed to recognize about taste as just a factor in pretty much everything we do, which is like, do we have good taste around the beliefs we choose to create within ourselves as product leaders? And then, those beliefs end up dictating everything we do, including how we manage, how we lead, how we make decisions. And so it's that taste I'm talking about when I say, "Do I really have good taste?"

(30:28):
And when I asked myself this question, and again, I really had to dig deep. It wasn't easy, but at some point I realized that no, actually, I don't have good taste. I don't have good taste in how I choose to evaluate things that come my way. Again, not in terms of the product, because by that time I had skills to say, "Well, this should not be a two-step flow. This should be a three-step flow," whatever the case may be. But I still did not have good taste in terms of how I choose what are the things I choose to believe, how do I learn, who do I learn from, what content I learned from, what content I resonate with? And then, I went on this journey to try to develop that better taste.

Lenny Rachitsky (31:21):
What I'm hearing is people focus maybe too much on the output, like the experience, these experience design taste versus what they choose to take in as informing their taste, and what they see as an example of great and correct. Is that what you're saying?

Shreyas Doshi (31:37):
Yeah. And look, taste is about the ability to identify what is really good, without needing to see its results, because, look, it requires zero taste right now for anybody to say, "Oh, that CEO of NVIDIA is a genius, right? Jensen is a genius." If you are saying that in 2024, it actually requires zero taste, because you can just look up NVIDIA stock price. It requires zero skill. But to be able to say that in 2010, you have to realize Jensen Huang didn't change much between 2010 and 2024. So Lenny, even in sports, there's this saying, "Game recognize game," and that's about taste.

(32:59):
But what we need to understand is it's game recognize game before the game is called, right? Like, game recognize game in the practice session. Because it takes no genius right now to say, "Well, Patrick Mahomes is great quarterback," or, "Virat Kohli is a great cricketer," or whatever else. It requires no genius to do that. It requires zero taste. So I also believe some of us, especially as we get more senior, and we get more successful, and we just get a lot more scope, and responsibility, and a lot of accolades, we become these tough graders. Like, "I don't like anything," right? Like, "Ah, this is crap, this is crap, this is crap." Again, that requires zero taste. Anybody can say that. Anybody can just say, "Everything is horrible."

(33:53):
So I do think there is something about being able to understand that, and I think I'll share some examples. This two-way door thing, so let me just share a few observations, if I might. So the first one is we get overly excited about cool metaphors, okay? Like, one-way door, two-way doors. There's some guy, I don't know who it is, I just read somewhere, there's some guy who had written a blog post about this idea, but he called it reversible and irreversible decisions, and it was the same idea. And I think somebody was lamenting that that did not catch on, reversible and irreversible decisions. But what caught on is two-way door and one-way door. What's the difference? The only difference is you got attracted to the catchy metaphor, and the other one is the authority bias, because Bezos said it.

(34:58):
Take another example, we get very impressed with alliterations. I'm serious, we get very impressed with alliterations. Okay, so how many of us love fail fast? Fail fast, okay, nobody's going to raise hands now. Okay, fine. Maybe you truly don't love fail fast. How about fast follow? How many of you love fast follow? Let's consider that, like fail fast, "We're going to fail fast." What if that thing were called fail quickly? It's the same meaning. Do you think you would be as attracted to that idea if it were called fail quickly?

Lenny Rachitsky (35:44):
No.

Shreyas Doshi (35:46):
Probably not, so what changed? The only thing that changed is one is an alliteration. So I see this in everything. Like, let's see, the other one is we also get very impressed with complicated charts and math we don't understand. And some of you product leaders who are at the top of the game, you actually use this as a strategy. So as I realized that here's the outcome of that, asking myself that question, was that what I realized is everybody says, "Oh, I'm a first principle thinker. I am a rigorous thinker," whatever.

(36:41):
But I realized that if I really want to be that, I have to shed a lot of these just patterns that were just built in me, and I kind of have to evaluate the idea separate from all of its social proof, and authority proof, and whatever else. And that ended up being a meaningful change in my growth as a product leader, because the moment I started shedding these kinds of social proofs, and authority proofs, and all of that, it just made me a much... We all again think we are critical thinkers, but we are not, right? So it made me a more critical thinker.

Lenny Rachitsky (37:31):
I want to move on to the next question, just so we can get through some of these questions. Before I do, can you just show people your notes real quick, just like show it from a distance? This is how Shreyas plans for something like this. There's color coding, I wish I understood what was going on there.

Shreyas Doshi (37:49):
People ask me, "What's your favorite note-taking app?" It's a common question I get, and I say this, right? It's a $5... Like I guess the pen costs $3, and I think the Office Depot clipboard costs-

Lenny Rachitsky (38:00):
Wait, doesn't that pen... Does it have the different color clicky thing?

Shreyas Doshi (38:02):
Yes, yes, yeah. A [inaudible 00:38:04]. Yeah, exactly. This is great.

Lenny Rachitsky (38:06):
That's going to be another podcast episode. Okay, so we want to try to do two more questions. We have six minutes left. The last one's a bonus, so maybe we touch on it briefly. Shreyas, what's your third question?

Shreyas Doshi (38:18):
So my third question is why does my job feel so frustrating? Why does my job feel so frustrating? And it goes back to the point that, look, I loved, loved my PM leadership job. I just absolutely loved it. And I think looking back, I would not have exchanged it for anything else, any other experience. That said, there were daily frustrations. There were daily frustrations in that job, and a lot of it has to do with the fact that the PM leader's job is extremely lonely. The PM's job, the PM's on your team, their job is also lonely. But a PM leader's job is further lonelier. So there's that. There's also what I learned at the time when I started asking this question is that our jobs get frustrating when we behave, most of the time, in misalignment with our superpowers and who we truly are at our core.

(39:32):
Okay, so for me, as I was evaluating that question, it's like, "Why am I getting frustrated every day? I love the job, I love the macro, but I do not like the micro. And so why is that?" And that's when I actually... There's a simple framework that I've shared, which is you can be doing your work at three levels. Product work happens at three levels. There's the impact level, there's the execution level, and there's the optics level. My epiphany as I was exploring this question was I have a preferred level at which I like to operate, but if most of the day, and most of the week, and most of the month, I am forcing myself to operate in not my happy place, in my non-default level, that makes me very frustrated.

(40:33):
So many product leaders, their happy place is the execution level. In my case, my happy place is the impact level. So that is fine. Your happy place can be whatever level, it doesn't matter. But the point is, as you go higher up in the corporate ladder, no matter what kind of company it is, you are now going to have to spend a lot of time on optics, at the optics level. And I have willpower, I have the skills to do it, I have all of that. So it's not about willpower or skills, but willpower is finite. So as I spent day in and day out, just mostly doing optics work, I realized I was not happy and I was getting frustrated. And so that's when I realized the solution, which is I have to abandon the traditional path, that like, "Oh, after this level, I'm supposed to do this, and then I'm supposed to do this, and then this is what society expects. This is what my mom expects. What will people say on LinkedIn when they see my LinkedIn profile?" Like, "Oh, he has this progression, this, and then what stopped? Why did it stop?"

(41:49):
So when I realized this, I said, when a team grew to a certain size, so when I was at Stripe and I realized this, when the team I was managing, it had a fan out of about 50 people, so this includes engineers and everything, I said, "This is enough." Because for me, any time a team goes to like 50s, and 100s, and beyond, it is a law of corporations that you're going to have to spend a lot of time at the optics level. So instead of just pushing, pushing through against who I truly am, what did I do? I just went back to more of an earlier stage product, and then I was fine with like, "You know what? I'm not going to just play the corporate game," as an example.

(42:34):
So I guess my suggestion would be identify your superpowers, and like Shakespeare said, "To thine own self be true." Just be honest to yourself. Operate your career and make your career decisions not out of expectation, not out of envy, like the LinkedIn envy of like, "Oh, this person is at a different level. We both went to the same grad school, so I got..." No, identify your superpowers, because if you identify your superpowers and work in accordance with them, you will do the best work of your life. You will love it, and you will be great at it, and you won't have that frustration.

Lenny Rachitsky (43:23):
I wish we had an hour for every single one of these questions, I feel like there's so much more to get into. We have 40 seconds. Do you want to touch on your last question or do you want to leave that for a follow-up discussion?

Shreyas Doshi (43:35):
Let's touch on it, let's touch on it.

Lenny Rachitsky (43:37):
Okay, we got to go though in 30 seconds.

Shreyas Doshi (43:38):
All right. My last question is am I really listening? Okay, and this is perhaps the hardest one for me, because I thought, of course I'm a good listener because I listen, then I recap, and I make eye contact, and I tell them, "This is what I heard," And all of that nonsense. I realized there is an entirely other level to listening, which once you understand that there's an entirely other level to listening, that is what enables you to be a world-class leader. And so that is what I guess my last takeaway is, is ask yourself, "Am I really listening?" If you want resources, there are very few people who actually talk about what that real listening means. I would refer you to what Rick Rubin says about listening, I would refer you to what [inaudible 00:44:30] said about listening, and what Drucker said about listening as some pointers.

Lenny Rachitsky (44:36):
Amazing. Shreyas, you said you were going to hang out for the next hour somewhere. You want to share that real quick, and then we'll get off?

Shreyas Doshi (44:42):
Yes, I will maybe try to hang out in the back part of the room.

Lenny Rachitsky (44:46):
Be quiet back there, too.

Shreyas Doshi (44:47):
Yes.

Lenny Rachitsky (44:47):
Shreyas, thank you so much for being here.

Shreyas Doshi (44:50):
Great, thank you. Oh, should we take a picture?

Lenny Rachitsky (44:54):
Oh, yeah. We're going to take a quick selfie.

Shreyas Doshi (44:59):
We're going to take a picture.

Lenny Rachitsky (44:59):
There we go. They're going to turn lights on, I think.

Shreyas Doshi (44:59):
Oh.

Lenny Rachitsky (45:00):
Okay.

Shreyas Doshi (45:03):
All right, folks.

_[3 additional lines trimmed for context budget]_

---

### The things engineers are desperate for PMs to understand | Camille Fournier (“The Manager’s Path”)
**Guest:** Camille Fournier | **Date:** 2024-09-15 | [YouTube](https://www.youtube.com/watch?v=hZSh0rs20uI)  

# The things engineers are desperate for PMs to understand | Camille Fournier (“The Manager’s Path”)

## Transcript

Lenny Rachitsky (00:00:00):
I'm curious what it is that PMs do that annoy engineers most.

Camille Fournier (00:00:04):
Hoarding credit. PMs, they tend to be the front-facing person for initiative. Engineers sometimes think that they don't get the credit for their work because the PM takes all the glory and all the credit for the project that they really worked very hard on.

Lenny Rachitsky (00:00:19):
I find the best PMs are the ones that talk the least and encourage other people to do the presenting-

Camille Fournier (00:00:23):
The next thing that engineers really get annoyed about with PMs, when they just don't understand the details and act like they don't matter, it just shows a real lack of empathy for the work that engineers are doing and I think it really can be very off-putting.

Lenny Rachitsky (00:00:34):
Is there any insight you can give about what people may be missed about the motivation of engineers, what gets them excited?

Camille Fournier (00:00:40):
A lot of people assume that engineers just write code and don't underestimate the ability for your engineers to want to understand the business problem, want to understand the customer problem. I think the product managers that have done the best, they're not threatened by other people having ideas.

Lenny Rachitsky (00:01:00):
Today, my guest is Camille Fournier. Camille is one of the most respected technology executives in tech and the author of the Manager's Path, which many considered the definitive guide for navigating your career and moving into management. Over the course of her career, she was CTO of Rent The Runway, VP of technology at Goldman Sachs, global head of engineering and architecture at JP Morgan Chase and head of platform engineering at Two Sigma. She's also releasing a new book later this year called Platform Engineering, A Guide for Technical Product and People Leaders, which you can actually pre-order today and we get into this topic in the latter half of the conversation.

(00:01:36):
We also dig into what PMs do that most annoys engineers and how to stop doing these things. Why major rewrites are often a trap. Why you may want to be doing fewer one-on-ones. What most surprises people when they become a manager and some really useful heuristics for how long you should stay in IC before you make the leap into management and tons more. This episode covers a lot of ground and we'll help you think about management. platform teams, team culture and the PM and end relationship in a whole new way. If you enjoy this podcast, don't forget to subscribe and follow it in your favorite podcasting app or YouTube.

(00:02:09):
It's the best way to avoid missing future episodes and helps the podcast tremendously. With that, I bring you Camille Fournier. Camille, thank you so much for being here, welcome to the podcast.

Camille Fournier (00:02:23):
Thank you so much for having me.

Lenny Rachitsky (00:02:25):
It's my pleasure. I want to start by asking you a question that is on the minds of a lot of product managers is how to be less annoying as a product manager. I know you worked with a lot of engineers over time. I'm curious what it is that PMs do that annoy engineers most and how can PMs stop doing that?

Camille Fournier (00:02:42):
I would say there are a few things that PMs do that annoy engineers and to be clear, I am sure that engineers annoy PMs, just as much. So I've realized this is a two-way street. So I think there's some things that are really easy to fix and some things that are maybe a little bit harder. So the easy things to fix are hoarding credit. Sometimes I think PMs because they tend to be the front-facing person for initiatives, they're talking to customers, they're talking to the executive team, whatever. Engineers sometimes think that they don't get the credit for their work because the PM takes all the glory and all the credit for the project that they really worked very hard on, right?

(00:03:26):
So making every effort to be credit sharing and inclusive of the engineering team and giving them the opportunity to speak about their contributions when it makes sense. I think those are all things that PMs can do to avoid that kind of ... What I consider a pretty easy annoyance, just like don't pour it all the credit. This is not just you, right? There's a lot of work has to go into that. I think that sort of dovetails into the next thing that engineers really get annoyed about with PMs when they just don't understand the details and act like they don't matter and I think this is just a little bit of a cultural difference.

(00:04:05):
I mean, even managers, just normal managers, people like me who are looking across really broad areas or you have to be kind of big picture focused, and you forget that engineering done successfully really is all about the details and you don't necessarily have to understand all of those details, but when you act like they don't matter and you don't care about them and it's just like, I don't care, just like tell me when you can get this done or why is it going to take so long? My god, this just seems like such a little thing. It just shows a real lack of empathy for the work that engineers are doing and I think it really can be very off-putting.

(00:04:42):
Even though I will totally agree that sometimes you're going to get details that don't really matter and you just have to be a little bit patient in those circumstances.

Lenny Rachitsky (00:04:50):
Today's episode is brought to you by DX. If you're an engineering leader or on a platform team, at some point your CEO will inevitably ask you for productivity metrics, but measuring engineering organizations is hard and we can all agree that simple metrics like the number of PRs or commits doesn't tell the full story. That's where DX comes in. DX is an engineering intelligence solution designed by leading researchers, including those behind the DORA and SPACE frameworks. It combines quantitative data from developer tools with qualitative feedback from developers to give you a complete view of engineering productivity and the factors affecting it.

(00:05:28):
Learn why some of the world's most iconic companies like Etsy, Dropbox, Twilio, Vercel and Webflow rely on DX. Visit DX's website at getdx.com/lenny. Let me tell you about CommandBar. If you're like me and most users I've built product for, you probably find those little in-product pop-ups really annoying. Want to take a tour? Check out this new feature, and these pop-ups are becoming less and less effective since most users don't read what they say. They just want to close them as soon as possible, but every product builder knows that users need help to learn the ins and outs of your product. We use so many products every day and we can't possibly know the ins and outs of everyone.

(00:06:09):
CommandBar is an AI-powered toolkit for product, growth, marketing and customer teams to help users get the most out of your product, without annoying them. They use AI to get closer to user intent, so they have search and chat products that let users describe what they're trying to do in their own words and then see personalized results like customer walkthroughs or actions, and they do pop-ups too, but their nudges are based on in-product behaviors like confusion or intent classification, which makes them much less annoying and much more impactful. This works for web apps, mobile apps and websites.

(00:06:42):
And they work with industry-leading companies like Gusto, Freshworks, HashiCorp and LaunchDarkly. Over 15 million end-users have interacted with CommandBar. To try out CommandBar, you can sign up at commandbar.com/lenny and you can unlock an extra 1000 AI responses per month for any plan. That's commandbar.com/lenny. By the way, CommandBar just changed their name to Command AI.

Camille Fournier (00:07:08):
The third is playing telephone. Anybody in a manager role can fall victim to this, but I think PMs especially can be very annoying. So if you are being asked questions that you cannot answer because you just don't know or because that's something that involves a level of technical detail that only the engineers have that you just don't have, and you put yourself in this in-between position where people ask you questions, you turn around, you ask the engineers questions, you take whatever they say, especially when you don't really understand it, which happens sometimes, right? Go back to the original asker and sort of get in this middle-person scenario.

(00:07:51):
I think that is very annoying and frankly, it's a waste of time for everyone. This is something that managers of all stripes do, but PMs definitely do it and that drives engineers, particularly senior engineers on projects, it's crazy. And then, the last one that I wanted to put on this list is just when sometimes it feels like product managers want to hoard all the ideas for themselves, right? They want to be the ones that come up with every single sort of product idea and every single detail. What I see happen in those cases is that I see engineers start to over-engineer things, because engineers are like, well, I need to take control of something.

(00:08:30):
I want to have some creative outlet, so I'm going to use my engineering skills as my creative outlet and I'm going to spend a lot of time obsessing over the right framework or the right this, that or the other. That may actually not matter that much with products delivery, but when you take the people that are part of the project team out of the creative loop entirely, they're going to find that creative outlet somewhere else and it's actually kind of bad for the product.

Lenny Rachitsky (00:08:56):
That's really interesting, the last one. So you're saying if you keep engineers from having a voice in what you're building and prioritizing, that's what encourages engineers to rethink, let's just rebuild this thing, let's use a new framework, let's rewrite this system.

Camille Fournier (00:09:09):
Yeah, I mean that's my ... that happens without you doing that, sometimes.

Lenny Rachitsky (00:09:10):
Yeah.

Camille Fournier (00:09:18):
I do think ... I think when I see it worst and I can basically always predict what I'm going to see, a lot of that kind of engineers building stuff, finding creative outlets and kind of building stuff, maybe they shouldn't be. I'm going to find that in places where they are so quashed their creativity for the actual business or product that they're building and their voice in that is so ignored that they don't have any outlets in that space and so, they are going to use the space that they have an outlet in, the place where they have some control and that's usually the technology choices and the details there.

Lenny Rachitsky (00:09:54):
That's fascinating. I want to actually dig further into that around rewrites. You have a really interesting take on that, but before we get there, let me spend a little time on some of these. These are awesome. So on this theme of not involving engineers in ideation and coming up with what you're actually building, what have you seen just very tactically, is there anything you've seen that PM do super well?

Camille Fournier (00:10:17):
I think the product managers that have done the best, they're not threatened by other people having ideas. They're not threatened by the engineering team being full of smart people because they realize that yeah, some of the engineers may have good ideas, but they still don't really know how to do the product job. Just my experience is there are plenty of engineers who actually think they can be product managers and they don't really understand all of the elements of the product job that they would need to be successful. And when product managers take the time to build those relationships, well, make sure that people do feel like they can both share their ideas, but also that they start to appreciate what the job of this product person actually is.

(00:11:02):
And what they're really bringing to the table in terms of really how do we measure this input? How do we really understand the customers, how do we really think through the details of what's going to make this successful from a business or a customer perspective? I do think that that creates just a much better interaction pattern and then, engineers can feel good about sharing ideas and understanding that many of them won't go anywhere, but there's somebody that's actually going to listen and take the time to care about them.

Lenny Rachitsky (00:11:37):
Coming back to some of the things that you said annoy engineers of PMs, this idea of playing the middle person, sounds like the solution clearly is there. Just connect the engineer to the other engineer or your engineer to the PM that's trying to figure this out, right?

Camille Fournier (00:11:49):
That one is not always easy because again, you have this tension of a lot of the job, of any management role is being in meetings and filtering stuff so that people who are in focus ... individual contributor mode can focus and get things done and not spend half of their weekend in meetings. You've just got to be very careful about knowing when you're crossing that line and when you're crossing it too often. And if you're having to often say, "Let me get back to you, let me get back to you, I don't know, let me get back to you." Maybe the person you're talking to is asking the wrong level of questions of you.

(00:12:26):
Maybe you need to connect them to the engineers directly, but just being aware that that shouldn't be a default behavior. That will happen occasionally, but it shouldn't be a thing that happens a lot because if it's happening a lot, then you're likely missing something, you're likely losing something in that telephone game translation and that's going to cause problems over time.

Lenny Rachitsky (00:12:47):
Awesome. So basically, if you're just finding your middle person too much, then it may be time to connect people directly. And I know the reason PMs often are afraid of this is the engineer may agree to something that they think is a bad idea for their team or may not understand all the ramifications on the product or just obviously, just spend their time in meetings and not be building anything.

Camille Fournier (00:13:12):
Yeah, yeah, and sometimes you mean you do it in a group meeting. I feel like Slack and other chat type things actually make it a lot easier to see, have the right people in a group in a thing, but again, that's distracting. So there's not an easy solution to that one, just I think it's important to be aware of it.

Lenny Rachitsky (00:13:27):
Yeah, that's a really good point. And then, in terms of hoarding credit, is there any tactical thing you've seen PMs do really well here is it, just every time they're announcing the product, "Hey, these engineers were involved or-"

Camille Fournier (00:13:37):
It's more than just saying thank you to all these people, but it's actually sometimes stepping back and letting other people speak, especially if it's something that's a really, really big technical lift. I don't think there's a super easy fix to that. I think it is just really being mindful that that can be very much a sore point for engineers when they just feel like, "This is my work, I'm not getting any credit for it, and this person is hogging all the glory."

Lenny Rachitsky (00:14:07):
I love that. Yeah, I find the best PMs are the ones that talk the least and encourage other people to do the presenting and announcing. And so, I think that's a really good reminder is let your engineers do that. Okay, amazing. So we talked a little bit about this idea of rewriting and how engineers sometimes just want to rewrite the system and I think a lot of PMs do too. A lot of times you're building your features on a thing that someone that doesn't even work at the company and were built five, 10 years ago, and there's always this sense of, "Okay, maybe we should just rewrite this thing everything will move so much faster."

(00:14:39):
Do you have a really interesting take that I think a lot of PMs will love to hear, which is that rewrites are often a big trap and often don't end up being what you think they might be? Can you just talk about your experience and perspective on this?

Camille Fournier (00:14:50):
Yeah, so I mean I have personally overseen a number of, if not quite rewrites, re-architectures and major system evolutions. So I absolutely do think they are sometimes a thing that needs to happen, but I also, have seen so many instances of cases where the engineers have convinced themselves that the only solution to the woes that they're experiencing with the system, it's hard to support, it's hard to change. Nobody wants to work on it, because it's this old crappy technology, is that they just have to go over to the side, build the new thing that will replace this old system and that is going to sort of free them from their misery.

(00:15:41):
And I think projects where you acknowledge that you do need to do an uplift, but you make a very thoughtful staged plan as to how you're going to do that, and you really think through, okay, we don't need to touch all of this stuff, but we're going to take the recommendation system, it really needs to be uplifted and that's a well-contained, you know, API and so we can start to fix that without having to change the whole whatever web framework, right? So I do think there are ways to do these evolutions, but people really underestimate. They underestimate the time to migrate stuff from the old system to the new system, is a huge, huge problem.

(00:16:24):
Particularly when you're talking about systems where you have sort of external people using the system in some way, whether it's web UIs or APIs. You think, "Oh, we kind of know what's going on, so it's not going to be that big a deal." Engineers notoriously, notoriously, notoriously, massively underestimate the migration time for old system to new system and that causes a lot of problems. By the way, you still have to support the old system while you're working on the new system. So I doubt many of the PMs in the audience are ever happy when they hear, we need to go away for six months, a year, two years to build this new thing and we just can't really add any features to the system in the interim.

(00:17:08):
That's infuriating, I'm sure, and frankly that's a problem and I don't know that that should be an acceptable answer in many cases. There may occasionally again be a case where that is what has to happen, but I think most of the time you can't really afford to just say we're going to go away and we're not going to touch the system for a long time and we're going to build something new over here. So many things about why that doesn't make any sense. So this is a little bit of field of the blog post that you're referring to. So, if you've got a system that doesn't really need feature enhancement or development because it's just sort of fine and the users are using it.

(00:17:47):
And it's just annoying to the engineers, why in the world would you invest so much money in writing a new version of it? There's a little bit of a cognitive dissonance that sometimes happens if you need to do new stuff and the old system literally is not ... it's not possible to do the new stuff that you need to do, you need to figure out a path to get to a sustainable system or you can continue to add and evolve. You should be investing and so there does need to be an investment, but you have to ask yourself, if I could go away and not touch this and not do anything to it for a long period of time without it really harming my business, is it worthwhile to change it at all?

(00:18:28):
Does it matter, and there's some questions there. I also think that when people try to do rewrites, particularly again if it's something that you're really trying to just move to a new language for example or sort of modernize in a certain way, [inaudible 00:18:45] a lot of times people really underestimate what the old system does and how well they know what the old system does. There's so much logic buried in legacy systems, it tends to be undocumented, it tends to be weird. You haven't thought through all the business rules, you haven't thought through the data formatting and I think again, it's much, much harder to replicate all the important things from the old system to the new system than people expect.

(00:19:14):
So there's more than that, but I do think sometimes you need to evolve systems and my advice would be when you're struggling making an evolution plan, take pieces potentially of the old system, uplift them, make them more scalable, make them easier to work with, clean up the tech debt, but trying to say we're going to just go away. We're going to rewrite, we're going to build something brand new and it's going to solve all our problems, it just very rarely works.

Lenny Rachitsky (00:19:42):
I think a lot of PMs will be like, "Yes, thank you so much for saying this, because I think that's also always a big struggle between the ENG team and the PM team." So just to summarize what I think a lot of people miss or what you're saying a lot of people miss when they're thinking about let's rewrite this thing, is the migration to the new system, migrating customers, users, data to the new thing is going to take a lot longer than you expect. You underestimate knowing what it actually does and you're going to miss features and you can introduce new bugs. This actually is very similar to what I've seen with redesigning a whole product flow.

(00:20:15):
There's always this sense of let's just rethink this onboarding flow from scratch or let's rebuild this part of the product and always it ends up being a negative experiment result. It always ends up being less good and then, you have to spend all this time clawing back to get to where you were and also, you forget the stuff that would ... the features that you had and you're like, "Oh, shit, I forgot about that feature, I forgot about that feature." So it's interesting, there's a very similar situation in the product side. Okay, amazing. I'm going to go to a slightly different topic, which is around engineering leadership.

(00:20:45):
So I know you've written a lot about engineering leadership, you spent a lot of times with engineering leaders, so I have a few questions here. One is that I know that one of the things that haunts engineering leaders most is finding the balance between staying technical and their technical expertise, and their leadership expertise and basically finding the right altitude of how high ... where to be in the org and also how in the details to be, and also staying technical enough to be relevant. What have you learned in your own experience of finding that balance and how do you advise engineering leaders as they struggle with this?

Camille Fournier (00:21:18):
Yeah, so one piece of advice I give everybody is don't stop being a hands-on technical until you feel like it's in your bones. You feel like you've got mastery that you could ... if you know a second language fluently or if you played an instrument really, really seriously for a long time or maybe a sport really, really seriously for a long time, you'll be familiar with the ... I haven't done that in a long time, but if I was to pick it up, it would be rusty, but I would get there pretty quickly, right? Maybe physically, I wouldn't be as strong as I was or whatever, but I would get there. You can do that with writing code.

(00:21:58):
You can do that with technical skills if you do it for long enough, I think you can develop sort of a baseline mastery, where you're not going to be as fast and a lot of the challenges of being technical is actually in all the tooling and all the tooling evolution, but you're not going to be necessarily as fast as people who have been doing it, but you won't be completely clueless and I think that all the things you learn getting to that really comfortable mastery of some part of hands-on tech will stay with you and will help you just maintain a level of confidence in your own technical know-how and maintain a level of empathy for what it means to be a good engineer.

(00:22:38):
And I think just make you a lot less anxious about being hands-off, even though I think everybody who makes that transition for a year or two, especially if you're really have to be hands-off or you just don't have time to write code at all, you are going to be anxious for a while no matter what. The things to then think about from that point is, being technical is also just about knowing what's going on and paying attention and being able to ask ... what people care about with technical leaders in my experience is they want people who actually seem like they sort of understand what you're doing and can ask good questions and help guide you to better decisions without actually being the one who's like, "Oh no, you need to use this library instead of that library."

(00:23:25):
It's actually sort of annoying when somebody that's very senior and hands-off tries to tell you, don't use this library, use that library because I don't know about you, but I don't really believe people who have been hands-off for that long when they try to tell me what to do and the thing that I'm kind of the expert in right now, but I do appreciate it when I'm given more guidance around, well, have you considered this? Tell me about how you're planning to handle that situation. What are the major technical challenges with implementing this and that can actually spend the time to listen and ask thoughtful questions on the back of that.

(00:24:00):
The last thing I would say is surround yourself with smart technical people, also as much as you can, and be willing to listen to them, talk about tech and ask them questions about things. I feel like that's part of the reason that I am able to stay technically savvy and credible amongst people who work for me is not that I'm writing code because I'm not. But I am listening to a lot of very smart people talk about technology a lot, down to the level of I'm trying to debug this database issue, what the heck is going on? And just constantly being interested in those stories and learning from them and learning what really smart engineers are thinking about and worrying about.

(00:24:47):
The more you can build that network of people that are still hands-on and stay in touch with that, I do think that helps a lot.

Lenny Rachitsky (00:24:56):
So on my last point, how are you actually doing that? Is it watching ... going to conferences with friends, something else?

Camille Fournier (00:25:01):
Yeah, yeah. I mean, I guess for me it started with going to conferences, meeting people. Now, I'm in a lot of different chat groups where people are just sort of regularly communicating, staying in touch with ... staying in touch with former colleagues. I will admit I'm kind of a social person and I have a big network, so this may be easier said than done, but I do think being in the right group chats ... I also think, I'm sure reading various tech news and tech sort of commentary and discussion boards, I mean it's definitely a mixed bag of that stuff I think that like ... but there are smart people. You find the smart people in there, you sort of follow what they're saying. I think that's another good way to keep that perspective.

Lenny Rachitsky (00:25:51):
Is it a sign, I wonder if you're not interested in that anymore that maybe you should move into something else. I don't know. If you're like, don't really pay attention to engineering technicalness.

Camille Fournier (00:26:02):
It's hard for me to say because I'm such a nerd. I really love tech. I'm in this industry, because I'm just actually genuinely very interested in certain corners, not every corner of technology but certain corners of technology. I want to know what the latest stuff that's happening in databases and infrastructure and I find it all very interesting. I find the problems interesting. So I think that makes me very successful because I just have that natural curiosity and passion and interest in it. But I don't know that that's a total prerequisite.

Lenny Rachitsky (00:26:36):
Yeah, but you've been talking about ... it reminds me a little bit of this guy, LevelsIO on Twitter. Have you heard of this guy? He was just on Lex Friedman. Have you listened to that yet?

Camille Fournier (00:26:47):
I think maybe I saw a clip of it, but I haven't listened to it.

Lenny Rachitsky (00:26:48):
So I think one of the most successful indie engineers where he just works on his own thing all by himself, never raises money, just launch his products that make money. And a funny thing about him is he works ... all stuff is in PHP and jQuery. He's just like, this works. I've always had this to do, learn Node.js, learn Python. And I'm like, I'm too busy to build ... while I'm building to learn these new things and he's been incredibly successful. So it touches on sometimes maybe you don't need to just keep rewriting to the newest frameworks.

Camille Fournier (00:27:15):
Yeah, no, I mean look, I actually ... I feel like I know a lot of smart engineers who are in that category. They're like, we built amazing things in PHP and relatively simple SQL and so much of tech is over-engineering things and I don't totally disagree. I think the challenge is though, of course, what works as a one person show, doesn't always work in a scaled organization for better or for worse, we're in different like, you've got to match what makes one person really productive. Will it make 100 people, 1000 people, even 10 people really productive? That's always a little hard to tell and that's why I do think you should be not ... I think it's always a good idea to be keeping up with what's happening and what's changing in whatever kind of side of tech you're in, but not obsessively chasing every fad.

(00:28:15):
I think being aware of, but not necessarily chasing them, but particularly, if you're working in groups, teams, larger companies, even midsize companies, there is some amount of, you're balancing the tech that makes one person go fast with the tech that makes 10 or 100 people go fast and those are not always exactly the same thing.

Lenny Rachitsky (00:28:35):
Just to kind follow this thread a little bit and to kind of nerd snipe you a little bit, is there a platform or language or framework these days that you're either very excited about that you think is helping people move faster and do better work or the opposite just like this is ... everyone is excited but this is not good.

Camille Fournier (00:28:52):
I will say this, one example I actually put in my own notes for this conversation was GraphQL. I would not tell a team to use or not use GraphQL at this point because it's a bit out of my expertise zone and my level of management. It's not really my job anyway, but it is one of the things that is both popular and thought relatively poorly of by most of the senior people that I know. And so, I guess I would say that's one where I would say if you're seriously thinking about it and you're not Facebook, you may really want to make sure you know what problem you're trying to solve because the impression that I have from sort of listening to people talk about it is that GraphQL is kind of trying to promise front-end engineers that they don't really have to collaborate with backend engineers.

(00:29:49):
And they can just sort of build whatever and it'll all be fine, and it just doesn't ever seem to work out that well for anybody who actually does it in practice. Again, obviously, it can work out that well because Facebook has made a great go of it. I'm sure there are other companies that are, but that's one where it's not that new but it remains one of these things where it seems like an interesting fad that maybe is burning a lot of people.

Lenny Rachitsky (00:30:13):
That's awesome. I appreciate you sharing that. I don't know if this is exactly an example of this, but on that podcast levels, I forget his actual name, Peter I think, shared that whenever there's a framework that has a VC funded startup behind it, that's not a good sign because their job now is to convince engineers to use it and then pay for it and that's not necessarily going to be the best product.

Camille Fournier (00:30:34):
That's true. Although I will say that some of the most time-wasting frameworks have also just come out of big companies, or the context of the big company that may have made that framework super useful within that context doesn't translate to startup or small company or even big company that doesn't have the rest of the context set, but I don't think ... He's not wrong. I also just think big companies share the blame on that one.

Lenny Rachitsky (00:31:05):
I guess we should all just come back to PHP and jQuery and be simple.

Camille Fournier (00:31:09):
Maybe.

Lenny Rachitsky (00:31:10):
Okay, so to close out this thread on engineering leaders, finding the right balance, just to summarize your advice here. One is get to mastery before and is your advice here, get to this point before you move into engineering.

Camille Fournier (00:31:23):
Engineering management. I will say part of this is also in particular, if you happen to be a woman or otherwise, underrepresented person in tech because people will tend to underestimate your technical abilities just unfortunately as a get-go. I also think it's particularly important to kind of develop that internal confidence in your abilities before you make this sort of scary leap, which is scary for everyone of have a ... if you have that mastery before you make the leap, I wish more people would do this because honestly, I do think there are a lot of people who never really gained the mastery. They go into management, they lose it and some of them are still perfectly good managers and look, there are good managers who were never technical to begin with.

(00:32:09):
I don't want to say that that's impossible. I just think that if you care about being technical, if you are technical now and you want to maintain that tech-savvy, don't just become a manager the first time somebody offers it to you. Make sure you've really spent your good time writing code.

Lenny Rachitsky (00:32:28):
Is there a number of years heuristic you think about or some way to tell you that maybe you've hit that point?

Camille Fournier (00:32:34):
I think this has been since disproven, but it was that 10,000 hours idea of mastery at some point. For me, it was like an undergraduate degree, a graduate degree, and four or five years of full-time work. So maybe I might be slow. I didn't start coding a lot in middle school like people might do now, but I felt like ... I took several years of hands-on work in a very intense undergraduate and graduate programs for me. So I do think it's probably somewhere in the 10-year range of really having spent a lot of your time over those years writing code and really understanding how to be a technical expert.

Lenny Rachitsky (00:33:17):
Got it. So essentially if you're thinking about moving into management as an engineer, you may want to wait until you've done it for 10 years in some form, which I think is a lot longer than a lot of people would've thought. And I imagine many people are not doing that. And then, in your experience not doing it as well, it could be-

Camille Fournier (00:33:35):
If you were programming a lot in high school and you got an undergraduate degree, so let's say you've got six years-

Lenny Rachitsky (00:33:42):
I see.

Camille Fournier (00:33:42):
You may only need four or five years of work, 40 hour a week writing code experience. I'm sure it depends on the company, but I do think when you see people that are 23, they are just out very recently out of school, it's a different thing if you're a founder and that's a whole different life, but if you're at a big company and somebody is like you have great communication skills, why don't you start to become a manager? Often they're actually pushing you to become a project manager, which is actually also the worst sort of path to real leadership in my opinion. If you don't feel like you're done ... Also, if you just don't feel like you're done, if you're still having fun writing code, don't rush becoming a manager, writing code is awesome. Have fun, enjoy it.

Lenny Rachitsky (00:34:29):
I know exactly what you mean. So I used to be an engineer, actually I was an engineer for 10 years. I definitely don't have mastery at this point. I moved into product from that and I definitely so missed actually just sitting there and writing code and building stuff, that was very hard to give up. I imagine you still missed that.

Camille Fournier (00:34:45):
I think I might've forgotten about it at this point, but there is nothing as satisfying because you get the fast feedback loop. It's just wonderful. Yeah.

Lenny Rachitsky (00:34:56):
This episode is brought to you by Coda. I use Coda every day to coordinate my podcasting and newsletter workflows from collecting questions for guests to storing all my research to managing my newsletter content calendar, Coda is my go-to app and has been for years. Coda combines the best of documents, spreadsheets and apps to help me get more done and Coda can help your team to stay aligned and ship faster by managing your planning cycle in just one location. Set and measure OKRs with full visibility across teams and stakeholders, map dependencies, create progress visualizations, and identify risk areas.

(00:35:31):
You can also access hundreds of pressure tested templates for everything from roadmap strategy, to final decision-making frameworks. See for yourself why companies like DoorDash, Figma and Qualtrics run on Coda. Take advantage of this special limited-time offer just for startups. Head over to coda.io/lenny and sign up to get six free months of the team plan. That's coda.io/lenny to sign up and get six months of the team plan, coda.io/lenny. On the topic of moving into management, you wrote maybe the definitive book on engineering manager career path, and so when someone moves from IC to management, what do you find is the most surprising thing to them? What do they most often not understand or are surprised by like, "Oh man, I did not see this as part of my job or my life."

Camille Fournier (00:36:24):
Yeah, I mean I think there's a few things. I do think ... assuming that they're actually trying to do it well, I do think there are a lot of people who move into management and then just don't really understand the job at all and aren't even self-aware enough to know that they don't understand it, but for those who are trying to do it, trying to do it well. I think a few things that tend to surprise them are the fact that you really don't own your time as a manager. Your team and your management and the company owns your time. More and more the more senior you become as a manager. I think individual contributors often think that if they become a manager, they will still have some of the freedom that they have as a senior individual contributor.

(00:37:08):
But then they'll also be able to tell people what to do and they'll have all this authority. And the reality is, management is much more ... I'm not a huge fan of servant leadership exactly, but management really is a service job. You are serving the team, you are serving the company. Your job is to help make things better and that usually doesn't mean that you're making all the decisions. It usually doesn't mean that you snap your fingers and people jump. Because if you try that, especially in tech, right? People are just going to revolt. They're not going to listen to you. It's just too hard to have ... that's not the culture that we live in.

(00:37:53):
And I don't think that's a good culture. I don't think that command and control, I tell you what to do and you do it. It just doesn't create creativity, right? It's the same thing as like PMs, trying to have all the ideas, right? No, you've got all these brilliant people working for you on a team, your job as a manager is not to tell them what to do in every single case. Occasionally, yes, a lot more. If you're trying to convince them of what you think should happen. In some ways, you're sort of nudging, you're encouraging, you're directing, you're setting guardrails for processes or behaviors or whatever, but it's not this glorious fearless leader.

(00:38:36):
I make all these decisions and everyone looks up to me and it's awesome kind of job. It's much more grueling, much more ... you are really just sort of reacting to things in the moment. And it can be very ... it's a hard job. I do think particularly management when done well, when you're really trying to do it in a thoughtful kind, but also, productive way is a very hard job.

Lenny Rachitsky (00:39:06):
I wonder if engineering is where most ... where the highest percentage of people that move into management move back to IC. I've seen that a bunch and I wonder if engineers are the most common.

Camille Fournier (00:39:16):
I don't know, but-

Lenny Rachitsky (00:39:18):
After realizing what you just said is true, like, "Oh, what I done-"

Camille Fournier (00:39:22):
So do you not think product managers also do this? Because I think product managers also actually suffer from exactly this kind of-

Lenny Rachitsky (00:39:28):
They do.

Camille Fournier (00:39:28):
Maybe sometimes-

Lenny Rachitsky (00:39:28):
They do.

Camille Fournier (00:39:28):
Yeah.

Lenny Rachitsky (00:39:30):
But interestingly, I was an engineering manager earlier in my career. I really did not like it. I was very unhappy in that role. As a PM manager though, I was very happy, it was a lot easier.

Camille Fournier (00:39:43):
Yeah, because PMs are way easier to manage. PMs are awesome to manage. PMs want to ... they're just so helpful. They want to do this ... they're good communicators. I love managing PMs. I have to say, I have to say, just my experience, engineers are such a pain. They're all primadonnas. I am an engineer, but PMs are more fun to manage my experience. So actually, you have a point. You have a point.

Lenny Rachitsky (00:40:12):
But I wonder. Yeah, because I haven't seen a lot of PM managers move back to IC product management. I find that once you can build product through teams and not sit there all day in Excel and check in on deadlines and things, it's hard to give that part up. You kind of enjoy being higher up in that chain. Yeah. Kind of along these lines, something that you have a really interesting perspective on is one-on-ones. Most people are like have one-on-ones with everyone, have them regularly, they're really important. You actually have this contrarian take that maybe you should have less one-on-ones, especially as an engineering manager, you talk about that.

Camille Fournier (00:40:46):
Yeah, so to be clear, you should have one-on-ones with your direct reports and your manager and you should hold those sacred ... I tend to do my weekly or maybe every other week. So this is not about that set of one-on-ones. So the one-on-ones of the people that you are directly managing and with your own manager. But I think there is this ... I don't know if it's the remote work, sort of explosion that's happened or it's big companies or what, but what I've seen and I've heard a lot of friends at many companies complain about is this idea that everybody is doing one-on-ones with everyone else. So the manager is doing one-on-ones with their team. They're also doing one-on-ones with all of their peers.

_[330 additional lines trimmed for context budget]_

---

### “I like being scared”: Molly Graham’s frameworks for rapid career growth | Molly Graham
**Guest:** Molly Graham | **Date:** 2026-01-04 | [YouTube](https://www.youtube.com/watch?v=twzLDx9iers)  

# “I like being scared”: Molly Graham’s frameworks for rapid career growth | Molly Graham

## Transcript

Lenny Rachitsky (00:00:00):
You've worked with many very high performing founder CEOs. Zuck, Cheryl Sandberg. Larry and Sergei at Google. Brett Taylor.

Molly Graham (00:00:07):
Google, when I was there, felt like two PhD students paradise. Facebook felt like 19-year-old hacker's dorm room. 80% of the culture of a company is literally defined by the personality of the founder. Our job as operators or as leaders is to help articulate the culture that they're creating.

Lenny Rachitsky (00:00:25):
When a lot of people think Molly Graham, a lot of people think of giving away your Legos.

Molly Graham (00:00:28):
You have to grow as fast as your company is growing if you really want to take advantage, both learning to give away what you've gotten good at and move on to the next shiny pile of Legos.

Lenny Rachitsky (00:00:39):
Sarah Caldwell. She told me that the framework that helped her most in her career is something that you call the J-curve versus stairs.

Molly Graham (00:00:46):
So Chamath, when he pitched me on this job, actually drew me a picture on a whiteboard. He said, the way a lot of people do careers is a set of stairs. Just walk up the stairs and you'll get promoted every two years. But that is boring. The much more fun careers are like jumping off cliffs and you do fall, but then you climb out way beyond where the stairs could ever get you.

Lenny Rachitsky (00:01:08):
Today, my guest is Molly Graham. Molly was an early employee at Google, also at Facebook, where she worked closely with Zuck on building the Chan Zuckerberg initiative. She also worked with Brett Taylor on scaling Quip, which he sold to Salesforce. She's also worked with hundreds of companies and founders helping them grow into the leaders that they want to become. Today, she leads Glue Club, which is a community for leaders operating in changing, growing environments who want to develop themselves as quickly as their companies. Molly is maybe most known for her advice to give away your Legos, which we chat about. Along with basically all of her favorite frameworks and mindsets and pieces of advice that she's developed and collected over time. For leaders who are going through rapid scale and growth and are just struggling to keep up. I think of this episode as a high growth handbook for leaders who are experiencing rapid scale.

(00:01:58):
We cover the J curves versus stairs approach to career growth, the waterline model, and why you want to snorkel before you scuba. Her six rules for creating goals and building alignment, her rules of thumb for dealing with rapid scale and lots of change. The biggest lessons she's learned from Zuck and Sergei and Larry and Cheryl and Brett Taylor and so much more. Molly is incredible and you will be a better leader after listening to this episode. A huge thank you to Eric Antonow, Ashley Murphy and Sarah Caldwell for suggesting topics and questions for this conversation. If you enjoy this podcast, don't forget to subscribe and follow it on your favorite podcasting app or YouTube. It helps tremendously. And if you become an annual subscriber of my newsletter, you get 19 incredible products for free for an entire year. Including, Lovable, Replit, Bold, gamma, Innate and Linear, Devon, Posttalk, Superhuman descript, Whisper Flow, Perplexity. Warp, Granola, Magic Patterns, Raycast, ChapiRD, Mobbit, and Stripe Atlas. Head on over to lennysnewsletter.com and click product pass. With that, I bring you Molly Graham after a short word from our sponsors.

(00:03:01):
Today's episode is brought to you by DX, the developer intelligence platform designed by leading researchers. To thrive in the AI era, organizations need to adapt quickly. But many organization leaders struggle to answer pressing questions like, which tools are working? How are they being used? What's actually driving value? DX provides the data and insights that leaders need to navigate this shift. With DX, companies like Dropbox, booking.com, Addien, and Intercom get a deep understanding of how AI is providing value to their developers. And what impact AI is having on engineering productivity. To learn more, visit DX's website at getdx.com/lenny. That's getdx.com/lenny.

(00:03:44):
If you're a founder, the hardest part of starting a company isn't having the idea. It's scaling the business without getting buried in back office work. That's where Brex comes in. Brex is the intelligent finance platform for founders. With Brex, you get high limit corporate cards, easy banking, high yield treasury, plus a team of AI agents that handle manual finance tasks for you. They'll do all the stuff that you don't want to do, like file your expenses, scour transactions for waste, and run reports all according to your rules. With Brex's AI agents, you can move faster while staying in full control. One in three startups in the United States already runs on Brex. You can too at brex.com. Molly, thank you so much for being here and welcome to the podcast.

Molly Graham (00:04:36):
Thanks, Lenny. I'm excited to be here.

Lenny Rachitsky (00:04:38):
I feel like this conversation was in an inevitability. I feel like you're the kind of guest where it's like, we will do this someday. I'm such a fan of your stuff. I've read all the stuff you've put out there over the years. We're going to be talking about the best frameworks and mindsets that you've developed over the years that have been really helpful to you, to founders, to companies that you've worked with to help them with growth and scale and change and all the stuff that comes with success. The way I think about this, I want to make this the greatest hits of Molly Graham.

Molly Graham (00:05:06):
Love it.

Lenny Rachitsky (00:05:08):
And so I sourced what I think are the greatest hits from a lot of colleagues that you've worked with, a lot of people you've worked with. We've chatted about the stuff that you find other people find most helpful. So we're going to be going through all that stuff. But let's help people understand why they should listen to this advice. What's kind of the backstory on these frameworks? Where did they come from? Where did you develop them? Tell us that story.

Molly Graham (00:05:29):
So first of all, Ami Vora, who you have had on your podcast, once said to me that all advice is just someone telling you what they did. And I always think about that. Because I really think that basically what I tell people is I've made every single mistake in the book. And then I got to the end of the book and I started inventing new mistakes. So mostly what I feel is that I like sharing my stories because I want to help people. I want to help people not make the same mistakes I did. And I also want to help people make sense of what they're experiencing. But I started in tech in 2007. I actually started at Google the week the iPhone launched and a lot of my scaling battle scars come from a couple of experiences. They come from a year and a half at Google, which is not very long.

(00:06:17):
And Google was pretty big when I was there. It has thousands of employees. But my department, which was the communications department, was 25 people when I joined and it grew in nine months to 125 people. And that was really my first experience with just all the sort of things that I still talk about today. In terms of what it feels like to grow really, really fast and sort of all the tools that I started developing from there. After Google, I left and followed Cheryl Sandberg and Elliot Schrage to Facebook. And I spent five years at Facebook. And I joined Facebook in 2008, and it's important context because it was 80 million users at the time. We were smaller than MySpace. It was 270 million in revenue, 500 employees. It did not feel inevitable. Most people thought we were going to sell it to Microsoft. When I told people I was going there, they were like, isn't that place just like a site for college kids? And so I was there for five years and it was a crazy five years.

(00:07:22):
When I left, it was 5,500 employees, five billion in revenue, over a billion users. So a huge amount of what I experienced, what I write about, what I talk about in Glue Club, which is the community that I run, comes from that rapid scale at Google and Facebook. But I also, I left Facebook right after we went public, about six months after we went public. And I only like doing jobs that I'm highly unqualified for. I like being on learning curves so steep that I'm scared I'm going to fall off. And so I left and I wanted to learn what it took to build something from nothing. And so I joined this little startup founded by Brett Taylor, a startup called Quip. I joined a couple of months before we launched and ran everything that wasn't product and engineering there for him. And that was such a valuable experience to me because the experience of building something from nothing is actually quite different than the experience of holding on for dear life while things are scaling so fast around you.

(00:08:27):
And it really taught me about all the tools and skills you need to go from zero to one and then from one to two and how lonely it can be to build something. And we eventually sold that company to Salesforce. And then again, only take jobs I'm highly unqualified for. But the last really chaotic scaling experience I had was actually helping Mark Zuckerberg and Priscilla Chan start their philanthropy, the Chan Zuckerberg Initiative. And I basically helped them for the first two years of its existence or its sort of like first full existence. And philanthropy sounds calm. You know what I mean? We're like, oh, giving money away. Must be so peaceful over there. And CZI grew from, I think the week I joined, it was 30 people and we bought two companies that week and it grew to 250 people that year. And it was like using every single tool in my toolkit that I had taken from every other job that I'd had.

(00:09:22):
So my advice and frameworks, like I said, come from having made a lot of mistakes. But I've also sort of made a personal study over the last 18 years, believe it or not. Essentially what does it take to thrive inside growing and changing companies, not just to hang on for dear life. What does it take to lead in the face of constant change? And really the other piece that I find truly fascinating is what genuinely makes the difference between a business that grows but then plateaus versus these generational businesses. The ones that go on forever. Sort of the difference between a Twitter or MySpace and a Facebook. Billions in revenue versus hundreds of billions in revenue. So what I like to do is take my experience and use it to help other leaders. I want to give people tools that work. And I also want to be honest about how hard all of this stuff really is.

Lenny Rachitsky (00:10:24):
Amazing. I say this a lot in this podcast. I just love the ROI that listeners of the podcast get. You spent 20 years toiling, struggling, working so hard, learning so much. And you're just here, here's all the answers that I've learned. And obviously not all the answers, but so many things that will help people avoid the pain and suffering that you've gone through.

Molly Graham (00:10:43):
That's the goal.

Lenny Rachitsky (00:10:45):
Also, a couple quick threads I want to follow here. One is Ami Vora, who you mentioned. She's now, I think, head of product at Anthropic.

Molly Graham (00:10:50):
Yes.

Lenny Rachitsky (00:10:50):
Amazing. Former podcast guest, also speaker at Lenny and Friends Summit two years ago. This other point you just made about how you've always gone to places that have been way beyond your... I forget how you phrased it, but just beyond your current capabilities almost. And were very difficult. I just had Matt McGinnis on the podcast. He's CEO at Rippling, now CPO at Rippling, and just recorded an episode with him. And he had this really powerful quote that if you're ever comfortable at work and feel like, oh, I got this, you're making a huge mistake. Something's going terribly wrong. That's not where you want to be.

Molly Graham (00:11:23):
Yeah. I always say I get bored really easily, which is both a strength and probably my greatest weakness. So I like being scared.

Lenny Rachitsky (00:11:30):
Okay. So let's actually dive into some of your greatest hits of frameworks. And the greatest of all greats, when a lot of people think Molly Graham, a lot of people think of giving away your Legos. Some people haven't heard of this, many people have, so let's cover this. What is this advice of giving away your Legos?

Molly Graham (00:11:47):
So this definitely started in my experience at Google. And then Facebook was a masterclass in giving away the Legos. But the way I like to talk about it is basically when I watch leaders and employees go through rapid scale, I like to think of somebody putting down a giant pile of Legos in front of a bunch of kindergartners and then just being like, build something. And that's sort of what it feels like when you start. It's like, well, there's so many Legos and it's so fun. There's a lot of opportunity, but it's also kind of scary and overwhelming. And you're like, there's so many Legos. What do I do? Isn't there an instruction manual hidden under this pile somewhere? But then you start building and you're like, oh, okay. You build something and then you take it apart and then you put it back together.

(00:12:33):
And then eventually you start to get momentum and you're like, okay, it's like I'm building a house. I got this. It's a house. All right, great. And then you're like, I'm good at building houses. I was put on earth to build houses. And almost assuredly inside of scaling companies, as soon as you're like, I feel good at this and I should do this forever. Somebody's going to show up and be like, okay, it's not a house. It's a neighborhood. And you need to take this house that's kind of half built and you're going to pass it off to this other person that we just hired. And you are going to go build dog parks and streets and other things that are entirely unhouse-like. And what happens when someone does that to you is you're like, wait a minute. First of all, I'm not done with this house. And I'm worried that this person's going to screw it up.

(00:13:18):
I'm also worried that building houses is actually the most fun thing and that I'm going to give the Legos to that person and they're going to have all the fun work and I'm going to hate building dog parks. Or that dog parks are irrelevant eventually and it's going to turn out we're in the house building business. So there's this incredible set of emotions that come territorialistic, paired with excitement. Fear paired with joy. But eventually you pass the house off and then you go work on neighborhoods and you're sort of like, okay, dog parks, I'm good at dog parks. I got this. And then again, you get to the like, I'm great. I was put on earth to build neighborhoods. And immediately someone shows up and says, it's not a neighborhood. It's a country or a city or a world. And it just goes on and on and on.

(00:14:03):
And for me, learning this muscle of both learning to give away what you've gotten good at and move on to the next shiny pile of Legos. And learning that the emotions associated with that are inevitable. I've been doing this for 18, 20 years, I still get attacked by these emotions all the time, but that doesn't mean that you shouldn't give them away and move on to the next thing.

(00:14:33):
That is both the torment of scaling companies, which is that the ground is moving under your feet. And as soon as you're comfortable, someone will make sure that you are uncomfortable, but it's also the opportunity, which is that you can go from being someone that's good at building houses to someone that knows how to build entire worlds. And that is where the Legos metaphor came from.

Lenny Rachitsky (00:14:55):
That is such a good metaphor. And if you've gone through this, you so understand what this is like and what... And also just the Legos is metaphor is so good for the different things you build.

Molly Graham (00:15:06):
I have a very weird brain that for some odd reason just always thinks in metaphors.

Lenny Rachitsky (00:15:11):
[inaudible 00:15:11].

Molly Graham (00:15:11):
So it showed up when I was... At Facebook in particular, I would find that every so often I would have to have what I called a Legos talk with someone where I would just see them start to ask these questions like, why are we hiring that person? Or what's that team even do? And I was like, okay, we need to have the chat about the Legos. And then eventually it turned into an article and a whole thing.

Lenny Rachitsky (00:15:33):
A whole thing. And just to be clear, the advice is give away your Legos, this is actually the path to a successful career.

Molly Graham (00:15:40):
I have watched a lot of people over many years struggle with feeling like they should hang on to the thing that they've been good at. And it almost always... Because, you know, essentially the nature of a scaling company is that the Lego pile is just getting bigger and bigger and bigger however fast that graph is going up into the right. I always say that's the graph of how fast your business is growing. It's the graph of how fast your company is expanding. And it's the graph of how fast your job is getting bigger. That means that if you actually just stay and build houses, eventually you're literally buried under a pile of Legos. Do you know what I mean? You held onto something that's down here and the opportunity is actually to stay on top of that pile and to learn to just give away your job every so often.

(00:16:27):
At Facebook, I got to a place where I was literally giving away my job every three weeks. I was constantly rehiring myself essentially because you have to sort of grow as fast as your company is growing if you really want to take advantage of the opportunity that comes with companies that are growing and changing quickly.

Lenny Rachitsky (00:16:45):
So people are hearing this, they're like, okay, my rational brain's like, I should give away my Legos. It'll help me. It'll be good for my career. In real life, it's very hard to actually do. To give away this empire that you've built, this team that you've built. This project that you're like, oh, this is going to be my thing. I know you have a really fun, useful tool to help people deal with that kind of irrational part of their brain. Talk about that.

Molly Graham (00:17:07):
So like I said, my brain works in weird metaphors. It's a weird brain. I was raised on The Muppets, and I like to think that this one came from, I guess, growing up watching weird animals. But basically, at some point I realized that this emotional rollercoaster that comes with scaling, with growing. With going through change, any kind of change. People feel that. Was never going to go away. And that no matter how good I got... Sometimes I think it gets worse the more senior you get, actually. Because you sort of feel like you're supposed to know what you're doing, and then you just get attacked by this monster that's like, who even gave you this job in the first place? So basically I externalized all these emotions that come with change into this little tiny monster. I named my monster, Bob. Your monster can be named whatever you want him to be named or her or them.

(00:17:56):
And Bob's job... I like to think his job is basically to make me the worst version of myself. He's the one that's like, oh, that person took all the fun Legos and you should go push them over and grab them back. Bob's job is... Bob's the one that wants to send the rage emails at 9:00PM and burn the house down. And the thing to learn about Bob is that, like I said, Bob never goes away. Bob is someone that you have to learn to deal with. But Bob's job is to make you the worst version of yourself. So your job is to let Bob do his thing, but not act on the emotions. Basically, all these emotions are normal and they are not useful. They are not the compass that should be telling you what to do.

(00:18:46):
But the other rule I have for managing Bob is a lot of people are like, oh, you're feeling off or tired or whatever. Go to bed and wake up tomorrow morning and you'll feel better. And the truth is that you're like, I want to send the rage email at 9:00PM. You still want to send it at 8:00AM. And a lot of these emotions just do not go away in 24 hours. So my rule of thumb from Facebook was give it two weeks. And the emotional, the sort of Bob... Bob is like these waves and they just roll through. So you made a new hire or somebody came in or you got layered or whatever. You'll have a set of reactions. And those reactions, again, they're normal, but they're not useful. They're not the ones that you should listen to. They are Bob.

(00:19:29):
And typically they go away in a couple of days, you get something new. Some new wave. But anything that lasts longer than two weeks is actually something you should pay attention to. It's something that if it's been around for two weeks, it's something you should go talk to someone about. Whether it's a manager or a friend or a coach or someone like that. That's the real stuff. Everything else is just Bob.

Lenny Rachitsky (00:19:50):
Is there a rule of thumb for when it actually, when you shouldn't give away your Legos? When it's like, okay, maybe you should fight back on this layering or whatever.

Molly Graham (00:20:00):
No rule of thumb. In general, I would actually say embracing change is far better than fighting it. And almost invariably, you cannot see what is around the corner, but it is almost always the thing to focus on. A lot of times I think inside of change, we get focused on the past, and one of the most valuable things you can do as a manager and a leader is help people focus on the future. I think... I'm sure there are times when people have done it and regretted it and it has led them somewhere.

(00:20:42):
I think being layered, for example, is one of the hardest things for people inside these experiences where someone brings in a manager above you. And I've also seen so many stories of that ending up being a great thing for someone. Even though they couldn't see it at the time. So in general, I would just say, step into the future and let the past go and see what you're going to learn. And sometimes you'll learn that it's time to leave or that this isn't the right pile of Legos for you. But it'll end up taking you somewhere that's worth exploring. Holding onto things almost always leads us to the worst version of ourselves.

Lenny Rachitsky (00:21:24):
It's a very Buddhist way of thinking too. Just don't cling.

Molly Graham (00:21:31):
There you go.

Lenny Rachitsky (00:21:31):
Yeah. And I think another part of this metaphor, I don't know if you think of it this way. Is the Legos aren't even your Legos, right? They're like the CEO's Legos, the shareholders' Legos. So you think they're your Legos, but no, you're not in charge.

Molly Graham (00:21:42):
Well, it is... I will say one of the hard-earned things is it can feel very emotional and it can feel very personal. It can feel like your work... I don't know, it can feel like your life is on the line sometimes. Just your work life. Oh, gosh, this matters so much. And one of the things that you learn as you get more senior and just have seen stuff is it's going to be okay. A friend of mine says, careers are long and nobody tells you that. But they're long. And this moment feels so dire and it feels so hard and it feels scary and it's going to be okay. So yeah, it is hard to know in the moment. And I think the story is going to be long and this is going to be one chapter or maybe even a part of a chapter, not a whole chapter. So embrace the length.

Lenny Rachitsky (00:22:37):
To build on that point, I've realized this is my fourth career doing what I do now. Whatever the hell this is. I was a engineer and then I was a founder. Then I was a product manager, and then what the hell I do now. Whatever this is, that's a whole different path.

Molly Graham (00:22:52):
You don't have a name for it yet, Lenny?

Lenny Rachitsky (00:22:55):
I don't. I hate all the terms people use for this world.

Molly Graham (00:22:58):
Somebody called me an influencer and I almost ripped their face off.

Lenny Rachitsky (00:23:00):
Yeah. [inaudible 00:23:02].

Molly Graham (00:23:02):
[inaudible 00:23:02].

Lenny Rachitsky (00:23:03):
Yeah.

Molly Graham (00:23:04):
Yeah, man. The most interesting careers are winding and they have starts and stops and failures and successes and control. Anybody that's been through a lot of this stuff, control is usually not the name of the game. It's usually just like, "Let's see what happens. We're going to try this and we're going to see what happens next."

Lenny Rachitsky (00:23:26):
This is a great segue to another framework that I've heard from folks you've worked with that have been really impactful on them. So, Sarah Caldwell, who's a big deal at OpenAI, she told me that the framework that helped her most in her career is something that you call the J-Curve versus Stairs career growth framework. Talk about what that's about.

Molly Graham (00:23:46):
I actually gave a TED Talk about this one a couple of years ago because I am so passionate about it, but you can listen to the very packaged eight-minute version of this, but I will tell you the real story because it's very relevant to a lot of folks that listen to your podcast. I was at Facebook for five years. Like I said, the first two years I was in HR and I was doing employment branding and culture work and I was ready to stay there. I think I had in my head I was going to stay there until we went public, that was my plan just because I wanted to help the company through that moment, again, in my head.

(00:24:21):
This guy that many people know, Chamath Palihapitiya, came to me and Chamath ran growth and mobile at the time. And he came to me and we had lunch and he said in his very Chamath way, "You're useless. What are you doing in HR? This is stupid. You should come work for me." And anybody that knows Chamath is like, "Yes, that is actually what he said." He managed to insult you and compliment you in one sentence.

(00:24:47):
He gave me all these options on his team. And then the last one he said to me was like, "I'm going to go build a mobile phone. Do you want to come do that with me?" And I had four simultaneous reactions. The first was like, " That is incredibly stupid. Why are we doing that?" And then it was like, "Is that actually a thing that we're doing?" And then it was like, "Whoa, I think that sounds kind of fun." And so I left the conversation at Chamath and I went and asked my boss, Lori Goler, who's the head of people at Facebook for a very long time, like, "Is this actually something we're doing?" And she was like, "I can't believe he offered you that, whatever."

(00:25:21):
And I basically just could not get it out of my head, but it didn't make any sense, A, that Chamath had asked me because I was in HR. Like, "What am I doing? I don't absolutely jack shit about mobile." But I had worked on a project with him and I guess he thought I was smart. And I talked to Cheryl and she was like, "Well, that project will be dead in two months, but you can do it because you'll still have a job here." My dad was like, "Well, don't do that. " And anyway, a lot of very wise people being like, "Don't do that."

(00:25:51):
But I kind of couldn't get out of my head. And my friend said to me, "You've proven you're really good at this sort of company-wide project management and HR. Why don't you go show yourself how actually good you are? Is this transferable?" So, I took the job and I spent the next six months feeling like an absolute idiot. I basically felt like a total jackass all the time. I was sitting in rooms with these brilliant people asking the dumbest questions of my life and at the end of the six months, Chamath, I think, took a lot of pride in giving me the lowest performance rating I've ever gotten in my life, and it just felt like falling off a cliff. Then, slowly, I remember I had been doing all these trips to Taiwan because we were actually working on hardware and I, at some point, came back from Taiwan and I drew on a whiteboard for him the layout of a mobile phone and trying to explain to him why something he wanted to do was not possible. I so vividly remember walking out of that meeting being like, "Oh, I actually know things." And slowly then, over the following three years, I became an expert in mobile. And I basically... The phone itself was a giant failure, massive, costly failure for Facebook, but it was not a failure for me. It was a huge job that taught me that I was capable of things that I never could have dreamed of if I had stayed in HR. It set me up to be capable of taking on things that I didn't know about.

(00:27:28):
Chamath, when he pitched me on this job, actually drew me a picture on a whiteboard. He said, "Look, you can stay..." The way a lot of people do careers is a set of stairs. "You can be boring." To use Chamath, "And stay on these stairs. Just walk up the stairs and you'll get promoted every two years and your title will change from manager to senior manager to director to senior director, whatever." And he was like, "But that is boring." And he's like, "The much more fun careers are like jumping off cliffs." Basically, that you jump off this thing and you do fall for a period of time. I always like to say it's about six to nine months, but then this thing happens where you climb out.

(00:28:05):
And the picture he drew had this J-curve sort of basically leading you to places that are way beyond where the stairs could ever get you. And to be totally honest, that has been my experience. That taking risks, accepting the sort of terrible fall and that experience of falling has been more than worth it. Part of the reason why Sarah mentions it is that I do give this sort of talk to people that are inside of really fast-growing companies, because it's such an important place to let go of Legos and jump off cliffs because there's so much opportunity. And it is a place where if you prove to people that you're actually good, if they believe that you are the kind of person that they can use to do lots of things, you can get these opportunities that you are just so deeply unqualified for, but they can take you to places that you could never have imagined.

(00:29:01):
You can come out of those companies with skills that no one would ever have reasonably hired you to do. But I ended my time at Facebook in product and did business development and hardware and a whole bunch of the stuff along the way. And again, nobody would've hired me to do that at the beginning, but it's just because I kept saying yes to things.

Lenny Rachitsky (00:29:22):
Molly, I got tingles listening to this story. Wow.

Molly Graham (00:29:25):
Does it sound familiar, Lenny?

Lenny Rachitsky (00:29:27):
It does. I want to ask, jumping off a cliff, sometimes you fall, really fall and you keep falling. Are there any kind of traits of like, "Okay, this is one that might be a J-Curve and worth the risk of falling, and this is when you should probably just not, let's not do this".

Molly Graham (00:29:46):
Yeah. I just think there are different kinds of fear. We talk a lot about this in Glue Club because one of the thing, there is a financial fear, right? Leaving a job and taking a job that has financial risk associated with it, or leaving a job and taking time off, which is something that I spend a lot of time talking to people about, you got to do the math and you got to... Sometimes there is a type of fear that is telling you like, "This is not the right time." Or, "I don't want to be financially anxious for months and months and months."

(00:30:21):
I use finances because it's the most concrete example of a type of fear that you should actually listen to. And sometimes you can do the math. I always counsel people through that. I'm like, "What is the number that you need to hit so that you're not constantly terrified financially?" And that number is wildly different for people based on their background and their life. "Can you do that? Can you consult, can you whatever in order to take this leap?" But a lot of times fear is just you saying, "I'm scared I can't do this. I'm scared I'm not capable of it. Yeah, I'm scared I'll fail."

(00:30:56):
And that's the kind of fear that I think of as a flashing green light because... And it sounds like Matt McGinnis said this too, where it's like, "That's the kind of fear that's saying, 'Why don't you go prove to yourself that you are actually capable of this?'" Or if you fail, like, "You'll have learned something, too." You know what I mean? You'll have learned, like, "I took this job in product at Facebook as my last chapter there, and let me tell you things that people should never fucking hire me to do." I was like, "I am not a good product manager." But I've got a great product mindset. I can sit in a bunch of chairs and hang with the product folks, but I'm not the person that cares about the button. Do you know what I mean?

(00:31:38):
And I would never have learned that. I wouldn't have known who I was if I hadn't taken that risk and failed or at least learned that it's not something I wanted to do again. So, there's many different lessons that come from facing down those fears and jumping off the cliff, but mostly what it is is knowing yourself better and knowing where you go next from there.

Lenny Rachitsky (00:32:03):
That is such helpful advice. I also love how you frame this of, "Prove it to yourself that you can do this." It's not, "I'm going to show them that I can do this." Because the way you describe this, usually it's an opportunity given to you. "Hey, can you do this thing? We want you to lead this new thing." And the fear is like, "I don't think I can do that." And what you're saying here is, "Prove it to yourself that you can." Or, I guess, it's also, "Okay, maybe I can't and then I'll learn that and then I'll know more about myself."

Molly Graham (00:32:27):
Yeah, exactly. I mean, one of the greatest gifts in a career is knowing yourself. And that's a lifelong journey because who you are and what you want changes, but that knowledge and that gift, nothing accelerates your self-knowledge faster than trying to do something that you don't know how to do and that you're scared of.

Lenny Rachitsky (00:32:50):
Probably the quote I use most on this podcast comes up again in my mind as you talk about this, this line that, "The cave you fear contains the treasure you seek."

Molly Graham (00:32:59):
Hell yes, exactly. Well said.

Lenny Rachitsky (00:33:03):
There it is.

Molly Graham (00:33:03):
I haven't heard that one from you, so clearly I need to listen more.

Lenny Rachitsky (00:33:04):
Okay, that's great. I'm glad I don't overuse it. It just feels like it comes up again and again, and I think your point about the runway and the finances is such an important one because that's a very real practical question. One thing I did when I took time off, I took a year off after I left my job. What helped me was I just created a runway goal for myself. I'm just like, "Okay, here's what it's going to cost me for six months or a year to live without any income. Am I comfortable just burning through these tens of thousands of dollars to explore and see something new emerge?" And so you just have to feel good. "Okay, yes, I'm going to burn all that money and that's part of it."

Molly Graham (00:33:37):
Yeah, that's exactly the exercise. You're saying "runway" I say "burn rate", so we both were raised inside of companies, incentive tech, but I think it is do the math, right? What can you afford? And it's both what can you afford and still feel safe? Because sometimes, I mean, again, I think that is different for everyone, but it is such an important set of math to do because, A, a lot of times that number is smaller than you think it is, then your brain makes it out to be if you have this sort of existential financial anxiety versus, I always say, "Specific financial anxiety is much more useful than existential financial anxiety." And some friends are leaving jobs and I'll be like, "Hey, your number is 5K or 10K a month. You have to believe that you can get a consulting gig that will pay you that. Do you believe that?" And it's like, "Either yes or no." And then, "Okay, either we're doing it or we're not.

Lenny Rachitsky (00:34:29):
The other part of this J- Curve that I think is really important to touch on is this idea of for the first six or nine months, you're going to be at the bottom of the J curve falling, still falling. And some projects don't last that long and then you're like, "Okay, total failure. I never emerged from this fall." So, is there any advice there? Just, how do you create that enough space to give you a chance to start to un-fall?

Molly Graham (00:34:49):
I mean, the most valuable thing that happens as you fall is learning. And even on the other side of failure, you've learned a shit ton. I always say, "The most important thing to do in the falling phase and the risk taking land is to learn to embrace being a professional idiot." Basically, being the one that shows up at the meeting and is like, "What are we talking about? What does that word mean?"

(00:35:18):
For a bunch of reasons. Number one, you can learn so much. And again, even in the face of failure, no one can take away your learning. Do you know what I mean? But the other thing is that it turns out that a lot of the questions in the world that, you're sitting in the meeting and you're like, "This is a dumb question. Everyone's going to think I'm an idiot." But then you get brave and you ask it and it turns out it wasn't a dumb question. Do you know what I mean? Turns out that everyone had that question in their mind, but no one was brave enough to ask it.

(00:35:48):
So, from a skills' perspective, again, regardless of outcome, being the person that sort of takes their learning in their own hands, learning no matter what and learning to ask those dumb questions, it's a superpower. I always say that, "Actually, my superpower is being a professional moron." Because I'm the one that shows up in a room and is like, "Do we have goals? What are we doing? Why are we talking about this? Why are we having this meeting?" And most of the time it's actually what I was hired to do, which is bring clarity.

Lenny Rachitsky (00:36:19):
It's so funny. I just recorded a podcast episode with a PM named Zevi who joined Wix and he had this thought, he's like a very young PM, just getting started and he's like, "Okay, I need to be a 10X PM because that's what they expect of me, that's what everyone that is really good, that's how I think of a 10X PM." And then he went into his first meeting and he just failed and he just felt so bad. He's like, "I guess I'm not that 10X PM. They're all going to see that. They think I'm terrible." And then he did another presentation a little bit later and people were so impressed with how he learned and evolved and improved. And he realized that he needs to be not a 10X PM, but a 10X learner, and that's what people actually expect from someone, especially a junior person.

Molly Graham (00:37:05):
Yeah. Well, I was having a conversation last night with a friend of mine who has a senior in high school and I was like, "What is the plan? What are we telling this senior in high school to think about relative to their career given everything that's going on with AI?" And we talked about it a bunch, but what we both circled back to was this idea of soft skills and that actually the only thing you can really anchor on right now is that teaching kids grit, teaching them hard work, teaching them learning, right? Learning how to learn, loving learning, being able to fall, in a world that's changing this fast. And I say this inside of companies too, right? I always say, like, "What you know today is way less valuable than what you can learn by tomorrow." If you're inside of a company where the growth curve is like this, what you know today is irrelevant.

(00:37:52):
Somebody once told... I'm sure this is faster now, but they rewrote the entire code base at Google every eight years, which means that if you're not learning, if you're not evolving, then you become irrelevant and extinct. It's actually the whole underlying point of the Legos stuff is that evolution is the way you stay on top, and I think that's more true today than it's ever been.

Lenny Rachitsky (00:38:14):
And luckily, AI is really good at helping us learn.

Molly Graham (00:38:16):
Totally.

Lenny Rachitsky (00:38:17):
So, that's good. Thank you, AI. And this actually comes up a bunch in the podcast. I ask a lot of AI-forward people what they're teaching their kids and curiosity is one of the main things people talk a lot about. Just like, "Help them develop curiosity about the world."

Molly Graham (00:38:31):
Yeah.

Lenny Rachitsky (00:38:31):
Yeah. Okay. I feel like I could be talking about this specific topic for a whole podcast episode, but I want to move on to a couple other frameworks that you've developed. One is something called a Waterline Model and another former colleague of here said, "This is the most impactful thing that they've learned from you on their career." So, talk about the Waterline Model.

Molly Graham (00:38:50):
Okay. Yeah. Well, first of all, the Waterline Model is not mine. It's from some business book somewhere, but I actually learned it. My first job out of college was leading wilderness trips. I led 75-day wilderness trips in Patagonia and Alaska for a school called NOLS, the National Outdoor Leadership School. NOLS basically teaches essentially leadership and communication skills to students.

(00:39:15):
I was mostly leading college age kids through wilderness expeditions. So, by having to lead a group of your peers that you don't know. Anyway, the Waterline Model is something that we taught on NOLS. It's a really, really helpful model for understanding how to diagnose when something is not working on a team, so I teach it inside of Glue Club and I'll just quickly explain it. Basically, the way to think about the Waterline Model is that a team is a boat and it's a boat on an ocean trying to get somewhere, getting somewhere is goals, right? "What are we trying to build or ship or do?" Essentially, that is going to be harder or easier based on whatever the shape of the ocean is, right? If it's really choppy, it's harder, if it's smooth and calm, it's going to be easy to get to your goals.

(00:40:02):
So, the Waterline basically asks the question like, "What is going on under the water? What is going on that's making it harder or easier to get to your goals?" And there's essentially four things underneath the water and they are in a descending order. The surface level is what's called structural things. Basically, structural things are like goal setting, vision, roles, expectations, kind of the structures you put in place to make a team and a company and a business make sense, that touch every single member of the team.

(00:40:37):
Right below that is something called dynamics, which is essentially how the team works together. It's culture, it's decision making, it's how we resolve conflict, all the sort of like interwoven pieces of how teams work together. And then below that is interpersonal, so basically relationships between two people and all the things that come with us being humans. And then the bottom is intrapersonal, meaning within one person, challenges and issues there.

(00:41:08):
The interesting thing about this model is that most people, when something's going wrong on a team, a lot of times we always go to the bottom. We go to the people. We're like, "The people aren't getting along, that person's having a rough moment." We go to the humans, but the rule with the Waterline Model, which is very memorable, is you snorkel before you scuba. So, 80% of problems on teams actually happen because of structural issues or dynamics issues. So, when there are problems on your team, where you start is at the top, you start structural issues.

_[414 additional lines trimmed for context budget]_

---

### An inside look at how CNN builds product | Upasna Gautam
**Guest:** Upasna Gautam | **Date:** 2023-02-23 | [YouTube](https://www.youtube.com/watch?v=gRiCzfFEd7c)  

# An inside look at how CNN builds product | Upasna Gautam

## Transcript

Upasna Gautam (00:00):
It happens all the time, right? That is the nature of breaking news. I mean, you have to be ready to pivot at the drop of a hat. I had a big working session planned with my users to do research with them, or do user testing, and breaking news breaks, and it takes so much time and effort to gather a team of editors across the globe to do a user testing session. And when breaking news happens, they have to prioritize that over everything. So what do you do in that situation? You can be frustrated, absolutely it's frustrating. But we always have to have the ability to A, pivot of course, but also have backup and buffers in those types of scenarios.
Lenny (00:45):
Welcome to Lenny's Podcast, where I interview world-class product leaders and growth experts to learn from their hard-won experiences building and growing today's most successful products. Today my guest is Upasna Gautam. Upasna is a product manager at CNN, where she leads the team responsible for the content management system that journalists use to write and publish their stories. She's also on the frontlines of elevating the discipline of product management within newsrooms through her work at the News Product Alliance. She's also a longtime meditation and mindfulness teacher, which as we discuss ends up being pretty damn handy working at a place like CNN. We dig into how the product team operates within CNN, how they collaborate with journalists for breaking news, dress rehearsals, and also some simple tricks to build your own mindfulness in your day-to-day work as a PM.
(01:31):
Enjoy this episode with Upasna Gautam after a short word from our wonderful sponsors. This episode is brought to you by Amplitude. If you're setting up your analytics stack but not using Amplitude, what are you doing? Anyone can sell you analytics, while Amplitude unlocks the power of your product and guides you every step of the way. Get the right data, ask the right questions, get the right answers, and make growth happen. To get started with Amplitude for free, visit Amplitude.com. Amplitude, power to your products. Today's episode is brought to you by OneSchema, the embeddable CSV importer for SaaS. Customers always seem to want to give you their data in the messiest possible CSV file, and building a spreadsheet importer becomes a never-ending sink for your engineering and support resources.
(02:19):
You keep adding features to your spreadsheet importer, the customers keep running into issues, six months later you're fixing yet another date conversion etch case bug. Most tools aren't built for handling messy data, but OneSchema is. Companies like Scale AI and Pave are using OneSchema to make it fast and easy to launch delightful spreadsheet import experiences, from embeddable CSV import to importing CSVs from an SFTP folder on a recurring basis. Spreadsheet import is such an awful experience in so many products. Customers get frustrated by useless messages like, "Error online 53," and never end up getting started with your product. OneSchema intelligently corrects messy data so that your customers don't have to spend hours in Excel just to get started with your product. For listeners of this podcast, OneSchema's offering a $1,000 discount. Learn more at OneSchema.co/lenny. Upasna, welcome to the podcast.
Upasna Gautam (03:15):
Thank you Lenny, I'm so excited to be here.
Lenny (03:18):
As you probably know, I'm on this kind of quest to understand how different companies build product, especially companies that are kind of outside the Silicon Valley, just like standard tech scene. And I've always wondered what it was like to build product at a company like CNN, which is very different from where most, I don't know, tech PMs work. And so I'm really excited to have you on and to give us a little glimpse into what it's like to build product at CNN.
Upasna Gautam (03:42):
Awesome, I'm very excited to share as well.
Lenny (03:44):
So I was doing a little research on you before this chat, and I noticed that you've been teaching and studying meditation and mindfulness for, I don't know, maybe a decade? And I imagine that comes in handy at a company like CNN, which also I imagine is quite hectic at times with breaking news all the time. And so here's my question, what have you learned or brought from that practice to your ability to lead and ability to just like keep your team calm and focused during I imagine many hectic moments?
Upasna Gautam (04:16):
Love that question, and working in product in news is a very [inaudible 00:04:22]. We hear a lot about having the skills to thrive in ambiguity in order to be a successful product manager, but to be a successful product manager in news you have to be able to thrive in chaos. And equanimity is the most important skill I've developed I think across my entire life, and it's due to all of those years of practicing mindfulness and meditation. And equanimity is one of my favorite words, it means mental calmness, composure, evenness of temper, especially in crazy, stressful situations. It's the ability to remain un-rattled in like the highest of highs and the lowest of lows. And if you think about it in the dynamic of a workplace or human interaction, it's really the ability to pause before reacting.
(05:11):
And reactivity is the root cause of so many of our workplace woes and product management frustrations, and when you're able to pause before reacting you can start to undo and break a lot of those negative patterns in your brain. And in news, and especially the world's biggest breaking news organization, you can cut through that chaos with equanimity because it also really gives you this outsized advantage in everything from stakeholder management, team morale, to the way you're able to translate user feedback, to product development philosophy. And the thing is is you're always going to hear things and feedback that you don't like, and opinions that you don't like, and you're going to get frustrated. That's product management, that's life.
(05:59):
But real power to me comes from equanimity, and that comes from managing your emotional reactions, and not trying to control others. So I think when you're able to cultivate that level of self-awareness for yourself, you're able to chart a clear path forward for your team and serve as a compass in the chaos.
Lenny (06:22):
Is there a story or an example that comes to mind where things were just like, "Oh shit," and you were able to tap into that skill that you built to stay calm and focused, or even just like help your team stay focused?
Upasna Gautam (06:33):
I mean, it happens all the time right? That is the nature of breaking news. I mean, you have to be ready to pivot at the drop of a hat. And so I think more than just like the big, chaotic moments in those daily interactions when there's chaos, I think that's when it really shows its power and helps me navigate those day-to-day interactions when I had a big working session planned with my users to do research with them, or do user testing, and breaking news breaks, and it takes so much time and effort to gather a team of editors across the globe to do a user testing session. And when breaking news happens, they have to prioritize that over everything. So what do you do in that situation? You can be frustrated, absolutely it's frustrating. But you always have to have the ability to, A, pivot of course, but also have backup and buffers in those types of scenario. So any time we're planning we build in buffers for all of that chaos that's happening on a daily basis.
Lenny (07:41):
Wow, that's cool. Is that a real thing that happened? You were like, in a big meeting with all the researchers and then something went off the rails in the world?
Upasna Gautam (07:47):
Oh yeah, absolutely. Happens all the time.
Lenny (07:49):
I don't know how much you can go into all this, but what is it that you would do in that case, like as a product team? What are they looking for you to do at that moment?
Upasna Gautam (07:56):
For us it's not really anything we have to do. It is more so, "Okay, well our goal at this session was to get user testing done, or user feedback from that user testing session with our editors. And so, what do we do? Okay, well good thing we had a backup planned, right?" We don't actually have to go into that scenario, but since our editors and our journalists are customers we're there to serve their needs, but they're not here to serve our needs. They have a whole another job that they have to do. And so we have to have the ability to adapt and accommodate to those really chaotic schedules across global time zones. And so that happens again like, it's not like once in a while, it's a very regular thing that we have to be aware of and also account for.
Lenny (08:46):
Okay, again, it's like the journalists are the ones in the meeting that like, "Oh, they have to go write about the stuff uncovered."
Upasna Gautam (08:50):
Totally.
Lenny (08:51):
I see, that makes sense. You said that you built in some buffer time to kind of account for these things. Is there like a systematic way of doing that, or is it just broadly, "Let's add buffer time"? Is there like a rule of thumb you think about there?
Upasna Gautam (09:02):
It depends on the scope of the work we're doing at that time. So one of my main responsibilities is to rebuild our content management system, which also involves onboarding our editors and journalists to it off of our legacy platform and onto our new one. So when we're thinking about onboarding especially the teams, our onboarding cycle's very long. It involves training, testing, lots of dialogue and feedback. And then only do we actually test, and then we onboard. And so it is a long cycle, and it's longer than it maybe seems like it needs to be because we have to build in those buffers. Sometimes that buffer is a day or two, and sometimes it's a week or a month. And sometimes when we think we need a buffer, we get it done in a day and we quickly move onto the next phase.
(09:53):
And so I think that quick decision-making in those times is super-important, because you have to be able to assess the situation as it stands. And so I think mindfulness is so important too in practices like this, because being able to objectively see the scenario for what it is, make a quick decision and move on to the next phase is something we have to analyze every single day.
Lenny (10:16):
Makes me think a little bit about a lot of CEOs are very busy and pulled into a lot of things, and so you have to kind of account for their schedule. And it sounds like you have many, many people that you work with that have crazy schedules.
Upasna Gautam (10:29):
Yes.
Lenny (10:30):
I wanted to dig a little bit deeper into how you, your team works with the news team. It's such a unique way of working, most product teams don't have a whole set of journalists that they have to work with as stakeholders. So I'm curious specifically, say a journalist has like, "Hey, I'd love to build a special immersive experience for this story," or, "I need some special feature to help support this work that I'm doing." How does that work? Do they come to you, and, "Okay, here's a roadmap [inaudible 00:10:57] maybe in the quarter [inaudible 00:10:59]." Imagine it's like, "I need this next week." How do you work with the journalist team, basically, on product?
Upasna Gautam (11:04):
Yep. This is pretty much the foundation of all of my work. We talk to our editorial staff every single day, and after a lot of observation and learning I implemented a system to manage that kind of intake with four different touch points or events. So we have weekly demo days, working sessions, breaking news dress rehearsals, and office hours. So with weekly demo days, I facilitate those with my product design lead and my tech lead. And we use that as an open form of communication to deep dive into features that exist on our platform, also to preview or give a sneak peek of new features to come, and kind of recreate their workflows and do a show and tell. Again, we wanted to open that up as a forum to not just our editorial partners but also anyone across the business who is interested in learning about the product and which is our platform.
(12:00):
And it was also a great way to kind of evangelize the product too. Coming off of a legacy system after several years onto a brand-new one is a big change at a place like CNN, and so we definitely tried to make sure that smart repetition in different ways is top priority. So, weekly demo days is one. Working sessions are interesting. I use the term working session because it's a combination of several things. This is a really critical part of our onboarding process where we gather breakout groups of our journalists and editors to work with us to recreate their workflows in the new platform. This allows us to address any friction or issues of course that are occurring in their workflows. But also we're able to gather a lot of awesome feedback from them in those sessions.
(12:54):
We used to call them user testing sessions, but decided to move away from that and just call them working sessions so they're more collaborative. And again, it's a really critical part of our onboarding process. We usually do three to six of them depending on the size of the group before the team is actually onboarded for two hours per session. So they're like deep dives into their specific workflows, so as you can imagine the way the politics team programs content is wildly different than the way the entertainment team does, or the way that the health team does, and very different than the way that the CNN homepage even interacts. So all of those are separate teams we have to work with when it comes to getting feedback. So in addition to that, we also do I mentioned breaking news dress rehearsals.
(13:44):
And you can imagine this is exactly what it sounds like. We create a script and do a simulation of a breaking news scenario to stress test our platform, because all breaking news scenarios are definitely not the same either. So this gives us a lot of great feedback in that short amount of time at the speed of breaking news. And then last but not least, we have office hours which I just started last year, and that's also probably what you can imagine, open blocks of time where me and my product design partner and my tech lead are just there to answer questions, help people troubleshoot any friction they might be experiencing in their workflow, get their feedback. We just wanted to open up another line of communications, so we do those right now once a week. Sometimes they go up to two times a week if we're in a really vigorous onboarding phase.
(14:33):
After those conversations and sessions and events happen, you know it's our job to translate their needs into the functions that we, A, either maybe already have that we can optimize, B, we build anew, or we tell them it's not viable. And the good thing is that because they've been along for the ride with us throughout the product discovery process we've earned their trust and respect. So that when we tell them something like it's not viable, they usually get it.
Lenny (15:07):
Man, I have so much questions about just working with journalists. I feel like pushing back on a journalist is probably extra hard versus other types of stakeholders.
Upasna Gautam (15:14):
It is.
Lenny (15:15):
Can we zoom out a little bit, and you talked about what you work on at CNN, which is the content management system and things around that. Can you just talk about whatever you can share, just like what does the product team look like at CNN, how many PMs are there, how's like the product work structured and where do you fit in there?
Upasna Gautam (15:31):
Yes. I mentioned earlier my team sits on CNN Digital, and I think when most people think of CNN of course you think of TV and linear programming. I have nothing to do with that, CNN Digital is digital, and it's made up of several teams that are structured around a lot of different product areas. It includes everything from the content management platforms that I work on, to data infrastructure, to personalization, to video experience, which includes like the products of video editor, and the video player, to podcasts, there's a lot. So each of those are product teams. And so we also split out... The way CNN is split out into CNN Digital is because there's a separate core content platform that powers broadcast and linear TV.
(16:21):
And the one that I work on powers CNN Digital. So on my team, the core platform team, our stakeholders and our customers, like I mentioned, are journalists and are editors. Which presents a really unique dynamic, and it's one that I love. We have direct access to our customers at all times of day, for better or for worse, and they're embedded into our product development process. And each of those teams under CNN Digital is kind of like a squad, very similar to how it is at other larger tech companies. We have PMs, we have engineers, designers, and delivery managers embedded in those squads. And since my team is a core platform team, our main responsibility is to A, build our newer, faster, more flexible content management platform to replace our legacy one, and B, onboard our entire editorial staff to it.
(17:19):
And so even more specifically my role is focused on rebuilding the tooling and the editorial experience of that platform, as well as doing the actual onboarding of our journalists. So we work cross-functionally because our team kind of is the umbrella over a lot of different teams, so we truly work cross-functionally to make sure that we're serving our journalists with the tools that they need to do their job in the most efficient way possible.
Lenny (17:48):
What would surprise people most about how product is built at CNN?
Upasna Gautam (17:54):
I think the most surprising thing may actually be that there's just so many different types of products that we build. Like I said, I think when most people think of CNN you see TV, and you think of Anderson Cooper, and all of the things that are very publicly visible. Most of our product work and product teams all sit behind the scenes, and so I think just that in itself, like the sheer size of that is surprising to a lot of people. So I mentioned like all of the different teams, product teams that are based off of function, content management platforms, infrastructure, personalization, video, podcasts, linear TV. And I think that in itself, like the sheer size of it, we're not the size of Google by any means. But at the same time we have the expertise and the depth and breadth of a lot of those larger tech companies.
Lenny (18:52):
I'm curious about the day-to-day operational work of how you build product at CNN. So our first question here is just, you use like OKRs, and OKRs with like key objectives, and outcomes, and 70% of a goal is success.
Upasna Gautam (19:06):
Yeah, absolutely, we do. We've used OKRs for a long time, and they've served as an anchor for my team over the last three years. I can't go into specifics on like what our OKRs are, but I did kind of cover the two main parts of it, which is rebuilding our content management platform and onboarding our users to it. So again, it's a long game at CNN when you're working on a core platform to replace a legacy one. And so we break those OKRs down of course even further into, we look two to three years out and build goals based off of that. Then we break them down by quarter and month, and then out of that for my team and what I do I need to, for my own sake and my team's sake, break those down by the week. And when we're in rapid development phases, we're planning on a daily basis.
Lenny (20:01):
How about in terms of just planning broadly? How far out does the product team at CNN plan in detail, like have an actual roadmap? And is that standardized across teams, or can each team kind of do their own approach?
Upasna Gautam (20:13):
It's a combination of both. So we have one that is an over-arching roadmap that tracks to like our company OKRs and our product organization OKRs. And then from there I mentioned we have our squads, product teams, and we have OKRs on those as well track up to the larger, broader ones. And so once we get down to that level though, there is flexibility and we have autonomy to tackle those how we want. Which is really great, because the scope of work across our different product teams is drastically different. Working on core platform, like I do, versus working on video experience, versus podcasts. I mean, it's apples and oranges. And so it's really awesome that we have the flexibility to kind of make it work for us, while also having these larger goals that we know that, "Okay, this is what we need to work towards."
(21:12):
And I mean, like the saying we always hear in product, of, "Stay firm on the goal, but flexible on the process." And we've definitely been able to use that and leverage that in the way that we track against our goals.
Lenny (21:27):
Something else that a lot of people are always curious about is product review meetings and design review meetings. You talked about a couple of these meetings that you have, but do you have standard product review meetings where folks get together and just review stuff and progress? And if so, how do they work? Like who runs them, who comes to them, how do you make decisions, things like that?
Upasna Gautam (21:46):
We have multiple variations of those. Some are standing ceremonies of course, and some are ad-hoc. Again, it's really based on the work we're doing in whatever phase we're in. But when it comes to product review, and product discovery or design review meetings, in addition to my design lead I've made it a point to bring in my tech lead and engineers along for this product discovery ride, and that's definitely been a game-changer as well. When our engineers are able to understand our journalists' needs at the same level as we are, they're able to define the how to do it with so much greater clarity and precision. And so my tech lead is embedded also in our product discovery process, he joins our user testing sessions, and our design jams, and our editorial conversation planning sessions.
(22:37):
And it's helped him and the rest of our tech and engineering team become experts on the why are we doing this and what are we working towards, to better determine technical feasibility. And he's done an amazing job of also passing that knowledge down to the rest of our engineers on our team, so it's really like this mentality of getting that important feedback from our journalists, understanding their pain points and then sharing and teaching it across the team. And I think the key takeaway on that is it's so important to think of your engineers as partners and not just resources. And when they are embedded into the process right upfront, it makes the whole process in general more efficient.
Lenny (23:21):
So it sounds like that was maybe an evolution of the way the team works, as engineering has been looped into a lot of this early stuff more recently?
Upasna Gautam (23:30):
Totally, yeah. Recently-
Lenny (23:33):
Cool.
Upasna Gautam (23:33):
... meaning, I mean... Again, long game right? Because it's been a couple of years.
Lenny (23:39):
Got it. Going in a slightly different direction, I'm curious if there's any fun or unique traditions or rituals that the CNN product team has. These are always fun to hear about.
Upasna Gautam (23:51):
I'll be very honest, we are not one of the cool teams. The politics team and the breaking news teams have all these cool rituals and fun things that they do.
Lenny (24:02):
What kind of stuff do they do?
Upasna Gautam (24:03):
It's different also because they're usually in-person in the newsroom and I work remotely, and so... And actually most of my team works remotely. And so they have their rituals, but I will say one of the things that we started to do is... So onboarding teams has been a big goal of ours, and that we've slowly checked off the list over 2022. And any time we successfully onboard a team or launch a new feature, it's not anything crazy or fun, like there's only so much you can do when you work remotely. But we definitely make it a point to celebrate, and it's been hard to figure out ways to keep team morale up when you're all-remote. And it started with... We knew we needed to start doing something when we would have a big launch, and it was so anticlimactic.
(24:51):
And we literally hit publish and onboard onto like a politics page, and we were like, "Oh, okay, so I guess that's it." So one thing we started doing is sending like these gift boxes of like random tchotchkes and stuff that mean things to us from the whole year, and it's like just cheap little gag gifts and stuff that we send to each other. I have random stuff all over my desk, I don't even know if I can share some of it. So we try to make it a point to at least share and celebrate, and then when we're in-person we do too. But again, being remote, there's definitely a challenge there.
Lenny (25:32):
Are you ever able to get like a cameo from an Anderson Cooper, or someone else on-air to thank the teams? That feels like a cool feature potentially.
Upasna Gautam (25:41):
So one of my key stakeholders, she leads the video team at CNN, and she is a... We've had some discussions about doing a sizzler reel for our content management platform, since we're onboarding people and building new features. And there have been discussions with her about getting a celebrity to come and promote the product for us, so working on it.
Lenny (26:07):
Yeah, it feels like a missed opportunity, you've got so much talent around.
Upasna Gautam (26:11):
Totally.
Lenny (26:12):
This episode is brought to you by Vanta, helping you streamline your security compliance to accelerate growth. If your business stores any data in the cloud, then you've likely been asked or you're going to be asked about your SOC 2 compliance. SOC 2 is a way to prove your company's taking proper security measures to protect customer data, and builds trust with customers and partners, especially those with serious security requirements. Also if you want to sell to the enterprise, proving security is essential. SOC 2 can either open the door for bigger and better deals, or it can put your business on hold. If you don't have a SOC 2, there's a good chance you won't even get a seat at the table.
(26:52):
But getting a SOC to your port can be a huge burden, especially for startups. It's time-consuming, tedious, and expensive. Enter, Vanta. Over 3,000 fast-growing companies use Vanta to automate up to 90% of the work involved with SOC 2. Vanta can get you ready for security audits in weeks instead of months, less than a third of the time that it usually takes. For a limited time, Lenny's Podcast listeners get $1,000 off Vanta. Just go to Vanta.com/lenny, that's V-A-N-T-A.com/lenny to learn more and to claim your discount. Get started today. Coming back a little bit to the way y'all built product, I'm curious how you balance new product work and new features versus maintenance and bugs. Do you have kind of a... Is there like a philosophy at CNN of how much goes to bugs, maintenance versus new stuff, or is it per team and just, how do y'all think about that?
Upasna Gautam (27:45):
It's certainly team-specific, because again, the scope of work across teams really varies. But since my team's product is an entire platform, it's my job to ensure we're still clear on our goals because like I said, they may change on a week-by-week basis. One sprint might be high-priority feature development, in another sprint maybe we're focused on medium-priority optimizations and bug fixes. But we know that any time there's a critical incident in production, it also takes critical priority over everything else. So it's a balancing and juggling act that relies heavily on intuitive decision-making. But I will say we're also really fortunate to have a global, 24/7 editorial support team that we work really closely with.
(28:35):
So they are the ones who handle, troubleshoot, escalate incidents up to the relevant teams. There are a lot of layers to that protocol when a critical incident occurs. And so our editorial support team is that first layer of support, whereas my team might be the second or third depending on the level of severity and who it gets escalated to. So there's definitely a lot of processes in place, we're not always that first line of defense, which is actually... It alleviates a lot of stress, to know that we have an entire team around the clock globally dedicated just to that.
Lenny (29:12):
When you're talking about these incidents and moments it makes me wonder, is there a memory of just like a wild time of something happened in the news, or just like, "Oh man, we've got to get on top of this thing"? Is there like a memory that you think of like, "Wow, that was crazy"?
Upasna Gautam (29:26):
It's funny but also crazy and amazing, is... So our new content management platform, we've been building it for a while and stress testing it for a while. And politics and election season at CNN is Superbowl like times a hundred. And so we have to be all hands on deck, like all layers of support intact. And so we have also a lot of layers of fallback in case something happened, right? Like there's layers and layers of infrastructure fallback if CNN.com goes down, or another service goes down. And so it actually did happen a couple of years ago in the 2020 election, and one of the fallbacks at that time, because we were not...
(30:14):
We're still on our current content management platform, but we had our new one as we're building it as one of the levels of fall. And if X, Y, Z fails, then it goes to our current platform, and it did. And so that's actually happened, where our new platform saved the day even though it wasn't fully ready. But that just goes to show we think about that kind of stuff like in granular detail, when we think about everything that could possibly go wrong. Because it has happened, and it will happen, and it does go wrong. So that was kind of cool, it was validation to know that the platform we have is really stable, and strong, and secure, that it was a fallback for the election.
Lenny (31:03):
Okay, and it must have felt good, it feels like a promotion material right there that your work-in-progress system saved the day.
Upasna Gautam (31:10):
Yeah, I agree.
Lenny (31:12):
You talked about how you have like these breaking news dress rehearsals. I just noted that, and I thought it would be cool to just revisit that briefly. Like, how does that work? You just get the team together, and like, "Here's a news story that just broke. What do we do?" Like, how does that work?
Upasna Gautam (31:27):
It is scripted, right. So we can only get so close to the real thing. But one thing we do is we script everything, like the teams and the players that we need involved in it, what the incident is. And it usually starts when breaking news occurs with an email, the news-gathering team sends an email to our writers and our producers. And then from there it goes out all the places it needs to go. There's a video that needs to be created, there's an article that needs to be written. Again, the TV side I have no insight into, it's like a foreign land to me. But as far as the digital scope goes, there's a lot of work that has to be done. Even things like photo and imagery, we have a whole team dedicated just to selecting photos.
(32:11):
The photo desk is very involved with that process. And so that is all scripted out, and so we say, "Okay, here's the news, here's how it's going to break," and then we run it. And so we use interpersonal communication tools at work of course, email, and we run the whole thing from the minute the email is sent to... And of course since we're testing the platform with this breaking news dress rehearsal, to the minute you hit publish on that page. And there's a lot of stuff that goes on in there, and it all happens in the span of minutes. And so while that is happening, while that dress rehearsal is happening, we have our engineers and our editorial support team on hand as well. So they're observing what's happening while it's usually me and one of our editorial stakeholders and leads facilitating the actual rehearsal part.
(33:02):
And you know, getting user feedback on, "Okay, did something break, did everything work as it was supposed to work?" And also allows us to understand like how far we can go with stress testing different features on our system. So each one is different, but it's a crazy amount of stuff and information you can gather in such a short span of time. Because if it cannot serve the needs of breaking news, then it's useless. So it's a lot of work that goes into it for this three-minute event.
Lenny (33:35):
That is cool. It feels like just a cool tactic you could use anywhere, and that's a good segue to the next question I had, is just having built product at CNN and seeing how the product team works, what is it that you think you'll take with you to future product teams and future companies if you ever move on to anything else?
Upasna Gautam (33:53):
So I of course can't speak for the entire scene and product team, but I can speak for myself and my specific team. In order to stay competitive in our modern news landscape, building this content management platform and the technology behind it has taught us how to enable rapid development and integration of new features and workflows. And we've been able to quickly test hypotheses while reducing risk, and again, time is of the essence when it comes to breaking news. And when you layer that with a very complex content management technology, there's a lot of moving parts. And we've been able to do that successfully, and it's pretty amazing. And because of that we've developed, I think, this unique skill of high agency under high pressure.
(34:49):
And I really think that that skill or combination of skills is our superpower, and that is something you can take anywhere, that is something that is an asset to any product development team.
Lenny (35:03):
Let's circle back to our original topic about mindfulness and meditation. Other than just living through it and either becoming a meditation expert or having worked at a company like CNN that has a lot of this, is there any, I don't know, tactic you could share, something that someone could take tomorrow and be like, "Okay, I'm going to get better at dealing with crazy, unexpected events," based on the experience you've had?
Upasna Gautam (35:25):
Two parts to this. So one, I would be remiss as a meditation educator if I didn't say that there is no substitution for meditation. So just do it, it takes a long time, and it's a lifelong work-in-progress. But the good thing is that it also... Like the more you do it and consistently you do it, it compounds over time. Okay, if you think of any job description for a product manager it's mostly the same core stuff. You need to be able to lead a team without having authority, you need to have great communication skills, you need to be able to bribe in ambiguity, you need to be able to work with a lot of different people on the technical side and business side, all of the stuff that we hear all the time right?
[NEW_PARAGRAPH]But how do you actually become better at those things? Like, there's only so much you can gain from reading a book. To me it's either A, you've got to go do it, and B, meditation has helped so much because of the clarity of mind and clarity of thinking you're able to cultivate because of it, especially when it comes to communication. And this comes back to another tactic I think. So meditation is one, you can't get around it. It's a very, very powerful tool, I will die on that hill. But thinking about workplace tactics, communication is so essential. When I started at CNN it was a very new chapter for me, it's my first official product management role after working a decade in tech, in data and search infrastructure.
(36:57):
And it was really scary to make that change so far into my career. But when I asked my boss why he took such a big chance on me he said, "It's because of your communication skills and relationship-building skills, and that I've seen you in a room where engineers listen to you and what you have to say, and you listen to them." And he was like, "That in itself is really, really hard to teach." And I was like, "Okay, yeah, I can do that." So when I think about what communication means I also attach that to mindfulness. Like mindful communication is so critical for successful product management, no matter what type of product environment you're in. And it means being like deeply aware of the conversation you're having.
(37:43):
And when I'm having this conversation with you, I'm only having this conversation with you. When I'm talking to my design lead, I'm only talking to my design lead. When one of my journalists is venting because this feature is not working the way we told them it was going to, I am shutting my mouth and I'm listening to them vent, and then I'm extracting what the root cause of the problem is. And I think the takeaway and the tactic is that goes back to exactly what we talked about in the beginning is the ability to pause before reacting is the key to mindful communication and being a successful product manager. I think a lot of times we are taught to have all the answers, or we're expected to have all of the answers.
(38:27):
And people assume we have all of the answers so it's a really frustrating, sometimes, place to be in. But, if you listen and you know where to go to get the answers, that in itself is like a tremendous place to be. So I think the power of meditation, being able to pause before reacting lets you do things like conversate in the language of the listener. So when I'm talking to my journalists I'm not using technical terms or like content management technology terms. They have a whole different vocabulary that they use, they have shortcuts that they have lingo for in our content management system. They call things nicknames, and so you have to speak many different languages as a product manager. And if there's one thing that learning and teaching mindfulness has strengthened, it is that communication, yes, it's very important.
(39:24):
How do you become an effective communicator? I think a lot of times we think of speaking articulately first and foremost, or we think about writing well, which those two are really, really important things. But those can't be effective for us as product leaders if we're not listening, so I think listening and being able to converse in the language of the listener is the utmost importance. And how do you do that in your day-to-day? It's like, okay, if you're having a conversation with your customer are you actually listening to what they say, or are you going in with your own assumptions? Because maybe you're a user of the product. And it was very interesting, because I actually started writing freelance for CNN this past year.
(40:12):
And I was like, "Oh, okay, I'm a user now." And no I'm not, I'm not going to say that it helped me understand their pain points so much more and where they're coming from in a whole different light. But again, I am merely a proxy, I'm not actually the user. What they do is still drastically different. And so when having those customer interactions and stakeholder interactions, the priority should be to listen. And I think that is one of the most important things I've learned, and it's also helped build trust and respect between a previously fragmented structure where product and editorial didn't talk to each other, and now we're a unified partnership.
Lenny (41:00):
Oh, I love that. I love the reminder that a lot of communication is incredibly important for product managers, and so much of it is not actually you talking, it's you being really good at listening, and that makes you a better communicator. I imagine some people listening to this are going to be like, "Yeah, meditation, I should be doing that." What's like one thing someone could do to move forward on the path to starting to meditate? Is there a resource to recommend, a tip of just starting to do something there?
Upasna Gautam (41:27):
Totally. I think when people think of meditation, you immediately think of sitting on a mediation cushion like a monk for an hour. And that doesn't have to be true, especially if you're just starting. And there's also of course social media has commercialized what meditation is a lot as well. But at its core, all it is is being deeply aware of the present moment. So I always tell people who are for example like really busy moms who have no time in the morning for an hour-long meditation is, "Okay, take something in your day that you do every single day. You brush your teeth every morning, right? Okay, take brushing your teeth. Can you brush your teeth and just brush your teeth? And when you're brushing your teeth you are engaging all of your senses, and being fully present and aware of how it feels, how it smells, how it sounds."
(42:27):
That's two, three minutes of your day. That's meditation, and I think it's an amazing way and an easy way... Oh, sorry, not easy. Simple, not easy way, always, to think about what meditation actually is. Is like, instead of like having to uproot your whole life and the way you live to bring meditation in, think about what you already do every day and how you can just be more present, like fully present. And that's one of the core examples I give, is start with brushing your teeth and just brush your teeth. There's an old app I've been using for a long time if you're into that, I stopped using it recently but it's still amazing. It's called Insight Timer, the free version is amazing. It started as like a single-frame app probably about 10 years ago, and it's turned into this...
(43:18):
They've done amazing things with that product. So that's what I recommend, but again, you don't... I think, again, when people have a vision of meditation in their mind they're like, "Well I need these tools, and I need these resources, and I need these things, and I need knowledge." You don't need anything, you need five minutes and yourself, and that's it.
Lenny (43:37):
Amazing. I imagine many people listening to this coming into thinking, "I'm going to learn about CNN and how they build product," and then get a free meditation lesson. What a bonus, that was awesome. I'm going to be brushing my teeth very mindfully today-
Upasna Gautam (43:49):
Love it.
Lenny (43:49):
... my takeaway.
Upasna Gautam (43:50):
Love it.
Lenny (43:52):
Final question, around something that I know is important to you, that is basically there's this growing shift of product thinking in the news industry at large. And I know that's something that you're passionate about and something you're trying to create, is just to kind of move the media industry and the news industry toward more product thinking. So I'd love for you to just talk about what you're doing there, what's happening there, and then maybe if there's anything people could do to help in that effort.
Upasna Gautam (44:16):
Compared to the rest of product management as a discipline and the tech industry, product management in news is still in its, I will say infancy. And that's just because it hasn't been an integrated role for as long as it has been in big tech. But it is starting to become more and more not just prominent now, but people are realizing the value and power of it because especially in smaller newsrooms... I mean, CNN, places like The New York Times, The Washington Post, we have great, embedded, strong product teams. But that is not the case for the majority of newsrooms across the country and the world. Newsrooms are strapped for resources, they are strapped in all resources, including financial resources.
(45:07):
And so first, bringing in product and tech talent is expensive, and that in itself has forced editorial staff and journalists to take on other jobs that they never signed up for. And so they've had to learn how to do things product management, and engineering, and coding hacks, because there was nobody else to do it. And so it's interesting because a lot of times when you talk to editors and journalists, they're doing so many other things other than just their reporting of the news, right? They've had to take on other roles because of those restraints in the newsroom. And so there was a group established and a community established during the pandemic called the News Product Alliance. And there was like a realization occurring all at the same time that, "Wow, there's a lot of us doing this work.
[NEW_PARAGRAPH]"Like, what do we call it? Is this product management?" And it was, it's product management at so many different levels. And so the News Product Alliance was formed during the pandemic in 2020 I believe by several media and news executives, who had been there and have seen people in their newsrooms do that. And the goal of it is to increase the awareness and education of product thinking and product management in newsrooms, while also increasing the diversity of thought and the diversity of people in those positions as well. And making the resources more readily available, especially for those smaller newsrooms that don't have the money to hire tech talent. And that in itself has...
(46:56):
So that group, the News Product Alliance, has grown dramatically in the past three years since I've been a part of it, since the beginning. And we regularly produce the resources that are catered... Product management resources that are catered to working in a newsroom, because it is different, it is very unique. But there are of course frameworks and processes that are still relevant, right? Like editors who have been cobbling together how to make this thing that they want, but hey, maybe this PRD framework will help you, just to keep it on track. So these very simple, like rudimentary types of things have helped them so much. And so last year we kicked off a mentorship network in which we had about 400 people apply.
(47:46):
We accepted 150 mentees from across the world, and matched them with 50 mentors from different newsrooms across the world in leadership positions. And each mentee defined a goal that they wanted to work on, and all of course having to do with product management. And some of them were creating a product management role for themselves in the newsroom. Some of it was creating a product management practice, maybe they were given the title of product manager and said, "Okay, you can hire a couple of people." But it was all on them, and they had no idea what to do, right? And so some of it, it's across the board, and it's amazing to see people doing this from scratch who never in a million years thought they'd walk into it.
(48:32):
You know, they're not coming from big tech, they don't have startup backgrounds. They had journalism degrees from around the world, and they're learning how to do product, and build... Not just learning how to do it, they're building product practices, they're actually building and developing products and features, and they're creating more leaders in this space as well, and across their newsrooms to empower each other. So it's been really amazing to see, and it's the most valuable professional organization I've been a part of just because it is so niche that talking about product management is always fun. But when you talk about it at a level that specific, it becomes even more valuable.
Lenny (49:16):
That is some really impactful product work. Where can people find that if they want to maybe participate, maybe apply if you're doing another cohort in the future?
Upasna Gautam (49:24):
Go to Newsproduct.org, there is a very active Slack group with a couple thousand people in it, a very active and popular job board as well. And you just have to submit an application on the website... A form, sorry, on the website to join the Slack, and ten you'll get an invitation to join the Slack. And then you'll have all of the resources at your fingertips, it's very low barrier to entry, and we want everyone and everyone who is interested in the intersection of news and product to be a part of it. And the cool thing about it is like I mentioned, it's a global network. We have people from the biggest newsrooms across the world, to all of the regional ones, to even big tech, one of my fellow board members was doing news at Twitter.
(50:10):
One was working on the Google News Initiative. So it's a really, really awesome group of people, and everyone is so hungry and passionate about sharing and paying it forward.
Lenny (50:23):
We'll put a link to this in the show notes. One last question, and what kind of people are you looking for? Who should apply, if they're listening?
Upasna Gautam (50:29):
If you are interested in working in news and have a product background, or the inverse of that, if you work in news and you're interested in breaking into product. So either side of the coin or any combination of that, news media, product technology, even... We have a lot of audience development, and analytics, and data science folks in there too. It's just an amazing community for knowledge sharing in tech and news, so there will be a fit for you somewhere, or to learn something, or at the very least to meet other really amazing people.
Lenny (51:07):
Well, with that we've reached our very exciting lightning round. I've got five questions for you, and are you ready?
Upasna Gautam (51:14):
I'm ready.
Lenny (51:16):
All right. What are two or three books that you recommend most to other people, or that you've recommended most?
Upasna Gautam (51:21):
One, Mindfulness in Plain English. Two, How To Win Friends and Influence People. I have to attribute a lot of my communication skills to my dad forcing me to read that book when I was like 10 years old, and-
Lenny (51:36):
Wow, 10 years old. Intense.
Upasna Gautam (51:39):
He was an engineer, and they were in their very beginning phases of introducing... Well, at that time there was no product, but product-type stuff into his engineering practice. And so he had to read the book and was like, "You need to read this book." So I never forgot about it. And then the third one is actually, it's not a book, but it's an article that I think is still very powerful and relevant to this day. It's The New Product Development Game, published by Harvard Business Review in 1986. And it looks at a really different approach, one that I resonate a lot with because it's a lot along the lines of how we work at CNN, around rapid development, and time and flexibility being like key anchors of successful product development. So, I threw that in there as well.
Lenny (52:29):
Favorite recent movie or TV show? And it can't be White Lotus.
Upasna Gautam (52:32):
Okay, I haven't even seen that show.
Lenny (52:34):
Okay, great. That's come up so many times, we've got to not allow it anymore.
Upasna Gautam (52:38):
TV show is a toss up between The Mandalorian and Ted Lasso. I know it's not very new, but I don't watch tons and tons of TV, and I usually find like the few that I latch onto, and then I get obsessed with, and then I've got to take a break for a while. So, those are my last two obsessions. Movie is Everything Everywhere All At Once, the last amazing one that really, really struck a chord with me.
Lenny (53:02):
If you like Mandalorian check out Andor, it's incredible.
Upasna Gautam (53:05):
Oh yeah, it's good too.
Lenny (53:06):
It's like the best Star Wars thing. Okay, you've seen it. Okay, great, you're on it.
Upasna Gautam (53:09):
I still love Mandalorian more, I do like that one too.
Lenny (53:11):
Wow.
Upasna Gautam (53:12):
My husband is obsessed.
Lenny (53:15):
Okay, okay. Wow, contrarian. Favorite interview question that you like to ask candidates?
Upasna Gautam (53:21):
What's something that would not exist without your initiative?
Lenny (53:28):
Mm-hmm. And what do you look for in an answer that's a sign that this is someone you may want to hire?
Upasna Gautam (53:31):
The whole point of the question is like, do you have and can you exhibit high agency? And especially again, working in news it's so important. And so there's not like a specific answer, I think the ability to actually define something specific and tangible in itself is a really, really, really good sign. I think a lot of people definitely get startled by that question sometimes, because it requires you to kind of have to know what it is that you put on the table, right? So yeah, it's different every single time. But if they can define something out of that question, it's usually a positive sign.
Lenny (54:15):
Cool, thanks for sharing that additional detail. Next question, what's something relatively minor you've changed in your product development process at CNN or on your team specifically that's had a tremendous impact on your ability to execute?
Upasna Gautam (54:29):
Bridging the gap between product and editorial, that mutual trust and respect has had an outsized impact on our success as a high-performing team.
Lenny (54:38):
Sadly not something other people can use, but that's cool to know. Then we'll have to hire some journalists now, I think, to take advantage of that learning.
Upasna Gautam (54:46):
I can add onto that to make it relevant for others-
Lenny (54:50):
Sure.
Upasna Gautam (54:50):
... which is, it just goes back to, there's no substitute for having those direct conversations with your customers and your users. And it's like, when you understand that the assumptions you make are not one to one, or ever going to be one to one with their actual feedback. And developing that line of communication with your users and your customers is of utmost importance, I think that in itself is the key takeaway. It's like, how do you bridge the gap between collecting that feedback, having those conversations. Like, how far are you from your users, how frequently are you having conversations, how consistently are you having conversations? And I always think the more frequently and the more consistently, and earlier on in the process that you're having those conversations, the better.
Lenny (55:44):
Final question, maybe you already answered this. What's the most crazy or most memorable story from working at CNN?
Upasna Gautam (55:51):
Every election season is absolutely crazy in the best way possible. It's incredible to see our democracy in action, and watch history unfold. And while that's happening, using the platform you had a direct hand in building, and it's never lost on me that my work impacts the world every single day. And on the crazy, frustrating, stressful days, that is what serves as my anchor.
Lenny (56:19):
Awesome. Plus, this was amazing, I learned how to brush my teeth more mindfully, about elections, downtime, Trump, all these things. Final two questions, where can folks find you online if they want to reach out and learn more, and how can listeners be useful to you?
Upasna Gautam (56:34):
It's very easy to find me. Online I'm pretty active on Twitter and Instagram. I recently became an amateur content creator, so I share lots of stuff here and there on social. So any social platform... Well not any, only on Twitter, LinkedIn and Instagram. So any of those three is good, and I always love to hear feedback on stuff that resonated and made sense, and the stuff that there are further questions on. Always love having conversations around that. So yeah, DMs are always open.
Lenny (57:10):
I know we'll link to this in the show notes, but just, what's your handle on these networks?
Upasna Gautam (57:12):
It's my first and last name, very creative. I thought about it all by myself.
Lenny (57:16):
Awesome. And then I don't know if you answered the second question, how can listeners be useful to you?
Upasna Gautam (57:21):
It's the feedback, right? Like I love to hear what you found valuable, what helped you, what's a tactic that you took away and employed that helped you, things that made sense, things that didn't make sense, what you'd like to hear more about, any specific topic that would be valuable to do a deeper dive on. All of those things are very valuable to me.
Lenny (57:42):
Amazing. Again, thank you for being here, and adios.
Upasna Gautam (57:46):
Thank you.
Lenny (57:49):
Thank you so much for listening. If you found this valuable you can subscribe to the show on Apple Podcasts, Spotify, or your favorite podcast app. Also please consider giving us a rating or leaving a review, as that really helps other listeners find the podcast. You can find all past episodes or learn more about the show at Lennyspodcast.com. See you in the next episode.

---

### Figma’s CEO: Why AI makes design, craft, and quality the new moat for startups | Dylan Field
**Guest:** Dylan Field | **Date:** 2025-10-16 | [YouTube](https://www.youtube.com/watch?v=WyJV6VwEGA8)  

# Figma’s CEO: Why AI makes design, craft, and quality the new moat for startups | Dylan Field

## Transcript

Lenny Rachitsky (00:02):
Today I am excited to bring you a very special episode, which was recorded live at Figma Config with Figma CEO and co-founder, Dylan Field, in front of a live audience at the Moscone Center in San Francisco. This is the first ever live recording of this podcast and it was so much fun. If you watch this on YouTube, you can see the epic stage that they built specifically for us to recreate my podcast studio. I could not be more thankful to the Config team for making this happen.

(00:28):
In my conversation with Dylan, we dig into how he builds and refines his product taste and intuition, how intuition is a hypothesis generator, the future of product management. How Dylan attempts to operationalize keeping Figma simple and to continue simplifying the experience. A bunch of stories from the early days of Figma that I've never heard before. Also, he shares his favorite AI tool called websim, which is wild. And if you wait till the very end, you can see a very young child actor Dylan Field in a clip that I found online that was hilarious.

(01:00):
If you enjoy this podcast, don't forget to subscribe and follow it in your favorite podcasting app or YouTube. It's the best way to avoid missing future episodes and it helps the podcast tremendously. With that, I bring you Dylan Field.

(01:12):
Dylan, thank you so much for joining me and welcome to the podcast.

Dylan Field (01:16):
Thank you, Lenny.

Lenny Rachitsky (01:18):
Hi all.

Dylan Field (01:22):
Is this your first live podcast?

Lenny Rachitsky (01:24):
This is my first ever live podcast. Also, a big thank you to the Config team who set up this crazy studio. I had no idea this was going to happen. I feel like I'm in my studio here with a thousand people watching us. It's very impressive. I very much dig the background and also the mics that may or may not be wired.

Dylan Field (01:40):
That's right. Don't say that. Don't tell people.

Lenny Rachitsky (01:42):
Oh, sorry.

Dylan Field (01:44):
There's no wires coming out of them.

Lenny Rachitsky (01:45):
There's no one behind the curtain either. Okay, so Dylan, I want to start by just checking in on how you're doing. So Config is about to wrap up. We've been at it for two days now. I know how much lift goes into doing these sorts of things. I imagine you've been thinking about this for a long time now. I'm just curious how you're doing, any surprises, any highlights, any low lights?

Dylan Field (02:06):
The highlight is the community and just the incredible, incredible people here at Config. Y'all are awesome. I don't know why I keep talking in the mic like this. It's instinctual. But seriously, it's just the most amazing community to be part of and I feel so lucky. And then in terms of how I'm doing at this exact moment, exhausted, but riding on caffeine and whatever this really cool probiotic drink is.

Lenny Rachitsky (02:36):
Any surprises from the past couple of days? Anything that's like, "Oh wow, that went a lot better than I thought, maybe less well."

Dylan Field (02:44):
Demo, definitely things I would've improved. But also Emil and Mihika were phenomenal, and it was just so awesome to see them do their demos and present materials. I was just really pleased with the conversation, I think, that's getting started at Config around AI. I was looking online on social media and I think people are already zeroing in the right conversation, which is, okay, in a world of more software being created by AI, what does that mean and the impact on craft and the impact on quality and the need to have more unique design and how design is a differentiator.

(03:37):
And I think some people are saying, "I agree with that." Some people are saying, "That I disagree with that", and that's exactly the bounds of what the conversation I imagined would emerge from yesterday. It was funny, the make design feature, I think that I said on the keynote, I was like, "This is going to give you the most obvious thing in the most obvious form possible." And then people online are like, "It's just going to give you some obvious thing." I agree.

Lenny Rachitsky (04:10):
This episode is brought to you by WorkOS. If you're building a SaaS app, at some point your customers will start asking for enterprise features like SAML authentication and skim provisioning. That's where WorkOS comes in, making it fast and painless to add enterprise features to your app. Their APIs are easy to understand so that you can ship quickly and get back to building other features. Today, hundreds of companies are already powered by WorkOS, including ones you probably know like Versal, Webflow and Loom. WorkOS also recently acquired Warrant, the fine-grained authorization service. Warrant's product is based on a groundbreaking authorization system called Zanzibar, which was originally designed for Google to power Google Docs and YouTube. This enables fast authorization checks at enormous scale while maintaining a flexible model that can be adapted to even the most complex use cases. If you're currently looking to build role-based access control or other enterprise features like single sign-on, skim or user management, you should consider WorkOS. It's a drop-in replacement for Auth0 and supports up to one million monthly active users for free. Check it out at workos.com to learn more. That's workos.com.

(05:28):
This episode is brought to you by Anvil. Their Document SDK helps product teams build and launch software for documents fast. Companies like Carta and Vouch Insurance use Anvil to accelerate the development of their document workflows. Getting to market fast is a top priority for product teams and the last thing that you or your developers want is to build document workflows from scratch. It's time-consuming, expensive and distracts from core work. You could stitch together multiple tools and manage those integrations or you can use an all-in-one Document SDK.

(06:03):
Most product managers will tell you paperwork sucks. Anvil's Document SDK helps teams get to market fast, incorporate your brand style and give you back time to focus on your company's core differentiated features. For your users, paperwork often starts with an AI powered Webform styled and embedded in your application. From there, you can route data to your backend systems and to the correct fields in your PDFs via API. Complete the process with a white labeled e-signature. The best part about Anvil is the level of customization their SDK provides. Non-technical folks love Anvil's Drag and Drop builder and developers love their flexible APIs and easy to understand documentation. Build documents software fast with Anvil. That's useanvil.com/lenny to learn more or start a free trial. That's use-A-N-V-I-L.com/lenny.

(06:58):
Let's keep talking about design. You once said that the definition of design is art applied to problem solving. Can you just add a bit more to that? What do you mean by that? Because that's an amazing line.

Dylan Field (07:09):
Well, I don't think it's my original line. I think someone else said it, but there's a lot of definitions of design out there too. There's also 'design is dialogue' or 'design is problem solving'. You just go straight there. I could go with 10 more. But I like art applied to problem solving because I think that design is often... There is some component of creativity to it and unique expression that you're trying to provide and create and put out into the world. But you are also trying to do it and match it to a user need, a problem that needs to be solved. And I think that it's not pure art, but if you lose the art and you're just solving the problem, it's totally utilitarian and it lacks soul. And so the combination of those two things is to me really beautiful.

Lenny Rachitsky (08:03):
I'm going to pivot to a very hard hitting question. I hope your PR people don't kill me for asking this. Many people asked me to ask you this question. Very important. Please explain a Figma tradition called raccoon feet and muffin hands.

Dylan Field (08:21):
I should probably just leave this interview now. So this is a conversation, I'm not sure exactly where it started, but it started in early Figma. And basically we had these lunch tables at Figma where we would just all gather and have very long, interesting meandering conversations before we got back to work. And one of the questions that, was a 'would you rather', was would you rather have raccoons for feet or muffins for hands? And I think this is a deeply philosophical question. I have pondered it since I've heard it. I still don't have one answer. If you've got an answer, I'm curious what it is.

Lenny Rachitsky (09:02):
I've got follow up questions. Can you control where the raccoon take you or are they just deciding on their own what's happening?

Dylan Field (09:09):
I think that raccoons probably wouldn't even agree with each other where to go.

Lenny Rachitsky (09:14):
Okay, that's complicated.

Dylan Field (09:16):
If you had raccoons for feet right now, do you think that it would interfere with this podcast?

Lenny Rachitsky (09:21):
But muffin hands would also interfere with my newsletter and I feel like I'd be out of work.

Dylan Field (09:25):
I don't know if you can type.

Lenny Rachitsky (09:26):
I'd need a special keyboard. This is very difficult.

Dylan Field (09:29):
You haven't even thought about the upsides of this yet.

Lenny Rachitsky (09:33):
What are the upsides?

Dylan Field (09:34):
We can get there, it's all-

Lenny Rachitsky (09:36):
Maybe I could eat some of the muffins.

Dylan Field (09:36):
It's the case for optimism.

Lenny Rachitsky (09:37):
Cupcakes?

Dylan Field (09:38):
If you have muffins for hands, maybe if you're hungry...

Lenny Rachitsky (09:41):
Do they regenerate as you you eat them?

Dylan Field (09:43):
That's a good question. There's no answers here, just questions. Do your nails grow?

Lenny Rachitsky (09:49):
Yes.

Dylan Field (09:50):
Oh, okay. Interesting. It's deeper than you might think.

Lenny Rachitsky (09:58):
I'm going to play a short clip with Rick Rubin and then I have a question about it. So we'll see if that plays.

Speaker 3 (10:06):
But exactly what he does and how is difficult to describe. Do you play instruments?

Rick Rubin (10:12):
Barely.

Speaker 3 (10:13):
Do you know how to work a soundboard?

Rick Rubin (10:15):
No. I have no technical ability and I know nothing about music.

Speaker 3 (10:21):
Then you must know something.

Rick Rubin (10:22):
Well, I know what I like and what I don't like. And I'm decisive about what I like and what I don't like.

Speaker 3 (10:29):
So what are you being paid for?

Rick Rubin (10:31):
The confidence that I have in my taste and my ability to express what I feel has proven helpful for artists.

Lenny Rachitsky (10:45):
So I'm not going to say this is you. You need to grow the beard. But I think this is a little bit you because what I've heard from a number of your colleagues is that one of your superpowers is intuition and product taste. And someone said that you have the sixth sense for what's going to work, when you're designing Figma and you're making decisions in the product. So I'm curious how you've built and refined your intuition and product taste when it comes to Figma and then even broadly.

Dylan Field (11:13):
That's a lot kinder than I thought you were going to be. I thought you're going to be like, "You don't know how to code and you don't know how to design."

Lenny Rachitsky (11:17):
No.

Dylan Field (11:19):
But no, here's my framework for it. I think intuition is like a hypothesis generator and you're constantly generating these hypotheses and others are generating hypotheses as well. And you then take these hypotheses and you put them forward and you debate them and you try to find data to support them or negate them. And then you winnow it down into what is our working hypothesis? And from that you move forward.

Lenny Rachitsky (11:48):
I heard that you read every tweet that mentions Figma and share them with folks. There's a Slack channel where you paste them. I imagine that is a part of this where you're just constantly watching what people are saying about Figma, what people are complaining about.

Dylan Field (12:00):
I definitely look everywhere trying to constantly ingest information about Figma, and it's not just Twitter/X, whatever that's called now, but anywhere on the internet, support channels, et cetera. And I'm always trying to understand. I also ask a lot of questions and I try to get to root problems and understand where people are coming from and what are they actually trying to solve. Sometimes people are saying, "Hey, I need X", but they really want Y or Z. And trying to do that myself and engage and dive deeper there, but also to encourage our team to do that, I think leads to really good outcomes in terms of what we ship.

Lenny Rachitsky (12:51):
Is there something you've changed your mind about, building on that, either based on customer feedback or some employee just making a case and like, "Okay, you're right." Is there something that comes to mind of something you've changed your mind about recently? Somebody said Flides.

Dylan Field (13:09):
For when we started out Flides. I have not. It's Figma Slides. Well, it's not recent, but one good example of me changing my mind is that you all have Pages in Figma, you're welcome. But I think I have deep skepticism of Pages still. I'm not sure they're... If you could freeze time and I could just go in with my team, work on Figma for a very long time, I'm not sure we'd come to the same implementation of Pages that we are at today. I just don't think it's the most elegant solution in the context of the entire system of product design that you could create. The world told me and our team that that did not matter and they needed Pages. And don't worry, we're not shipping Pages. But I am still very skeptical of them and I think that in general, probably my team would tell you that I don't always change my mind, but I also build trust with people in deep ways.

(14:22):
And I think across our organization, if things are not going to be fatal, then if I hear from someone, "Hey, I really think we should do X", then I'll say, "Okay, just go with it. And here's my feedback, here's what I'm skeptical of, let's see what happens." And then sometimes they come back to me and they're like, "See I was right." But usually they're pretty polite about it.

Lenny Rachitsky (14:49):
Just to build on that, something a lot of people try to work on is being good at influencing leadership execs, CEOs. What do you find works to change your mind? What do people come to you with that helps you like, "Okay, you're actually right?"

Dylan Field (15:05):
I think the more concrete an artifact is or the more you can debate something, the better. I ask for examples a lot, I try to ask follow up questions about things and make sure I fully understand it. And I think where I get stuck sometimes is if I ask follow up questions and we don't have answers yet, and then my response might be, "Let's go find the answer to these questions and then let's go back to this conversation", if I think it's something that's really important. And I think for some people they might go, "Okay, this is actually really obvious. I can't believe you're so dense and you don't get it yet." And sometimes they're right and they come back and they're like, "Okay, here's the data now, can we move on?" And we do, we move on and they're right. And I just think that it's important though to just really understand something from first principles for a lot of decisions. And maybe it's just a perfectionist quality repeated over time, I think it leads to good outcomes as long as you make sure it's not bottle-necking the organization.

Lenny Rachitsky (16:15):
So following up on that, let's talk about product management. So last year you had Brian Chesky here, I think maybe on this stage, maybe a bigger stage. And he said that they got rid of product management at Airbnb and everyone cheered and all the PMs were very sad. And he didn't actually mean they got rid of product management, they changed the function and evolved it. I'm curious just to get your take.

Dylan Field (16:38):
It's funny. This year we have you here Lenny, so that's your answer. No...

Lenny Rachitsky (16:43):
I had him on the-

Dylan Field (16:45):
Before and after, all. Surprise.

Lenny Rachitsky (16:45):
We're still here. We're still here. I want to get your take on product management. You all have amazing product managers at Figma. I've had three of them on the podcast already. I'm curious just what value you find the best product managers bring to Figma?

Dylan Field (17:00):
It was really funny last year after that interview, so Yuhki, our chief product officer, had invited me to a dinner for our PM team. And it took a while to get out of Config at the end of the day, and I eventually made the dinner but I was 40 minutes late. And I walk in and Mihika who was on stage yesterday, presenting Figma Slides, Flides, she was standing up and doing a mock Brian Chesky impersonation. And she's standing up in front of the entire product team and she goes, "And then Brian Chesky's like, 'There don't need to be any PMs.' And Dylan's like... Ooh." And I'm like, "Hi, Mihika." And I'd never seen her so red. And then I gave a quick, "Hey PM team, I believe in you. Thank you for your hard work."

(18:08):
Seriously, I think that if you zoom out, it's always tricky whenever you're asked to formally define, what is the separation between a product manager, a designer, and an engineer? It's always hard to actually create those clear lines. And I think in many organizations they're blurry. But at the end of the day, a PM and designer, they need to have some technical expertise or at least understand how some systems work to probably create the best things they can possibly make. A designer, engineer, they should probably have some sense of the business objectives. They should have some sense of what users want. An engineer and a product manager, they should have taste and craft and some sense of the option space, and some desire to care about the visual implementation.

(19:08):
And I think you can include research in there too, if you want to make it four legs of the stool rather than the trio. And you can talk about all three probably should have exposure to users and be talking in dialogue with users. So I think that if you think about that group holistically, each is important. If you think about a team, there's all these qualities that you have to have to make a great product. And that said, I think for product managers and the product function... I think sometimes when you see people that fall down in that function is because they treat it too much like process. Which is very important too, don't me wrong. Good process can help support good outcomes. But I think that you can't lose sight of the problems that you're solving. You have to go talk to users and you have to actually have a strategy. And if you're really good, you should have a point of view. And some point of views are going to lead to good outcomes and some point of views aren't. And there's some tense sense of taste.

(20:16):
And you also have to bring everyone together and make sure that they get to the objective, that it's celebrated, and that at the end of the project or when you complete a milestone, everyone's stoked. Otherwise, it's not going to be a team that gels, you're not going to get to the next outcome. Even if you get to an outcome and it's a milestone, but if everyone's unhappy, you failed. And so somehow good product people are able to do all this and they're able to create great frameworks that bring everyone along with them. And so everyone's able to have a shared head space around what it is they're trying to get to.

Lenny Rachitsky (20:54):
Someone once said that if PMs disappeared or if a PM goes on vacation, everything's okay for a week or two or three and then things start to crumble a little bit because they glue everything together. Do you find that sort of thing? Let me actually ask a different question along those lines, are you bearish or bullish on the future of product management? Do you think PMs will continue the way they are? Do you think PMs will dwindle any sense of the future of product management?

Dylan Field (21:23):
I think probably everyone's learning to do a bit more of everyone else's job in this current moment. That said, I definitely think there's still immense value in product, immense value in design, immense value in engineering. And so I think those roles will continue to exist.

Lenny Rachitsky (21:43):
So maybe I just want to come back to the question of just, with the best PMs that you work with, do you find, what value do they most bring? I guess is there anything that's like, "Here's what would be gone if we didn't have these PMs"?

Dylan Field (21:57):
The best PMs, I think again, create those frameworks that bring everyone else along and those frameworks also have a point of view and a strategy associated with them. So you're able to take the strategy, take the point of view, wrap it all up in a framework, and then make it so that everyone knows what the destination is and how to get there.

Lenny Rachitsky (22:20):
So along these lines, something I've heard you're really big on is simplification. Somebody told me that when you're in a designer view and things just feel too complex to you, quote, "You furrow your brow and insist there must be something simpler." Why is simplification so top of mind for you, why is it so important for you and just why is it so hard to do?

Dylan Field (22:41):
Oh, gosh. Well, I think probably anyone here who's worked on product knows how hard it is. I think the more that you add, the harder it is to create something that's coherent. One essay that Evan, my co-founder, introduced me to early on in famous history, I think from Stevie's [inaudible 00:23:06] grants or something like that, contains the term irreducible complexity. And it's basically this idea that one plus one does not equal three, it sometimes equals one and a half. And the more that you add and the more that you continue to put in something, the more complex it gets and the worse it gets. And I think this is definitely true for tools.

(23:28):
So in the context of Figma, we can make it more powerful, but to do that in a way that's not making it more complex at the same time is extremely hard. And we have to always be paying attention to how complex or how simple things are because if we don't, it just becomes a monstrosity really fast. And there's parts of our product that, I don't want to dive into that part of the conversation, the self-critique, but definitely as I'm in conversation with a bunch of our product leaders at Figma, there's parts where it's like, "Okay, this thing is too complex as a system and we made all the right local decisions and yet together they're too complex and they're not working anymore. And let's go revisit the system now."

Lenny Rachitsky (24:14):
This episode is brought to you by UserTesting. Transform how you build products and experiences with UserTesting. Get fast feedback throughout the development process so that you can build the right thing the first time. Make better decisions that lead to better business outcomes. Companies are being asked to do more with less. They need to move quickly to build experiences that meet changing customer expectations and do so faster than ever, all while minimizing risk and costly rework. With UserTesting, you have a trusted partner in experience research. They empower user research product and design teams to make higher confidence decisions with human insights. Learn more today at usertesting.com/lenny.

(24:59):
I know you just redesigned Figma. I imagine part of that came from things are just getting too complicated, not as simple as we want. Is there anything that's been bugging you in the old Figma but like, "Oh, this is way too complicated, I really want to simplify this thing"?

Dylan Field (25:09):
Yes.

Lenny Rachitsky (25:11):
What's that?

Dylan Field (25:12):
We'll move on, but many things.

Lenny Rachitsky (25:18):
Sounds good. And in terms of how to keep things simple, so I had Dharmesh Shah on the podcast, he's the co-founder of HubSpot, and the way he described it is that you're always fighting the second loft through more dynamics of entropy, just the product getting more complicated. And he sees himself as part of the solution, of top down, you have to be on top of that. Is that the way you see it? That's your role, to keep things simple. Do you think people further down the ladder can do that?

Dylan Field (25:46):
Absolutely, everyone's responsible for simplicity. And I think another quote that is not mine but is a really a good one is "Keep the simple things simple. Make the complex things possible." And I think that's a really important principle to hold as you're designing tools. And I'd say that it's really easy to make the simple things complex, unfortunately.

Lenny Rachitsky (26:10):
I want to pivot to talking about early days Figma. So I don't know how many people know this, but it took three and a half years to launch Figma from when you were beginning to work on it.

Dylan Field (26:19):
Way too long, don't do that.

Lenny Rachitsky (26:22):
This is my question. So it took three and a half years to launch and then five years to get your first customer. Dylan, what the hell were you doing all that time?

Dylan Field (26:28):
I don't think it took five years for a first... Well okay-

Lenny Rachitsky (26:32):
Paying.

Dylan Field (26:33):
First paying customer, sure. Okay, fine. Slightly less but approximately five years, it gets to be round up. I think that if I had been probably better at hiring and recruiting... I see Nadia in the audience, making eye contact with her the entire time, for some reason. She's our chief people officer. If she had been at Figma from day one, we would've hired probably faster and we would've gotten to market faster. But I think that it was a hard product to build and to get everything to come together with. I also see Sho. And I think for... Sho's joined us as a director of engineering. He's a VP of product now. Again, people can wear many hats. And he was someone that joined Figma and said, "Hey, y'all need to ship this thing, you're really close." And he really helped catalyze us to ship in that moment. And I think, in week one, he gave a presentation. It was like, "Here's what we got to do, here's the gap. Everyone agrees on it. Let's go."

Lenny Rachitsky (27:45):
You already said that you wish you shipped earlier. Is there any advice there for just people building something today of-

Dylan Field (27:49):
Get it out as fast as you possibly can. Everything they tell you about making sure that you get a product out really quickly is totally true. The faster you get it out, the more feedback you get. That is a positive thing. And now I index on that when we try to build. And FigJam's a great example of that, we shipped it incredibly fast and it helped us get to market and get feedback faster. Figma Slides, great example of that too. Dev Mode, for what it's worth, it took us longer. We just had to keep iterating and building it and building it again. Certain directions we tried didn't work out and we really had to get to a place where we were able to really believe that we were adding value and really understood the developer's user, and it just didn't happen for a long time. So it's interesting because I think people look at Dev Mode and sometimes they go, "Oh, this is quite simple", to the point about simplicity.

(28:53):
Figma, is this simpler than FigJam? And the reality was it took at least three times as long.

Lenny Rachitsky (29:04):
So your advice is ship quickly. There's also this push the-

Dylan Field (29:09):
I'd hold the bar, for sure.

Lenny Rachitsky (29:11):
That's the question I have, is there's also a lot of talk of just the bar has risen. You need, especially B2B software, craft is really important. Linear talks a lot about this, just the bar is very high for people to switch from something out there. Is there anything... I don't think you'll have, "Here's the answer. When you're ready to ship...", but just any advice of just like, "Here's good enough" versus "No, you should probably wait."

Dylan Field (29:32):
Well, another thing that Evan taught me was that for a new launch, you got quality, features, deadline, choose two. And I think that the beautiful thing about software is you can keep iterating on it. So it's not like a physical product where you have to always have quality in there, otherwise it's never going to have quality. You can ship it with features and deadline and then improve it iteratively over time. I'm not saying you should always do that. Sometimes you need to at least have a minimum bar of quality for the things you have and you're going to ship less features maybe.

(30:04):
So you choose quality and deadline and sometimes you say, "Actually here's the minimum feature set and we're going to have this quality bar and you're willing to push it out." But I think you have to know when you're introducing a new thing, what it's going to take and then to make that minimally awesome product. But also I think that when you're iteratively improving it, you shouldn't just be focused on the features, you have to focus on the quality too.

Lenny Rachitsky (30:33):
I like this term you use, 'minimally awesome product'. Love it. So the way you got your early users for Figma is quite fascinating. I don't know how many people know this story, but you basically wrote a script to scrape Twitter and create a graph of the most influential designers on Twitter, and then you made it your mission to convince them to use Figma and make them evangelists. Is there anything more to the story there? And then I have a question along those lines.

_[195 additional lines trimmed for context budget]_

---

### The Godmother of AI on jobs, robots & why world models are next | Dr. Fei-Fei Li
**Guest:** Fei Fei | **Date:** 2025-11-16 | [YouTube](https://www.youtube.com/watch?v=Ctjiatnd6Xk)  

# The Godmother of AI on jobs, robots & why world models are next | Dr. Fei-Fei Li

## Transcript

Lenny Rachitsky (00:00:00):
A lot of people call you the godmother of AI. The work you did actually was the spark that brought us out of AI winter.

Dr. Fei Fei Li (00:00:07):
In the middle of 2015, middle of 2016, some tech companies avoid using the word AI because they were not sure if AI was a dirty word. 2017-ish was the beginning of companies calling themselves AI companies.

Lenny Rachitsky (00:00:22):
There's this line, I think, this was when you were presenting to Congress. There's nothing artificial about AI. It's inspired by people. It's created by people, and most importantly, it impacts people.

Dr. Fei Fei Li (00:00:30):
It's not like I think AI will have no impact on jobs or people. In fact, I believe that whatever AI does, currently or in the future, is up to us. It's up to the people. I do believe technology is a net positive for humanity, but I think every technology is a double-edged sword. If we're not doing the right thing as a society, as individuals, we can screw this up as well.

Lenny Rachitsky (00:00:56):
You had this breakthrough insight of just, okay, we can train machines to think like humans, but it's just missing the data that humans have to learn as a child.

Dr. Fei Fei Li (00:01:03):
I chose to look at artificial intelligence through the lens of visual intelligence because humans are deeply visual animals. We need to train machines with as much information as possible on images of objects, but objects are very, very difficult to learn. A single object can have infinite possibilities that is shown on an image. In order to train computers with tens and thousands of object concepts, you really need to show it millions of examples.

Lenny Rachitsky (00:01:36):
Today, my guest is Dr. Fei-Fei Li, who's known as the godmother of AI. Fei-Fei has been responsible for and at the center of many of the biggest breakthroughs that sparked the AI revolution that we're currently living through. She spearheaded the creation of ImageNet, which was basically her realizing that AI needed a ton of clean-labeled data to get smarter, and that data set became the breakthrough that led to the current approach to building and scaling AI models. She was chief AI scientist at Google Cloud, which is where some of the biggest early technology breakthroughs emerged from. She was director at SAIL, Stanford's Artificial Intelligence Lab, where many of the biggest AI minds came out of. She's also co-creator of Stanford's Human-Centered AI Institute, which is playing a vital role in a direction that AI is taking. She's also been on the board of Twitter. She was named one of Time's 100 Most Influential People in AI. She's also United Nations advisory board. I could go on.

(00:02:29):
In our conversation, Fei-Fei shares a brief history of how we got to today in the world of AI, including this mind-blowing reminder that 9 to 10 years ago, calling yourself an AI company was basically a death knell for your brand because no one believed that AI was actually going to work. Today, it's completely different. Every company is an AI company. We also chat about her take on how she sees AI impacting humanity in the future, how far current technologies will take us, why she's so passionate about building a world model and what exactly world models are, and most exciting of all, the launch of the world's first large world model, Marble, which just came out as this podcast comes out. Anyone can go play with this at marble.worldlabs.ai. It's insane. Definitely check it out. Fei-Fei is incredible and way too under the radar for the impact that she's had on the world, so I am really excited to have her on and to spread her wisdom with more people.

(00:03:22):
A huge thank you to Ben Horowitz and Condoleezza Rice for suggesting topics for this conversation. If you enjoy this podcast, don't forget to subscribe and follow it in your favorite podcasting app or YouTube. With that, I bring you Dr. Fei-Fei Li after a short word from our sponsors.

(00:03:37):
This episode is brought to you by Figma, makers of Figma Make. When I was a PM at Airbnb, I still remember when Figma came out and how much it improved how we operated as a team. Suddenly, I can involve my whole team in the design process, give feedback on design concepts really quickly and it just made the whole product development process so much more fun. But Figma never felt like it was for me. It was great for giving feedback and designs, but as a builder, I wanted to make stuff. That's why Figma built Figma Make. With just a few prompts, you can make any idea or design into a fully functional prototype or app that anyone can iterate on and validate with customers.

(00:04:15):
Figma Make is a different kind of vibe coding tool. Because it's all in Figma, you can use your team's existing design building blocks, making it easy to create outputs that look good and feel real and are connected to how your team builds. Stop spending so much time telling people about your product vision and instead show it to them. Make code-back prototypes and apps fast with Figma Make. Check it out at figma.com/lenny.

(00:04:40):
Did you know that I have a whole team that helps me with my podcast and with my newsletter? I want everyone on that team to be super happy and thrive in the roles. Justworks knows that your employees are more than just your employees; they're your people. My team is spread out across Colorado, Australia, Nepal, West Africa, and San Francisco. My life would be so incredibly complicated to hire people internationally, to pay people on time and in their local currencies, and to answer their HR questions 24/7. But with Justworks, it's super easy. Whether you're setting up your own automated payroll, offering premium benefits, or hiring internationally, Justworks offer simple software and 24/7 human support from small business experts for you and your people. They do your human resources right so that you can do right by your people. Justworks, for your people.

(00:05:31):
Fei-Fei, thank you so much for being here and welcome to the podcast.

Dr. Fei Fei Li (00:05:34):
I'm excited to be here, Lenny.

Lenny Rachitsky (00:05:36):
I'm even more excited to have you here. It is such a treat to get to chat with you. There's so much that I want to talk about. You've been at the center of this AI explosion that we're seeing right now for so long. We're going to talk about a bunch of the history that I think a lot of people don't even know about how this whole thing started, but let me first read a quote from Wired about you just so people get a sense, and in the intro I'll share all of the other epic things you've done. But I think this is a good way to just set context. "Fei-Fei is one of a tiny group of scientists, a group perhaps small enough to fit around a kitchen table, who are responsible for AI's recent remarkable advances."

(00:06:10):
A lot of people call you the godmother of AI, and unlike a lot of AI leaders, you're an AI optimist. You don't think AI is going to replace us. You don't think it's going to take all our jobs. You don't think it's going to kill us. So I thought it'd be fun to start there, just what's your perspective on how AI is going to impact humanity over time?

Dr. Fei Fei Li (00:06:30):
Yeah, okay, so Lenny, let me be very clear. I'm not a utopian, so it's not like I think AI will have no impact on jobs or people. In fact, I'm a humanist. I believe that whatever AI does, currently or in the future, is up to us. It's up to the people. So I do believe technology is a net positive for humanity. If you look at the long course of civilization, I think we are, and fundamentally, we're an innovative species that we... If you look at from written record thousands of years ago to now, humans just kept innovating ourselves and innovating our tools, and with that, we make lives better, we make work better, we build civilization, and I do believe AI is part of that. So that's where the optimism comes from. But I think every technology is a double-edged sword, and if we're not doing the right thing as a species, as a society, as communities, as individuals, we can screw this up as well.

Lenny Rachitsky (00:07:47):
There's this line, I think, this was when you were presenting to Congress, "There's nothing artificial about AI. It's inspired by people. It's created by people, and most importantly, it impacts people." I don't have a question there, but what a great line.

Dr. Fei Fei Li (00:07:59):
Yeah, I feel pretty deeply. I started working AI two and a half decades ago, and I've been having students for the past two decades and almost every student who graduates, I remind them when they graduate from my lab that your field is called artificial intelligence, but there's nothing artificial about it.

Lenny Rachitsky (00:08:23):
Coming back to the point you just made about how it's kind of up to us about where this all goes, what is it you think we need to get right? How do we set things on a path? I know this is a very difficult question to answer, but just what's your advice? What do you think we should be keeping in mind?

Dr. Fei Fei Li (00:08:36):
Yeah, how many hours do we have?

Lenny Rachitsky (00:08:39):
How do we align AI? There we go. Let's solve it.

Dr. Fei Fei Li (00:08:41):
So I think people should be responsible individuals no matter what we do. This is what we teach our children, and this is what we need to do as grownups as well. No matter which part of the AI development or AI deployment or AI application you are participating in, and most likely many of us, especially as technologists, we're in multiple points. We should act like responsible individuals and care about this. Actually, care a lot about this. I think everybody today should care about AI because it is going to impact your individual life. It is going to impact your community, it's going to impact the society and the future generation. And caring about it as a responsible person is the first, but also the most important step.

Lenny Rachitsky (00:09:37):
Okay, so let me actually take a step back and kind of go to the beginning of AI. Most people started hearing and caring about AI, as what it's called today, just like, I don't know, a few years ago when ChatGPT came out. Maybe it was like three years ago.

Dr. Fei Fei Li (00:09:51):
Three years ago, almost one more month, three years ago.

Lenny Rachitsky (00:09:55):
Wow, okay. And that was ChatGPT coming out. Is that the milestone you have in mind?

Dr. Fei Fei Li (00:09:56):
Yes.

Lenny Rachitsky (00:09:57):
Okay, cool. That's exactly how I saw it. But very few people know there was a long, long history of people working on, it was called machine learning back then and there's other terms, and now it's just everything's AI and there was kind of a long period of just a lot of people working on it. And then there's this what people refer to as the AI winter where people just gave up almost, most people did, and just, okay, this idea isn't going anywhere. And then the work you did actually was essentially the spark that brought us out of AI winter and is directly responsible for the world where now of just AI is all we talk about. As you just said, it's going to impact everything we do. So I thought it'd be really interesting to hear from you just the brief history of what the world was like before ImageNet and just the work you did to create ImageNet, why that was so important, and then just what happened after.

Dr. Fei Fei Li (00:10:44):
It is, for me, hard to keep in mind that AI is so new for everybody when I lived my entire professional life in AI. There's a part of me that is just, it's so satisfying to see a personal curiosity that I started barely out of teenagehood and now has become a transformative force of our civilization. It generally is a civilizational level technology. So that journey is about 30 years or 20 something, 20 plus years, and it's just very satisfying. So where did it all start? Well, I'm not even the first generation AI researcher. The first generation really date back to the '50s and '60s, and Alan Turing was ahead of his time in the '40s by asking, daring humanity with the question, "Is there thinking machines?" And of course he has a specific way of testing this concept of thinking machine, which is a conversational chatbot, which to his standard we now have a thinking machine.

(00:12:02):
But that was just a more anecdotal inspiration. The field really began in the '50s when computer scientists came together and look at how we can use computer programs and algorithms to build these programs that can do things that have been only capable by human cognition. And that was the beginning. And the founding fathers the Dartmouth workshop in the 1956, we have Professor John McCarthy who later came to Stanford who coined the term artificial intelligence. And between the '50s, '60s, '70s, and '80s, it was the early days of AI exploration and we had logic systems, we had expert systems, we also had early exploration of neural network. And then it came to around the late '80s, the '90s, and the very beginning of the 21st century. That stretch about 20 years is actually the beginning of machine learning, is the marriage between computer programming and statistical learning.

(00:13:23):
And that marriage brought a very, very critical concept into AI, which is that purely rule-based program is not going to account for the vast amount of cognitive capabilities that we imagine computers can do. So we have to use machines to learn the patterns. Once the machines can learn the patterns, it has a hope to do more things. For example, if you give it three cats, the hope is not just for the machines to recognize these three cats. The hope is the machines can recognize the fourth cat, the fifth cat, the sixth cat, and all the other cats. And that's a learning ability that is fundamental to humans and remaining animals. And we, as a field, realized, "We need machine learning." So that was up till the beginning of the 21st century. I entered the field of AI literally in the year of 2000. That's when my PhD began at Caltech.

(00:14:33):
And so I was one of the first generation machine learning researchers and we were already studying this concept of machine learning, especially neural network. I remember that was one of my first courses at Caltech is called neural network, but it was very painful. It was still smack in the middle of the so-called AI winter, meaning the public didn't look at this too much. There wasn't that much funding, but there was also a lot of ideas flowing around. And I think two things happened to myself that brought my own career so close to the birth of modern AI is that I chose to look at artificial intelligence through the lens of visual intelligence because humans are deeply visual animals. We can talk a little more later, but so much of our intelligence is built upon visual, perceptual, spatial understanding, not just language per se. I think they're complementary.

(00:15:37):
So I choose to look at visual intelligence and my PhD and my early professor years, my students and I are very committed to a north star problem, which is solving the problem of object recognition because it's a building block for the perceptual world, right? We go around the world interpreting reasoning and interacting with it more or less at the object level. We don't interact with the world at the molecular level. We don't interact with the world as... We sometimes do, but we rarely, for example, if you want to lift a teapot, you don't say, "Okay, the teapot is made of a hundred pieces of porcelain and let me work on this a hundred pieces." You look at this as one object and interact with it. So object is really important. So I was among the first researchers to identify this as a north star problem, but I think what happened is that as a student of AI and a researcher of AI, I was working on all kinds of mathematical models including neural network, including Bayesian network, including many, many models.

(00:16:53):
And there was one singular pain point is that these models don't have data to be trained on. And as a field, we were so focusing on these models, but it dawned on me that human learning as well as evolution is actually a big data learning process. Humans learn with so much experience constantly. In the evolution, if you look at time, animals evolve with just experiencing the world. So I think my students and I conjectured that a very critically-overlooked ingredient of bringing AI to life is big data. And then we began this ImageNet project in 2006, 2007. We were very ambitious. We want to get the entire internet's image data on objects. Now granted internet was a lot smaller than today, so I felt like that ambition was at least not too crazy. Now, it's totally delusional to think a couple of graduate student and a professor can do this.

(00:18:05):
And that's what we did. We curated very carefully, 15 million images on the internet, created a taxonomy of 22,000 concepts, borrowing other researchers' work like linguists work on WordNet, and it's a particular way of dictionarying words. And we combine that into ImageNet and we open-sourced that to the research community. We held an annual ImageNet challenge to encourage everybody to participate in this. We continue to do our own research, but 2012 was the moment that many people think was the beginning of the deep learning or birth of modern AI because a group of Toronto researchers led by Professor Geoff Hinton, participated in ImageNet Challenge, used ImageNet big data and two GPUs from NVIDIA and created successfully the first neural network algorithm that can...

(00:19:12):
It didn't totally solve, but made a huge progress towards solving the problem of object recognition. And that combination of the trio technology, big data, neural network, and GPU was kind of the golden recipe for modern AI. And then fast-forward, the public moment of AI, which is the ChatGPT moment, if you look at the ingredients of what brought ChatGPT to the world technically still use these three ingredients. Now, it's internet-scale data mostly texts is a much more complex neural network architecture than 2012, but it's still neural network and a lot more GPUs, but it's still GPUs. So these three ingredients are still at the core of modern AI.

Lenny Rachitsky (00:20:16):
Incredible. I have never heard that full story before. I love that it was two GPUs was the first. I love that. And now it's, I don't know, hundreds of thousands, right, that are orders of magnitude more powerful.

Dr. Fei Fei Li (00:20:30):
Yep.

Lenny Rachitsky (00:20:31):
And those two GPUs where they just bought, they were like gaming GPUs, they just went to the-

Dr. Fei Fei Li (00:20:34):
Yes.

Lenny Rachitsky (00:20:35):
... GameStar that people use for playing games. As you said, this continues to be in a large way, the way models get smarter. Some of the fastest growing companies in the world right now, I've had them all mostly on the podcast, Mercor and Surge and Scale. They continue to do this for labs, just give them more and more label data of the things they're most excited and interested in.

Dr. Fei Fei Li (00:20:53):
Yeah, I remember Alex Wang from Scale very early days. I probably still has his emails when he was starting Scale. He was very kind. He keeps sending me emails about how image that inspired Scale. I was very pleased to see that.

Lenny Rachitsky (00:21:08):
One of my other favorite takeaways from what you just shared is just such an example of high agency and just doing things that's kind of a meme on Twitter. Just you can just do things. You're just like, okay, this is probably necessary to move AI. And it's called machine learning back then, right? Was that the term most people used?

Dr. Fei Fei Li (00:21:25):
I think it was interchangeably. It's true. I do remember the companies, the tech companies, I am not going to name names, but I was in a conversation in one of the early days, I think is in the middle of 2015, middle of 2016, some tech companies avoid using the word AI because they were not sure if AI was a dirty word. And I remember I was actually encouraging everybody to use the word AI because to me that is one of the most audacious question humanity has ever asked in our quest for science and technology, and I feel very proud of this term. But yes, at the beginning some people were not sure.

Lenny Rachitsky (00:22:12):
What year was that roughly when AI was a dirty word?

Dr. Fei Fei Li (00:22:14):
2016, I think because that was-

Lenny Rachitsky (00:22:15):
2016, less than 10 years ago.

Dr. Fei Fei Li (00:22:18):
That was the changing. Some people start calling it AI, but I think if you look at the Silicon Valley tech companies, if you trace their marketing term, I think 2017-ish was the beginning of companies calling themselves AI companies.

Lenny Rachitsky (00:22:40):
That's incredible. Just how the world has changed.

Dr. Fei Fei Li (00:22:43):
Yes.

Lenny Rachitsky (00:22:43):
Now, you can't not call yourself an AI company.

Dr. Fei Fei Li (00:22:46):
I know.

Lenny Rachitsky (00:22:46):
Just nine-ish years later.

Dr. Fei Fei Li (00:22:48):
Yeah.

Lenny Rachitsky (00:22:49):
Oh, man. Okay. Is there anything else around the history, that early history that you think people don't know that you think is important before we chat about where you think things are going and the work that you're doing?

Dr. Fei Fei Li (00:23:01):
I think as all histories, I'm keenly aware that I am recognized for being part of the history, but there are so many heroes and so many researchers. We're talking about generations of researchers. In my own world, there are so many people who have inspired me, which I talked about in my book, but I do feel our culture, especially Silicon Valley, tends to assign achievements to a single person. While I think it has value, but it's just to be remembered. AI is a field of, at this point, 70 years old and we have gone through many generations. Nobody, no one could have gotten here by themselves.

Lenny Rachitsky (00:23:54):
Okay, so let me ask you this question. It feels like we're always on this precipice of AGI, this kind of vague term people throw around, AGI is coming, it's going to take over everything. What's your take on how far you think we might be from AGI? Do you think we're going to get there on the current trajectory we're on? Do you think we need more breakthroughs? Do you think the current approach will get us there?

Dr. Fei Fei Li (00:24:13):
Yeah, this is a very interesting term, Lenny. I don't know if anyone has ever defined AGI. There are many different definitions, including some kind of superpower for machines all the way to machines can become economically viable agent in the society. In other words, making salaries to live. Is that the definition of AGI? As a scientist, I take science very seriously and I enter the field because I was inspired by this audacious question of, can machines think and do things in the way that humans can do? For me, that's always the north star of AI. And from that point of view, I don't know what's the difference between AI and AGI.

(00:25:10):
I think we've done very well in achieving parts of the goal, including conversational AI, but I don't think we have completely conquered all the goals of AI. And I think our founding fathers, Alan Turing, I wonder if Alan Turing is around today and you ask him to contrast AI versus AGI, he might just shrugged and said, "Well, I asked the same question back in 1940s," so I don't want to get onto a rabbit hole of defining AI versus AGI. I feel AGI is more a marketing term than a scientific term as a scientist than technologist. AI is my north star, is my field's north star, and I'm happy people call it whatever name they want to call it.

Lenny Rachitsky (00:26:05):
So let me ask you maybe this way, like you described, there's kind of these components that from ImageNet and AlexNet took us to where we're today, GPUs essentially, data, label data, just like the algorithm of the model. There's also just the transformer feels like an important step in that trajectory. Do you feel like those are the same components that'll get us to, I don't know, 10 times smarter model, something that's like life-changing for the entire world? Or do you think we need more breakthroughs? I know we're going to talk about world models, which I think is a component of this, but is there anything else that you think is like, oh, this will plateau, or okay, this will take us just need more data, more compute, more GPUs?

Dr. Fei Fei Li (00:26:44):
Oh no, I definitely think we need more innovations. I think scaling loss of more data, more GPUs, and bigger current model architecture is there's still a lot to be done there, but I absolutely think we need to innovate more. There's not a single deeply scientific discipline in human history that has arrived at a place that says we're done, we're done innovating and AI is one of the, if not the youngest discipline in human civilization in terms of science and technology, we're still scratching the surface. For example, like I said, we're going to segue into world models. Today, you take a model and run it through a video of a couple of office rooms and ask the model to count the number of chairs. And this is something a toddler could do or maybe an elementary school kid could do, and AI could not do that, right?

(00:27:50):
So there's just so much AI today could not do, then let alone thinking about how did someone like Isaac Newton look at the movements of the celestial bodies and derive an equation or a set of equations that governs the movement of all bodies, that level of creativity, extrapolation, abstraction. We have no way of enabling AI to do that today. And then let's look at emotional intelligence. If you look at a student coming to a teacher's office and have a conversation about motivation, passion, what to learn, what's the problem that's really bothering you. That conversation, as powerful as today's conversational bots are, you don't get that level of emotional cognitive intelligence from today's AI. So there's a lot we can do better, and I do not believe we're done innovating.

Lenny Rachitsky (00:29:00):
Demis had this really interesting interview recently from DeepMind slash Google where someone asked him just like, "What do you think, how far are we from AGI? What does it look like going through there?" He had a really interesting way of approaching it is if we were to give the most cutting-edge model all the information until the end of the 20th century, see if it could come up with all the breakthroughs Einstein had and so far we're nowhere near that, but they could just-

Dr. Fei Fei Li (00:29:22):
No, we're not. In fact, it's even worse. Let's give AI all the data including modern instruments data of celestial bodies, which Newton did not have, and give it to that and just ask AI to create the 17th century set of equations on the laws of bodily movements. Today's AI cannot do that.

Lenny Rachitsky (00:29:49):
All right. We're ways away is what I'm hearing.

Dr. Fei Fei Li (00:29:50):
Yeah.

Lenny Rachitsky (00:29:51):
Okay, so let's talk about world models. To me, this is just another really amazing example of you being ahead of where people end up. So you were way ahead on, okay, we just need a lot of clean data for AI and neural networks to learn. You've been talking about this idea of world models for a long time. You started a company to build, essentially there's language models. This is a different thing. This is a world model. We'll talk about what that is. And now, as I was preparing for this Elon's talking about world models, Jensen's talking about world models, I know Google's working on this stuff. You've been at this for a long time and you actually just launched something that's going, we're going to talk about right before this podcast airs. Talk about what is a world model? Why is it so important?

Dr. Fei Fei Li (00:30:33):
I'm very excited to see that more and more people are talking about world models like Elon, like Jensen. I have been thinking about really how to push AI forward all my life and the large language models that came out of the research world and then OpenAI and all this, for the past few years, were extremely inspiring even for a researcher like me. I remembered when GPT2 came out, and that was in, I think, late 2020. I was co-director, I still am, but I was at that time full-time co-director of Stanford's Human-Centered AI institute, and I remember it was... The public was not aware of the power of the large language model yet, but as researchers, we were seeing it, we're seeing the future, and I had pretty long conversations with my natural language processing colleagues like Percy Liang and Chris Manning. We were talking about how critical this technology is going to be and the Stanford AI Institute, Human-Centered AI Institute, HAI, was the first one to establish a full research center foundation model.

(00:31:59):
We were, Percy Liang, and many researchers led the first academic paper foundation model. So it was just very inspiring for me. Of course, I come from the world of visual intelligence and I was just thinking there's so much we can push forward beyond language because humans, humans use our sense of spatial intelligence, a world understanding to do so many things and they are beyond language. Think about a very chaotic first responder scene, whether it's fire or some traffic accident or some natural disaster. And if you immerse yourself in those scene and think about how people organize themselves to rescue people, to stop further disasters, to put down fires, a lot of that is movements is spontaneous understanding of objects, worlds, human situational awareness. Language is part of that, but a lot of those situations, language cannot get you to put down the fire.

(00:33:21):
So that is, what is that? I was thinking a lot. And in the meantime, I was doing a lot of robotics research and it dawned on me that the linchpin of connecting the additional intelligence, in addition to language embodied AI, which are robotics, connecting visual intelligence, is the sense of spatial intelligence about understanding the world. And that's when I think it was 2024, I gave a TED talk about spatial intelligence at world models. And I start formulating this idea back in 2022 based on my robotics and computer vision research. And then one thing that was really clear to me is that I really want to work with the brightest technologists and move as fast as possible to bring this technology to life. And that's when we founded this company called World Labs. And you can see the word world is in the title of our company because we believe so much in world modeling and spatial intelligence.

Lenny Rachitsky (00:34:41):
People are so used to just chatbots and that's a large language model. A simple way to understand a world model is you basically describe a scene and it generates an infinitely explorable world. We'll link to the thing you launched, which we'll talk about, but just is that a simple way to understand it?

Dr. Fei Fei Li (00:34:56):
That's part of it, Lenny. I think a simple way to understand a world model is that this model can allow anyone to create any worlds in their mind's eye by prompting whether it's an image or a sentence. And also be able to interact in this world whether you are browsing and walking or picking objects up or changing things as well as to reason within this world, for example, if the person consuming, if the agent consuming this output of the world model is a robot, it should be able to plan its path and help to tidy the kitchen, for example. So world model is a foundation that you can use to reason, to interact, and to create worlds.

Lenny Rachitsky (00:36:00):
Great. Yeah. So robots feels like that's potentially the next big focus for AI researchers and just the impact on the world. And what you're saying here is this is a key missing piece of making robots actually work in the real world, understanding how the world works.

Dr. Fei Fei Li (00:36:17):
Yeah. Well, first of all, I do think there's more than robots. That's exciting. But I agree with everything you just said. I think world modeling and spatial intelligence is a key missing piece of embodied AI. I also think let's not underestimate that humans are embodied agents and humans can be augmented by AI's intelligence. Just like today, humans are language animals, but we're very much augmented by AI helping us to do language tasks including software engineering. I think that we shouldn't underestimate or maybe we tend not to talk about how humans, as an embodied agents, can actually benefit so much from world models and spatial intelligence models as well as robots can.

Lenny Rachitsky (00:37:15):
So the big unlocks here, robots, which a huge deal if this works out, imagine each of us has robots doing a bunch of stuff for us, they help us with disasters, things like that. Games obviously is a really cool example, just like infinitely playable games that you just invent out of your head. And then creativity feels like just like being fun, having fun, being creative, thinking of magic, wild new worlds, and environments.

Dr. Fei Fei Li (00:37:39):
And also design, humans design from machines to buildings to homes and also scientific discovery. There is so much. I like to use the example of the discovery of the structure of DNA. If you look at one of the most important piece in DNA's discovery history is the x-ray diffraction photo that was captured by Rosalind Franklin, and it was a flat 2D photo of a structure that it looks like a cross with diffractions. You can google those photos. But with that 2D flat photo, the humans, especially two important humans, James Watson and Francis Crick, in addition to their other information, was able to reason in 3D space and deduce a highly three-dimensional double helix structure of the DNA. And that structure cannot possibly be 2D. You cannot think in 2D and deduce that structure. You have to think in 3D spatial, use the human spatial intelligence. So I think even in scientific discovery, spatial intelligence or AI-assisted spatial intelligence is critical.

Lenny Rachitsky (00:39:08):
This is such an example of, I think it was Chris Dixon that had this line that the next big thing is going to start off feeling like a toy. When ChatGPT just came out, I remember Sam Altman just tweeted it as like, "Here's a cool thing we're playing with, check it out." Now, it's the fastest growing product to all of history, changed the world. And it's oftentimes the things that just look like, okay, this is cool, that it's a fun to play with that end up changing the world most.

(00:39:33):
This episode is brought to you by Sinch, the customer communications cloud. Here's the thing about digital customer communications. Whether you're sending marketing campaigns, verification codes, or account alerts, you need them to reach users reliably. That's where Sinch comes in. Over 150,000 businesses, including 8 of the top 10 largest tech companies globally use Sinch's API to build messaging, email, and calling into their products. And there's something big happening in messaging that product teams need to know about, Rich Communication Services or RCS. Think of RCS as SMS2.0. Instead of getting texts from a random number, your users will see your verified company name and logo without needing to download anything new.

(00:40:16):
It's a more secure and branded experience. Plus you get features like interactive carousels and suggested replies. And here's why this matters, US carriers are starting to adopt RCS. Sinch is already helping major brands send RCS messages around the world and they're helping Lenny's podcast listeners get registered first before the rush hits the US market. Learn more and get started at sinch.com/lenny. That's S-I-N-C-H.com/lenny.

(00:40:45):
I reached out to Ben Horowitz, who loves what you're doing, a big fan of yours. They're investors I believe in...

Dr. Fei Fei Li (00:40:51):
Yeah, we've known each other for many years, but yes, right now they're investors of World Labs.

Lenny Rachitsky (00:40:57):
Amazing. Okay, so I asked him what I should ask you about and he suggested ask you why is the bitter lesson alone not likely to work for robots? So first of all, just explain what the bitter lesson was in the history of AI and then just why that won't get us to where we want to be with robots.

Dr. Fei Fei Li (00:41:17):
Well, first of all, there are many bitter lessons, but the bitter lessons everybody refers to is a paper written by Richard Sutton who won the Turing Award recently, and he does a lot of reinforcement learning. And Richard has said, if you look at the history, especially the algorithmic development of AI, it turns out simpler model with a ton of data always win at the end of the day instead of the more complex model with less data. I mean, that was actually... This paper came years after ImageNet. That to me was not bitter; it was a sweet lesson. That's why I built ImageNet because I believe that big data plays that role. So why can't bitter lesson work in robotics alone? Well, first of all, I think we need to give credit to where we are today. Robotics is very much in the early days of experimentation.

(00:42:25):
The research is not nearly as mature as say language models. So many people are still experimenting with different algorithms and some of those algorithms are driven by big data. So I do think big data will continue to play a role in robotics, but what is hard for robotics, there are a couple of things. One is that it's harder to get data. It's a lot harder to get data. You can say, well, there's web data. This is where the latest robotics research is using web videos. And I think web videos do play a role. But if you think about what made language model worth a very... As someone who does computer vision and spatial intelligence and robotics, I'm very jealous of my colleagues in language because they had this perfect setup where their training data are in words, eventually tokens, and then they produce a model that outputs words.

(00:43:36):
So you have this perfect alignment between what you hope to get, which we call objective function and what your training data looks like. But robotics is different. Even spatial intelligence is different. You hope to get actions out of robots, but your training data lacks actions in 3D worlds, and that's what robots have to do, right? Actions in 3D worlds. So you have to find different ways to fit a, what do they call, a square in a round hole, that what we have is tons of web videos. So then we have to start talking about adding supplementing data such as teleoperation data or synthetic data so that the robots are trained with this hypothesis of bitter lesson, which is large amount of data. I think there's still hope because even what we are doing in world modeling will really unlock a lot of this information for robots.

(00:44:53):
But I think we have to be careful because we're at the early days of this and bitter lesson is still to be tested because we haven't fully figured out the data for. Another part of the bitter lesson of robotics I think we should be so realistic about is again, compared to language models or even spatial models, robots are physical systems. So robots are closer to self-driving cars than a large language model. And that's very important to recognize. That means that in order for robots to work, we not only need brains, we also need the physical body. We also need application scenarios. If you look at the history of self-driving car, my colleague Sebastian Thrun took Stanford's car to win the first DARPA challenge in 2006 or 2005. It's 20 years since that prototype of a self-driving car being able to drive 130 miles in the Nevada desert to today's Waymo and on the street of San Francisco.

(00:46:17):
And we're not even done yet. There's still a lot. So that's a 20-year journey. And self-driving cars are much simpler robots, they're just metal boxes running on 2D surfaces, and the goal is not to touch anything. Robot is 3D things running in 3D world, and the goal is to touch things. So the journey is going to be, there's many aspects, elements, and of course one could say, well, the self-driving car, early algorithm were pre deep learning era. So deep learning is accelerating the brains. And I think that's true. That's why I'm in robotics, that's why I'm in spatial intelligence and I'm excited by it. But in the meantime, the car industry is very mature and productizing also involves the mature use cases, supply chains, the hardware. So I think it's a very interesting time to work in these problems. But it's true, Ben is right. We might still be subject to a number of bitter lessons.

Lenny Rachitsky (00:47:28):
Doing this work, do you ever just feel awe for the way the brain works and is able to do all of this for us? Just the complexity just to get a machine to just walk around and not hit things and fall, does just give you more respect for what we've already got?

Dr. Fei Fei Li (00:47:44):
Totally. We operate on about 20 watts. That's dimmer than any light bulb in the room I'm in right now. And yet we can do so much. So I think actually the more I work in AI, the more I respect humans.

Lenny Rachitsky (00:48:03):
Let's talk about this product you just launched. It's called Marble, a very cute name. Talk about what this is, why this is important. I've been playing with it, it's incredible. We'll link to it for folks to check it out. What is Marble?

Dr. Fei Fei Li (00:48:14):
Yeah, I'm very excited. So first of all, Marble is one of the first product that World Labs has rolled out. World Labs is a foundation frontier model company. We are founded by four co-founders who have deep technical history. My co-founders, Justin Johnson, Christoph Lassner, and Ben Mildenhall. We all come from the research field of AI, computer graphics, computer vision, and we believe that spatial intelligence and world modeling is as important, if not more, to language models and complementary to language models. So we wanted to seize this opportunity to create deep tech research lab that can connect the dots between frontier models with products. So Marble is an app that's built upon our frontier models. We've spent a year and plus building the world's first generative model that can output genuinely 3D worlds. That's a very, very hard problem.

(00:49:30):
And it was a very hard process and we have a team of incredible, founding team of incredible technologists from incredible teams. And then around just a month or two ago, we saw the first time that we can just prompt with a sentence and the image and multiple images and create worlds that we can just navigate in. If you put it on Google, which we have an option to let you do that, you can even walk around. Even though we've been building this for quite a while, it was still just awe-inspiring and we wanted to get into the hands of people who need it. And then we know that so many creators, designers, people who are thinking about robotic simulation, people who are thinking about different use cases of navigable interactable, immersive worlds game developers will find this useful. So we developed Marble as a first step. It's again, still very early, but it's the world's first model doing this, and it's the world's first product that allows people to just prompt, we call it prompt to worlds.

Lenny Rachitsky (00:51:00):
Well, I've been playing around with it. It is insane. You could just have a little Shire world where you just infinitely walk around middle earth basically, and there's no one there yet, but it's insane. You just go anywhere. There's dystopian world. I'm just looking at all these examples and my favorite part, actually, I don't know if there's a feature or bug, you can see the dots of the world before it actually renders with all the textures. And I just love like, you get a glimpse into what is going on with this model, basically-

Dr. Fei Fei Li (00:51:27):
That is so cool to hear because this is where, as a researcher, I am learning because the dots that lead you into the world was an intentional feature visualization, is not part of the model. The model actually just generates the world. But we were trying to find a way to guide people into the world, and a number of engineers worked on different versions, but we converged on the dot, and so many people, you're not the only one, told us how delightful that experience is, and it was really satisfying for us to hear that this intentional visualization feature that's not just the big hardcore model actually has delighted our users.

Lenny Rachitsky (00:52:19):
Wow. So you add that to make it more, like to have humans understand what's going on-

Dr. Fei Fei Li (00:52:24):
To have fun, yes.

Lenny Rachitsky (00:52:24):
... get more delightful. Wow, that is hilarious. It makes me think about LLMs and the way they, it's not the same thing, but they talk about what they're thinking and what they're doing.

Dr. Fei Fei Li (00:52:32):
Yes, it is. It is.

Lenny Rachitsky (00:52:34):
It also makes me think about just the Matrix. It's exactly the Matrix experience. I don't know if that was your inspiration.

_[165 additional lines trimmed for context budget]_

---

### $46B of hard truths: Why founders fail and why you need to run toward fear | Ben Horowitz (a16z)
**Guest:** Ben Horowitz | **Date:** 2025-09-11 | [YouTube](https://www.youtube.com/watch?v=KPxTekxQjzc)  

# $46B of hard truths: Why founders fail and why you need to run toward fear | Ben Horowitz (a16z)

## Transcript

Ben Horowitz (00:00:00):
The worst thing that you do as a leader is you hesitate on the next decision. The thing that causes you to hesitate is both decisions are horrible. Probably one of my bigger ones on that was we went public with $2 million in trailing 12 months revenue at 18 months old. That's obviously a bad idea. But the truth of it was the alternative was going bankrupt, and that's a worse idea.

Lenny Rachitsky (00:00:23):
It's a very difficult and painful to be a CEO, to be a founder. In spite of that. So many people want to start companies.

Ben Horowitz (00:00:29):
The psychological muscle you have to build to be a great leader is to be able to click in the abyss and go, "Okay, that way's slightly better. We're going to go that way. If everybody agrees with the decision, then you didn't add any value because they would've done that without you." So the only value you ever add is when you make a decision that most people don't like.

Lenny Rachitsky (00:00:47):
You are famous for writing one of the most popular pieces of literature for product managers.

Ben Horowitz (00:00:52):
What I was trying to get out in Good Product Manager, Bad Product Manager, was the job is fundamentally a leadership job. And it's a tricky leadership job because nobody is actually reporting to you.

Lenny Rachitsky (00:01:06):
There's always this kind of sense that the PM is not the mini CEO. How dare you call yourself that? I actually think that's exactly what the PM is.

Ben Horowitz (00:01:12):
It doesn't matter if you write a good, spec or you have a good interview or you do this or do that. What matters is that the product works.

Lenny Rachitsky (00:01:21):
Today my guest is Ben Horowitz. Ben is the Z in A16Z, the world's largest venture capital firm with over 46 billion in committed capital. They're investors in OpenAI, Cursor, Andrel, Databricks, Figma, basically every generational tech company. He's also the author of two New York Times bestselling books, the Hard Thing About Hard Things and What You Do is Who you are.

(00:01:43):
Ben is endlessly fascinating. He started a rap group when he was younger. He started his career as a product manager and wrote the now famous Good Product Manager, Bad Product Manager piece. In our wide-ranging conversation, we cover a ton of ground and Ben shares stories and insights that he's never shared anywhere else. A huge thank you to Shaka Senghor, Ali Goetze, and Adam Newman for suggesting topics for this conversation. If you enjoy this podcast, don't forget to subscribe and follow it in your favorite podcasting app or YouTube. It helps tremendously. And if you become an annual subscriber of my newsletter, you get a year free of 15 incredible products, including a year free of Lovable, Replit, Bolt, n8n, Linear, Superhuman, DScript, WhisperFlow, Gamma, Perplexity, Warp, Granola, Magic Patterns, RateCast, ChatPRD, and Mabin. Check it out at lennysnewsletter.com and click Product Pass. With that, I bring you Ben Horowitz.

(00:02:38):
Today's episode is brought to you by DX, the developer intelligence platform designed by leading researchers. To thrive in the AI era, organizations meet to adapt quickly. But many organization leaders struggle to answer pressing questions like which tools are working? How are they being used? What's actually driving value? DX provides the data and insights leaders need to navigate this shift. With DX. Companies like Dropbox, Booking.com, Adyen, and Intercom get a deep understanding of how AI is providing value to their developers and what impact AI is having on engineering productivity.

(00:03:13):
To learn more, visit DX's website at getdx.com/Lenny. That's getdx.com/Lenny.

(00:03:22):
This episode is brought to you by Basecamp. Basecamp is the famously straightforward project management system from 37 Signals. Most project management systems are either inadequate or frustratingly complex, but Basecamp is refreshingly clear. It's simple to get started, easy to organize, and Basecamp's visual tools help you see exactly what everyone is working on and how all work is progressing. Keep all your files and conversations about projects directly connected to the projects themselves, so that you always know where stuff is and you're not constantly switching contexts.

(00:03:55):
Running a business is hard. Managing your project should be easy. I've been a long-time fan of what 37 Signals has been up to, and I'm really excited to be sharing this with you. Sign up for a free account at basecamp.com/lenny. Get somewhere with Basecamp.

(00:04:13):
Ben, thank you so much for being here and welcome to the podcast.

Ben Horowitz (00:04:17):
All right, thank you Lenny. Excited to be here.

Lenny Rachitsky (00:04:19):
I'm even more excited to have you here.

(00:04:21):
I want to start with a question that a close friend of yours suggested I ask you, Shaka Senghor. So Shaka, he's, we could do an hour just on how interesting this guy and the things he's done.

Ben Horowitz (00:04:32):
Three hours on Joe Rogan that day. He is very-

Lenny Rachitsky (00:04:35):
So we're not going to do that. Just to give a glimpse, he was in prison for 19 years. He was in solitary for seven years. He led a huge prison gang. You wrote about him in your book as a great exemplar of great culture in the prison gang that he ran. So interesting. But something that he learned from you that he told me I need to ask you about is about success and how to be successful and how it's not what people think. And he said that you learned this lesson from a pilot. What is that story? What is that lesson?

Ben Horowitz (00:05:08):
I mean, I would say it's a long life lesson. But the pilot story is I actually, I ask people silly questions sometimes when they meet them. And so I met this gentleman who was a pilot and it was right around the time JFK Jr. crashed his airplane and ultimately died. And I asked him, I was like, "What happened?" Because there's always the story in the press, and I know this from them writing about me or anything, is it's always what's the best narrative not what's true. So you can never actually find out what happened, you just find the best story version of what happened.

(00:05:53):
And the story in the press was all about, "Oh, he wasn't trained on instrumentation was flying at night." And I wanted to know is that right? And the pilot said, "Well," he said, "really, it's like all plane crashes are a series of bad decisions. And none of the decisions by themselves is that bad, but when you add them up, it's bad."

(00:06:14):
So the first decision was he needed to get wherever he was going and that was the priority. And in flying, that can't ever be the priority because there are conditions, there are things that happen. And then the second one was, "Well, his timing of when the sun would go down was wrong." So he thought he'd be flying in sunlight and he wasn't. And then once he got up there, it was when the plane was going down making it go up was a bad decision because he was upside down. And so it was like, I can't remember all the things, but this guy had 17 bad decisions in a row. And the big thing for me that I felt was really true is it's one decision leads to another. And so if you can break psychologically, you can take the sunk cost, then that gets you out of a lot of bad paths. And then a little good decision may be difficult, but you have to believe it's going to lead to the next one. And a lot of success is about that. It's a small thing, a small thing that's hard to do that doesn't seem to have a high impact, but it leads to the next small hard to do thing and then eventually you get an outcome, so that was kind of the concept.

Lenny Rachitsky (00:07:46):
So the lesson there is just success is just a bunch of little things. It's not this, "Cool, I got here in a big thing."

Ben Horowitz (00:07:52):
If somebody were to write a story about me, they would be like, "Then Ben this really smart thing and blah, blah, blah, happy ending." But it really wasn't like that and I don't think it's like that for you or anybody. And I spent a lot of time with Shaka on how, because it's always your own psychology that gets you. And one of the most insightful things he said to me is, because most people who are in solitary for seven years, that's it. You're insane. You're never coming back from that. It's just an impossible thing. But if you study his story, he actually really was massively self-improved coming out of solitary, and he wouldn't recommend that for anybody, just to be clear. It wasn't solitary. But what it was was he changed, in solitary he was able to change a big set of beliefs that he had about himself that got him out of that.

(00:08:50):
And the thing that his conclusion from it, which I thought was really interesting, he's like, "Look, was in prison for 19 years. I was in solitary for seven. I come out, I can't rent an apartment, I can't vote, I can't get a gun, I can't do, no rights. None of that was anything compared to what I did to myself."

(00:09:08):
And I think that's very true for CEOs in general and people in general is all the things that you perceive that are happening to you that are bad, be it the systems against you or somebody undercuts you or racism or sexism or this or that or the other is very small compared to ... It means a lot if you believe it. If you believe what people say about you, if you believe what they did to you, then that destroys you. But if you go, "That's not me," you can overcome almost anything. And he's got a new book out on that anyway that I think is very good because that's the, I'd say more than anything, that's the key to success.

Lenny Rachitsky (00:09:52):
If you look at all the writing you've done, it's essentially about the struggle and pain and suffering of being a CEO, your first [inaudible 00:09:59] to Hard Thing About Hard Things. There's a lot of talk these days about just how important struggle is and how valuable it is to go through struggle. Jensen's big on this. He talks a lot about just you have to go through pain and suffering to be a great leader.

Ben Horowitz (00:10:12):
You don't really have a choice. That's true.

Lenny Rachitsky (00:10:15):
There's something that I saw you share that I love, which is running towards fear versus running away from fear. Something that you tell all your leaders to work on. Easy to hear, hard to do. We don't like doing things that are scary, running towards things that are scary. Why is this so important? Why is this something people need to learn to do?

Ben Horowitz (00:10:33):
Well, so the biggest mistake that you make, the worst thing that you do as a leader, there's things in your control and there's things out of your control and hesitation, that's generally the most destructive. And I go through all the ways that it's destructive, but it's extremely bad. And the thing that causes you to hesitate is both decisions are horrible. It's that business school where you're going through a case study, "And if you had done that, then the company would have gone this way. But if you had done that, it's a great success." That's not actually what happens to you as CEO.

(00:11:16):
What happens to as CEO, it's like, "Okay, if we rearchitect, this product, the architecture is not actually get us to where we need to go. I kind of know that. But if we rearchitect it, we're going to probably miss all the features, miss the quarter, have trouble raising money, shudder, et cetera. So that's really bad. And then not rearchitecting is really bad, and so I'm just going to try to and avoid this subject because I don't even want to deal with either of those." And that's the worst thing, because if action is the better choice and that's good. And then if you don't make an explicit decision, then the whole company's going to get nervous because they know that the architecture is whack and you got to fix it.

(00:12:07):
And probably one of my bigger ones on that was we went public with $2 million in trailing 12 months revenue at 18 months old. That's obviously a bad idea. I mean, there's no question that wasn't a bad idea. But the truth of it was the alternative because of where the private markets were was going bankrupt. And that's a worse idea.

(00:12:34):
And if you look at that time, March of 2001 when we went public, you just look at the number of CEOs that hesitated on that and didn't do it and went bankrupt. It's a lot. And so that getting good at making a decision that everybody's going to go, "Wow, that was insane, Ben." The Wall Street Journal wrote a whole long story about how stupid I was. And then Businessweek wrote a story called the IPO From Hell. That was the name of our idea, the IPO from hell, which was accurate in a sense.

(00:13:13):
So that's really bad, but it wasn't as bad, and this is why it's so scary you make that decision that's going to happen. I knew those stories would get written, there was no question. And yeah, that's the kind of muscle. So if you think about the psychological muscle you have to build to be a great leader is to be able to look in the abyss and go, "Okay, that way slightly better. We're going to go that way. And it's very hard to do." I would say it's a thing people struggle with. And it began something, should I fire the head of sales? So I don't want to have that conversation. And then I'll have to replace them. And then there's going to be a bad PR story. And you can kind of quickly calculate all the bad stuff that's going to happen if you do it. But if you don't do it, that's probably going to be much worse and that's why you have to run towards the pain in darkness.

Lenny Rachitsky (00:14:13):
What is the advice you share with founders? Because as you said, it's very hard to do this, just what helps them actually get better at this? Is it just Ben being by their side telling them this is how it is? Is there anything else you can share?

Ben Horowitz (00:14:23):
No, no. I would say this is one where I can't really coach you to be good at this. I can point it out so that you recognize that you were slow or whatever. But it's kind of like I always liken when I talk to them, it's like football. You can have a really fast great athlete, but if they don't trust their eyes, if they don't run to the ball when they see it, if they think, "Oh, maybe that was a fake," then they're that step slower and then they'll never be as good. And CEOs are like that. If you don't trust what you see and you don't run at it, then you're just not going to be good. And it's hard to get CEOs not to hesitate. But look, the thing that does help is they look at it and I look at it and I confirm, "No, that is as it appears."

(00:15:23):
And sometimes they're afraid of the conversation. So that one I can help with. So A CEO might be afraid, like they want to do something but they don't know how to say it. They don't know how to have the conversation with the employee so I can walk them through that. I had an instance where the CEO said, "Hey, I need your help, Ben. My CTO, he's an asshole." And I was like, "Okay, great." I said, "But you're not going to fire him because I know he's a good CTO, or are you asking me should you fire him?" He said, "No, no, I don't want to fire him." And I was like, "So you're asking me what to do? You don't know how have that conversation with him about being an without him quitting. That's what you're saying?" And he goes, "Yeah, that's the problem."

(00:16:09):
And so I go, "Well, why is he an asshole?" And he says, "Well, he's an asshole because the other day he made a very junior young woman in our finance organization cry." And I was like, "Yeah, I got you." I said, "Look, this is what I would say to him. I'd say I just sit them down and I would say, 'Look, you're a really good director of engineering because you do a great job at managing the team, get the products out, all that. But you're not really a CTO because to be a CTO, you have to be effective with other parts of the organization. You can't just be effective only with engineering. And making somebody cry, she's never going to do anything you want. You lost all effectiveness with all of finance by doing that. And so if you want to get good at that, I'll help you. I'll work with you on it, but if you don't, I'm going to have to hire a CTO at some point because obviously I need that.'" And then he was like, "Oh, okay, I can have that conversation. I can't have the conversation that, 'Hey, you're an asshole,' because I don't want them to quit, but I can have the conversation that's more specific." And a lot of getting people not to hesitate is just getting them over that.

(00:17:20):
And so often, and I would say early in a CEO's career, a lot of it is just not knowing how to have the conversation.

Lenny Rachitsky (00:17:29):
There's also I imagine an element of I just want to be liked. I don't want people to hate me. You have this great line that you want to be liked and respected in the long run, not the short run.

Ben Horowitz (00:17:37):
Yeah, that's tricky. By the way, I have to deal with this in the firm too, and people want to be entrepreneur friendly. I'm like, "No, it's not friendly. Respectful. But you've got to be able to tell them the truth in a way that you probably don't tell most of your friends the truth." Because your friend, look anthropologically, we want people to like us. It's just so they don't throw us to the lion or whatever. That's just kind of a thing. So you say tell people what they want to hear, but in dealing at a company level in a context of you're on the board of somebody's company, you've got to be able to tell them what they don't want to hear. That's the most important thing you're going to say.

(00:18:22):
And yes, they're not going to like it when you say it, there's no question. But over time it could save the company. And all the most important things I've said are things that I've said to CEOs that they did not want to hear. And that's what the leadership is about. If everybody agrees with the decision, then you didn't add any value because they would've done that without you. So the only value you ever add is when you make a decision that most people don't like and that's where leadership comes in because you know that's where it's got to get to. And that's the thing that takes practice.

(00:19:05):
I think when Jensen talks about luck, you've got to get to near death to get yourself to do that. That's true. It's hard to build that if everything's going great. And I would say the CEOs who had an easy run of it for their, let's say, they just launched a product that's an instant hit, it's very hard for them to develop that muscle compared to the ones that built a company like Jensen where he gutted it out for multiple decades before they had big success.

Lenny Rachitsky (00:19:36):
Clearly, it's very difficult and painful to be a CEO, to be a founder. In spite of that so many people want to start companies. So many people dream of having their own company. Who is not right to start a company? What advice do you share with folks that are thinking about starting a company that may not understand just what they're about to get into?

Ben Horowitz (00:19:57):
Yeah, so it's funny. So there's a couple of things. John Reed, who was the CEO of Citigroup when I started as CEO, said to me something I never forget. He said, "Ben, the only reason to start a company is because you have an irrational desire to do so, because it's not worth the money." And I was like, "Wow, he doesn't even quantify how much money and this guy's running Citigroup." So he is a very numbers, banking guy and he didn't quantify it. And I remember when we sold LoudCloud for $1.6 billion, I remember thinking, "Wow, that wasn't worth the money whatsoever."

(00:20:37):
So I think if you're doing it for the money, that's a very bad reason and it will be extremely difficult to get you an outcome. You really have to have an irrational desire to do something larger than yourself to improve the world in some way that somehow that is your purpose. And if you don't feel that, then you'll never get through it. It's just too many bad things happen along the way.

Lenny Rachitsky (00:21:08):
So then how do you think of founders that are looking around for ideas that brainstorm, that look for market opportunities versus come from, "I have this a mission, I've got to do this thing in the world."

Ben Horowitz (00:21:19):
My business partner Mark always talks about that. So if you have a product that forces you to build a company, that is a great case of it, right? Okay, you built something and the world wants it and you need a company to deliver it, you know, already have the right product. And so that's very helpful.

(00:21:37):
I think there are cases of people, I think Hewlett-Packard was built that way, that they're like, "Okay, we've got to build technology," it was that abstract, "We've got built in technology for the world." And then they started with, "Well, what do you need?" They called it the next bench thing. What does the engineer sitting next to me need? The next engineer on the bench? So how they define the first set of products.

(00:22:03):
So it can work the other way, but I think the thing that is in common is it's just a very abstract idea that you have to build something that's going to be important that people are going to like working there, people are going to benefit from the products. You have to have some weird concept other than, "Oh, this is going to be successful and I'm going to make a lot of money." I think way better off taking Zuck's offer at Meta and just doing that. That's a way better deal.

Lenny Rachitsky (00:22:37):
Along these lines, something else Shaka suggested I ask you about, apparently there's a story where the CEO of Databricks asked you for $200,000 in the early days and you said no. And it's not because you didn't want to invest and it was more about helping them think bigger. What happened there?

Ben Horowitz (00:22:55):
So there were six of them, they were six HD student, well, and Jan Stoicka (phonetic) who was their professor. And Jan was this super genius, but when I met with them, they were like, "We need to raise $200,000." And I knew at the time that what they had was this thing called Spark and the competitor was something called Hadoop. And Hadoop had very well-funded companies already running towards it and Spark was open source, so the clock was ticking.

(00:23:39):
And I think they didn't quite know what they had. And then there's also a thing always, although I wouldn't say Jan has this mentality, but professors in general it's a pretty big win if you start a company and you make $50 million. You're a hero on campus. That's a pretty cool thing to have done. And so I'm always a little nervous about a company that comes out of academia thinking too small anyway.

(00:24:09):
And so I said, "Look, I'm not going to write you a check for $200,000. I'll write you a check for $10 million because this company, you need to build a company. You need to really go for it if you're going to do this, otherwise you guys should stay in school." And they were all graduating right then, so that was kind of that. And Ali actually was VP of engineering at the time, and it was a while before I made him CEO and that was very good luck on my part because I had no idea that they had a guide that good inside the company who could become CEO when I invested. That was just, God smiled on me and gave me that one.

Lenny Rachitsky (00:24:54):
So speaking of Ali, I actually asked him what to ask you about, and he immediately shared this story. I don't know if you remember this. In your first one-on-one with him, after you-

Lenny Rachitsky (00:25:00):
I don't know if you remember this. In your first one-on-one with him, after you made him CEO, he was struggling with a bunch of low performers because he was coming in to lead the company and he was trying to turn things around, trying to coach them, trying to level them up. And your advice to him was quote, "You don't make people great. You find people that make you great, that make the company great, that you learned from, not the other way around." And there's something that he called managerial leverage. What is that all about? What's the lesson there?

Ben Horowitz (00:25:27):
Oh yeah, yeah. So understand, he had just become CEO. So I was teaching him, he had been VP of Engineering and CEO is different. And I'll get into why and what I mean by leverage.

(00:25:40):
So I actually wrote a post about this with a little Wang quote where I think the quote was, "The truth is hard to swallow and hard to say too but I graduated from that bullshit, now I hate school." And that was always my feeling about this particular idea was, look, if you're VP of engineering, you can develop people. You can teach them to be better engineers. You can teach them, be better engineering managers. That's very doable.

(00:26:08):
But if you're a CEO, what do you know about being CFO? Like what do you know about being VP, HR? What do you know about any of these jobs except maybe VP of engineering?

(00:26:22):
And so the idea that you're going to take somebody who isn't world-class at marketing and make and you them world class and you don't know anything about marketing, is a dumb idea. It just doesn't work.

(00:26:34):
And then the company can't afford for you to be spending time on that because they need you to make very high quality, fast decisions. They need you to set the direction for the company and they need you to have a world-class team. And so it's a very hard lesson if you've been VP of engineering because if you're a good VP of engineering, you do develop your people.

(00:26:55):
But as a CEO, it's not like you don't do any of it, but it is very, very small compared to it. So I like to make things just very stark. So you get what I'm saying? I don't like to hedge it.

(00:27:08):
And then managerial leverage means it is very simple. It's okay. If I have the ideas about what your department should do next. If I am kind of pushing you to kind of move your organization forward, then that's no leverage. What's leverage is if you're telling me what you should do and how you can push the company forward, that's leverage, then I'm getting more than I'd have if you weren't there otherwise I could just manage a team.

(00:27:39):
And that's the point when you feel like you're not getting leverage. When you got to go say, "Hey, why aren't we doing this? Why aren't we doing that?" That's when you got to make a change. And by the way, he's unbelievable at that, as good as anybody I've seen as a guy who's not callous as a CEO. He really cares about the people who work for him. He really wants him to have great careers and all that, but he does not hesitate. If he's losing leverage, he'll make a move.

Lenny Rachitsky (00:28:06):
Kind of going back to the origin story of A-Sixteen-Z something you guys were really big on was helping founders stay COs become great COs, not replace them with professional COs.

(00:28:16):
I want to flip this question on you. When does it actually make sense to replace CO? When are people not going to make great COs?

Ben Horowitz (00:28:26):
There's a very consistent thing that happens, which is when somebody doesn't make it and it kind of starts with confidence is the way I would put it.

(00:28:35):
So if you invent a product, you kind of recruit a team so forth, all of a sudden you're CEO, but you don't run a big organization, you don't know how to do that. Most founders are like that. And so, if you don't know what you're doing, you're going to make mistakes and they all make a lot of mistakes. And then when you make those mistakes, they're very expensive. They could cost you to do a down round or they could cost you to lose a company or they could cost you a customer or you scrub the product. They're very high impact and not just on you, but everybody who you talked into joining you. And so that kind of motion can really cause you to lose confidence.

(00:29:20):
And then if you lose confidence, what happens is you hesitate on the next decision. And as we talked about, hesitation is very dangerous because one, it locks up the company, but even worse what happens is if you have senior people working for you, they get very nervous and they feel like they need to jump into that void and make the decision for you. And that's when it gets political, very political, because people are vying for power inside your little screwed up company.

(00:29:57):
And so now you've got a political dysfunctional organization and that's generally where, okay, the founder probably can't run this thing anymore. That's how it happens.

(00:30:11):
So most of what we do as a firm is to try to help people with that confidence problem and there's a whole series of ideas that we have around that, but you kind of have to somehow climb the confidence and the competency curve together. It's very hard to do and particularly if you're an engineer and you're used to getting things right or if you've been a straight A student or something like that, it's very disconcerting. Sometimes it's better to have CEOs who are like C minus students.

Lenny Rachitsky (00:30:45):
Why is that?

Ben Horowitz (00:30:45):
Yeah, a little facetious. Well, it's just good to be used to failing. So I think I wrote this, but the median on the CEO kind of test is like 18. It's not like 90. And so you got to be comfortable getting a lot of D minuses because the D minus is fine, as long as you don't get the F, as long as you don't run out of cash, as long as you don't lose all thing. Okay, you got through it, keep going. And match, that's a lot of the thing that we try to do CEOs.

Lenny Rachitsky (00:31:20):
Yeah, it comes back to your core, I don't know, message through your first book is just how much you'll fail and how much you'll struggle and how much paid you'll go through as CO.

Ben Horowitz (00:31:29):
Yeah, yeah, and I mean a lot of why I wrote that book was just to nanalyze it. I think what happens is, particularly when I wrote it, and I think it's come back and been true now, is the way the narrative gets written on all these successes is like, "Ph, they came up with a genius idea and then they built this company and they hired all these smart people and it was all great." But that's not at all how it happens and I've spent enough time with everybody from Mark Zuckerberg to Sam Altman and so forth, that they all go through that same thing that who has your struggling company go through? You screw a lot of things up and they have massive consequences, but you have to maintain your confidence.

Lenny Rachitsky (00:32:19):
Actually, I was at a storytelling event last night and I was chatting with someone that I ran into there and told her I was chatting with you today, and she said how meaningful your first book was to her as a founder. Exactly as you said, normalizing that it's very hard and painful and this is just the way it is.

Ben Horowitz (00:32:36):
And the feeling, look, if you think about organizational design or goals and objectives or OKRs or whatever management technique, you need a basic eighth grade education to do any of that stuff. It is not that complicated. The difficult part is the feeling that you have when you have to do it is very, the hard thing of matter a reorg is you're redistributing power, so you're going to have people really fricking mad at you because somebody's losing power if you do it correctly. And that person may be a really good employee. Dealing with that is the hard thing. Knowing how the organization should work to make communication better, it's not that complex.

Lenny Rachitsky (00:33:18):
Yeah, I think about I was at Airbnb for a long time and just thinking of Brian, who I don't know even know if he had a job before Airbnb now.

Ben Horowitz (00:33:25):
Oh yeah. I spent a lot of time with Brian and after COVID, it all kind of clicked for him and then he did that he and that good talk on founder mode and so forth. But the reason that was so articulate is he had screwed every one of those things up and he hired LT and all this stuff, and these are very senior people and he wanted to defer to them, but you can't defer as the CEO because you know what Airbnb should be doing. He may know what finance should do, but you know what Airbnb should do and this kind of thing. And then it gets really wild when you can't defer decisions as the CEO. You got to understand what people are saying and go, "Now we're going to do this."

Lenny Rachitsky (00:34:13):
And this again comes back to the point of you have to go through the struggle and pain and failure to learn those lessons.

Ben Horowitz (00:34:18):
Yeah, no. They're really hard to learn without doing and often without paying the consequence. Even I, like I make mistakes. I was having conversation with Ali the other day and I was like, he's like, "How's it going Ben?" And I was like, "Well, I'm finally dealing with something that I had put off for a very long time." And he said, "Why'd you put it off?" I said, "Because things were going too good. I didn't have to deal with it. "And he was like, "Yeah." He said, "I know that." I'd say Ali is one of the, if not the kind of best private company CEO out there, and he's making a mistake and I'm making a mistakes. So, it is just tough.

Lenny Rachitsky (00:35:01):
You said that one of maybe the main reason founders fail the CEOs is they lose confidence, and you had some ideas that you guys have to help founders work through that. Are there a couple you can share how you help?

Ben Horowitz (00:35:12):
Yeah. So we do a lot of things on that. So the kind of design of the firm is about confidence. So the first thing is, well, what would give you? Well if you can get stuff done. So what if I could give you a network that's as good as Bob Iger's network, day one, the day you stepped into the job.

(00:35:31):
And so we have 600 people at the firm and why is that? Well, most of them are building that network for you, so you can call any CEO or anybody in Washington or any executive or that kind of thing and get them on the phone and they'll talk to you and you can kind of deal with that thing. And then that just makes you feel like a CEO.

(00:35:55):
And then we have a lot of people in the firm like myself, who you can talk to on a CEO to CEO basis, as opposed to an investor to CEO, and just kind of feel that. Early in the firm days, we used to do this thing. I think I'm going to bring back in some form this thing called the CEO barbecue. And it was like a lot of people have these events where they bring in speakers and this and that and the other. And I always felt like those were one, they were too many days. And then sometimes what they said wasn't really applicable and that kind of thing.

(00:36:32):
So I said, "Why don't we just have a barbecue?" I would barbecue. We get everybody in my backyard because was 500 people at the peak, which is why at the stuff I couldn't cook that much, that kind of stuff. And then we'd have Larry Page and Mark Zuckerberg and Kanye West, and so you're a CEO in there with portfolio. You're like, "Wow, I must be important. I'm here with all these guys" and we're just hanging out having a drink, eating barbecue.

(00:37:04):
And so then when I go back to my company, I feel like I am somebody. And okay, I might not be perfect at all this, but I am really a CEO. I was at the CEO barbecue, for crying out loud and that kind of thing. But the whole idea was always like, "Okay, do you feel like you can do it?" Because that's half the battle?

(00:37:29):
And look, having been in and every CEO has been in a position where they feel like, "Well, maybe I shouldn't be the one running this thing. Maybe it's just too big for me." And that's a bad, you don't want to go there. And because as we've said, founders can get to the next product and that's something that almost no professional CEO is able to do. There've been rare cases, but very rare.

Lenny Rachitsky (00:37:57):
So clearly you've worked with a lot of companies, a lot of founders. Let me kind of zoom out a little bit and ask you this question.

(00:38:03):
What's the most counterintuitive lesson you've learned about building companies that goes against common startup wisdom?

Ben Horowitz (00:38:09):
Well, the common startup wisdom keeps changing. One of the early ones that was wrong, and Brian articulated it, and then now I think a little bit of what people have gone to is also wrong.

(00:38:25):
So the first idea that was wrong was like, okay, build a team of senior executives as soon as you get product market fit as fast as possible and they can scale the thing. And I think that you got to build that team slowly and deliberately, kind of pace to your ability to integrate and then manage them. Because if you bring in a bunch of senior people and you don't know really how they match to your company or how that function works or so forth, then you're going to start deferring. And once you start deferring, it's going to get out of control very fast because they're going to build empires, they're going to get political, they're going to do all that kind of thing.

(00:39:08):
So that was bad advice. You kind of have to do it in a measured way. I think that founder mode, I think a lot of people have taken to never hire anybody with experience. And that's also bad advice in that, look, somebody who knows how to do something can really accelerate your thing. So very early on, one of the founders, great founder Arsalan at Databricks was running sales. And I'm like, "Oli, you're going to have to hire somebody who knows sales because Arsalan's, PhD in computer science. I like that, but that's probably not where you're going to have to start if you're going to catch these guys before they take Spark and use it against you."

(00:39:53):
And I sat down with Arsalan and I explained why. I said, "Look, a lot of what sales, there's a lot of knowledge in how to build a worldwide sales organization; maybe knowledge of customers, territories, territory splitting, rep profiles. There's just a litany of stuff that you really can only learn by doing trial and error and you don't know anything. And so you're phenomenal. Let's get you. And still, he's a very senior executive in the company now, but we need somebody who knows that. And the idea that there are companies that go, " Okay, we're just not going to hire that in founder mode." That's also a mistake. So there's a lot of, it's more subtle than you think, and it's more complex than you think. And so you kind of have to get all the way to the truth. And these little snippets of advice that he sees good, because they watch some fucking podcasts, are all fucking stupid.

(00:40:51):
There's a lot of depth to these things. You have to know the answer to the next question, the next question and the next question, and it does drive me crazy. One of the funnier things that happened along these lines, just to show you how little. You know as an investor, about what it means to be CEO.

(00:41:11):
We were at a board dinner. One of the CEOs says to me, he goes, or one of our CEO says, "Hey Ben, that thing you told me a while ago about don't be CEO at home." He said, "I was doing that and I stopped and it really helped me."

(00:41:28):
And then the other kind of VC said, " Yeah. You got to unplug some time." And I said, I was like, "What the fuck are you talking about? He's CEO. He's not unplugging. He's getting shit all the fucking time. He's got to deal with that. That was not what I meant. I was like, you can't go home and boss your family around. That's what I meant."

(00:41:48):
You hear something from somebody who, but if you haven't done it, you don't even know what that means. And so then you then trying to transfer the advice to the next guy, that's not going to work. So anyway, but he was very innocent. I just don't want to kind of speak bad of him, but that's how it sounds, right? But that's not what it is.

Lenny Rachitsky (00:42:09):
That's an amazing story. So the advice partly here, is just don't believe everything you see on Twitter and little sound bites of advice.

_[381 additional lines trimmed for context budget]_

---

### How to build deeper, more robust relationships | Carole Robin (Stanford professor, “Touchy Feely”)
**Guest:** Carole Robin | **Date:** 2024-04-25 | [YouTube](https://www.youtube.com/watch?v=Cew9-GlC_yk)  

# How to build deeper, more robust relationships | Carole Robin (Stanford professor, “Touchy Feely”)

## Transcript

Lenny Rachitsky (00:00:00):
Many people told you your class at Stanford made them feel like their entire college tuition was worth it.

Carole Robin (00:00:05):
Even more rewarding for me are the, "I'm pretty sure your class just saved my marriage."

Lenny Rachitsky (00:00:11):
I want to talk about how to give feedback well.

Carole Robin (00:00:12):
I feel that you don't care and I feel you're being insensitive are not feelings, and that's where we make our biggest mistakes when it comes to feedback.

Lenny Rachitsky (00:00:19):
How do you avoid people getting defensive?

Carole Robin (00:00:22):
Questions that start with what, when, where, how. Stay away from why.

Lenny Rachitsky (00:00:25):
I think it might be helpful to talk about this concept that you call the three realities.

Carole Robin (00:00:28):
We don't understand that we are only privy to two out of the three, so I know what's going on for me and I know what I did. I have no idea what happened on your end.

Lenny Rachitsky (00:00:37):
That's a really profound point that anger is a secondary emotion. Really what's going on is you're afraid or you're hurt.

Carole Robin (00:00:43):
What a disservice to not help people understand that anger is a distancing emotion and there are other emotions that are connecting.

Lenny Rachitsky (00:00:54):
Today my guest is Carole Robin. For over 20 years, Carole taught the legendary course at Stanford's Graduate School of Business, nicknamed Touchy Feely technically called Interpersonal Dynamics, which helps people learn how to build strong relationships and become much more effective leaders. She then went on to start a non-profit called Leaders in Tech, which brings these same lessons to leaders of high-tech growth companies, and she also wrote an incredibly impactful book called Connect, which distills all the key insights and lessons from her decades running this course. I've had so many friends go through the Stanford course or the Leaders in Tech program, and every single one of them was transformed in terms of how they relate to other people, how they communicate, and how they lead. In my conversation with Carole, we talk about why we're often trapped in mental models that we formed when we were younger and how they now limit us and limit our potential and our ways of seeing world.

(00:01:49):
Why disclosing 15% more than you naturally feel comfortable will make you a more effective leader? Why there are actually three realities around us at all times and how that insight changes the way you relate to people. We also get into how to give feedback to anyone about anything, why vulnerability is so essential to great leadership, how to build exceptional relationships, and why they're so important and so much more. This is a very special and very unique episode and I am so excited to bring it to you. With that, I bring you Carole Robin. After a short word from our sponsors, and if you enjoy this podcast, don't forget to subscribe and follow it in your favorite podcasting app or YouTube. It's the best way to avoid missing future episodes and it helps the podcast tremendously. This episode is brought to you by Eppo. Eppo is a next generation A/B testing and feature management platform built by alums of Airbnb and Snowflake for modern growth teams.

(00:02:43):
Companies like Twitch, Miro, ClickUp and DraftKings rely on Eppo to power their experiments. Experimentation is increasingly essential for driving growth and for understanding the performance of new features. And Eppo helps you increase experimentation velocity while unlocking rigorous deep analysis in a way that no other commercial tool does. When I was at Airbnb, one of the things that I left most was our experimentation platform where I could set up experiments, easily, troubleshoot issues, and analyze performance all on my own. EPO does all that and more with advanced statistical methods that can help you shave weeks off experiment time and accessible UI for diving deeper into performance and out-of-the-box reporting that helps you avoid annoying prolonged analytics cycles. Eppo also makes it easy for you to share experiment insights with your team, sparking new ideas for the A/B testing flywheel. Eppo powers experimentation across every use case, including product growth, machine learning, monetization, and email marketing.

(00:03:40):
Check out Eppo at geteppo.com/lenny and 10X your experiment velocity. That's geteppo.com/lenny. Let me tell you about CommandBar. If you're like me and most users I've built product for you probably find those little in-product pop-ups, really annoying, want to take a tour? Check out this new feature, and these Pop-ups are becoming less and less effective since most users don't read what they say. They just want to close them as soon as possible, but every product builder knows that users need help to learn the ins and outs of your product. We use so many products every day and we can't possibly know the ins and outs of everyone. CommandBar is an AI-powered toolkit for product growth, marketing and customer teams to help users get the most out of your product without annoying them.

(00:04:25):
They use AI to get closer to user intent, so they have search and chat products that let users describe what they're trying to do in their own words and then see personalized results like customer walkthroughs or actions, and they do pop-ups too, but their nudges are based on in-product behaviors like confusion or intent classification, which makes them much less annoying and much more impactful. This works for web apps, mobile apps and websites, and they work with industry-leading companies like Gusto, Freshworks, HashiCorp and LaunchDarkly. Over 15 million end-users have interacted with CommandBar. To try out CommandBar you can sign up at commandbar.com/lenny and you can unlock an extra 1,000 AI responses per month for any plan. That's commandbar.com/lenny. Carole, thank you so much for being here. Welcome to the podcast.

Carole Robin (00:05:19):
Thank you so much for having me, Lenny. I'm delighted to be here.

Lenny Rachitsky (00:05:22):
I've heard from so many people over the years how much you and your course have impacted their life, both friends of mine, and also people when I shared on Twitter that you were coming on the podcast, so many people left comments just like, "Oh my God, that course and Carole's changed my life in so many ways." So I am really excited to have you here. I'm really honored to have you here.

Carole Robin (00:05:23):
Thank you.

Lenny Rachitsky (00:05:43):
I wanted to start with when we were preparing for this podcast, you shared this quote with me. You told me that "I was put on this planet to help people learn that it's possible to learn how to build and develop robust, meaningful relationships." What are robust, meaningful relationships and why are they important to you? Why are they important to people?

Carole Robin (00:06:05):
I think that most people experience a richer, fuller, more meaningful life when they have at least some high quality relationships or maybe put the other way. If you have none, it's unlikely you're going to experience quite as rich and full a life. So I often talk about relationships exist on a continuum. At one end of the continuum is contact and no connection, or we could also say dysfunction. And by the way, contact no connection. Those are thousands of Facebook friends. Those are not relationships and those are not friends in my vocabulary. At the other end of the continuum is what my co-author David Bradford and I came to call exceptional, and exceptional relationships have a particular set of characteristics that I can get into if you want to. But before I even do that, we're not suggesting, and I'm not suggesting that everybody needs to turn every one of their relationships into something exceptional.

(00:07:05):
That would be, first of all, impractical, second of all, unnecessary. But it turns out that the skills you need to move along that continuum actually take you from contact and no connection and dysfunction to at least functional and robust. And then once you've acquired those skills, then you can decide whether or not you want to take a few of those relationships a lot farther, take them all the way to exceptional, but at least you've gained what you need to know in order to get to functional and robust. And I believe if we had a critical mass of human beings on this planet who had those skills and knew how to get to at least robust and functional, we wouldn't just have more functional teams and organizations, we'd have stronger communities, we'd have more functional schools. We might, in my wildest dreams, even have a more functional government. And so that's my life's mission.

Lenny Rachitsky (00:08:03):
Hopefully this episode is going to actually do exactly that, help people build from-

Carole Robin (00:08:07):
From your mouth to God's ears.

Lenny Rachitsky (00:08:10):
To give people a little more motivation to really dig in and pay deep attention to this. What are some of the benefits you've seen from people moving further along the spectrum, building more robust relationships and more exceptional relationships?

Carole Robin (00:08:23):
Obviously, I taught a course for many, many years at Stanford Business School, and it was the Stanford Business School, by the way. It wasn't hidden away somewhere in the psychology department. And that's because the premise of the course is that people do business with people not ideas, not products, not machines, not tactics, strategies, not even money, they do business with people. So you better get the people part right if you really want to succeed. And that interpersonal competence is a determinant of both personal and professional success. So, to your question, I've lost track of how many hundreds of emails and calls and visits I've had from former students who come to tell me I just became a CEO. I'm pretty sure I owe it all to you. I just raised my third round. I'm pretty sure I owe it all to you.

(00:09:20):
I just figured out how my co-founder and I are going to navigate this very difficult situation we're in. Thank you for everything you taught me, and those are exceedingly satisfying. And I'll tell you, even more rewarding for me are the, "I'm pretty sure your class just saved my marriage." I just reconciled my relationship with my brother who I hadn't talked to for two years because he voted for X and I voted for Y. We don't need to get into who X and Y are. And now I get thank you for finally writing a book because my VP of product didn't go to Stanford and doesn't understand what I'm talking about. So at least I bought them a book and now I've got more and more folks sending people to Leaders in Tech, which we can talk about later, which is the nonprofit I started after I left Stanford, so that more and more people can learn this from more than just a book.

Lenny Rachitsky (00:10:19):
Amazing. And I think what I love, you also shared this point that many people told you that that one class at Stanford Business School at GSB was made them feel like their entire college tuition was worth it from that one class, which is so surprising because it's called Touchy Feely, it has nothing to do specifically with business.

Carole Robin (00:10:38):
Except of course it has everything to do with business.

Lenny Rachitsky (00:10:41):
So to follow that thread, can you just talk about what this class is trying to do, what the goal of this class is and what actually goes on in this class? And again, people call this class Touchy Feely. I think it's technically called Interpersonal Dynamics.

Carole Robin (00:10:57):
That's correct.

Lenny Rachitsky (00:10:58):
Okay.

Carole Robin (00:10:58):
That's correct. Yeah.

Lenny Rachitsky (00:11:00):
Yeah. What is this class all about? Help people understand what goes on here.

Carole Robin (00:11:03):
Well, first of all, it's a quarter long class at a really fundamental level we've already really talked about what goes on there. People learn how to be more interpersonally competent or how to connect with other people in more functional ways. And what I mean by connect is learn how I need to show up in order for you to trust me, in order for you to feel closer to me, in order for you to want to spend more time with me in a leadership. By the way, this is part of the leadership curriculum I taught, in a leadership sense that makes you more likely to want to follow me because in the end, that's the question leaders should ask themselves, "Why should somebody follow me?" And you know what? You could study other leaders. Why did somebody follow Steve Jobs? Why did somebody follow Ursula at Xerox? Why did some people follow Sheryl Sandberg or why should somebody follow me? Because I'm not any of those people. So the question I ask my students to sit in and now my Leaders in Tech participants to sit in is why should somebody follow you?

(00:12:18):
Now, there are lots of reasons why people might follow you. You've got a great vision that's very inspiring. You have a product that they want to bring into the world. They think they may make a lot of money if they hitch their wagon to you. Those are all good reasons. But if you want to build a sustainable long-term legacy, then you probably want to think about showing up in a way that other people come to see you as a referent figure, somebody in their life that they say, "When I grow up, I want to be more like that." And that is a form of power. There's a lot of research that supports this, referent power. You're a referent figure, and then people are much likely to be open to your influence, and open to working harder, and open to doing some of the things that you believe are going to make you great as a whole organization.

Lenny Rachitsky (00:13:27):
The class is quite unusual in that there's not just a bunch of lectures and talks, there's a lot of experiential pieces where you have to do quite uncomfortable things in order to learn how to do this well. Can you share an example or two of some of the things you put people through, whatever you're able to share?

Carole Robin (00:13:45):
Sure. I mean, I think that we don't learn to be... And I don't learn how to connect with you by reading about it in a book, which by the way is why it took us four years to write our book because at the end of every chapter there's a, here are things you can go do with what you just learned in the reading. And likewise in the class, the lectures are scaffolding on which you can hang your experience. But most of the learning happens in these small groups called T groups. The T stands for training, not therapy. And sometimes people think they sound like therapy, and they sometimes even experience them as feeling a little bit like therapy, but that's not what they are. We call them training groups. And what happens in that group is that there's 12 participants and two facilitators, and they are, for example, given a task, I might pair two students up and say, okay, you've got 10 minutes.

(00:14:51):
Allow the other person to get to know you. And that's the only instruction I give. Okay, so now you and I look at each other and we're like, "I don't know what the heck that's supposed to mean." And then maybe I share something about myself or maybe I ask you a question or who knows what I do with that, and then who knows what you do with that? So after 10 minutes, then I stop them and I say, okay, so take a moment and recognize that you just had a bunch of choices that you actually probably never even think about. You had a choice whether you began by sharing something or you waited for the other person to be in. You had a choice whether you began with a question, which was a nice, safe place to be, or whether you began with a disclosure.

(00:15:40):
Then you had choices with regard to how you responded to what your partner did. And the whole course is about having interactions and then having the guidance and the space and the time and the focus to unpack what just happened. So now do you want to have a more conversation with this person or less? Are you intrigued or are you like, "Can I just get paired up with somebody else?" But now we get to talk about it. And then I put them into a second conversation and I say, okay, now having learned that, by the way, one of the ways we build relationships is through disclosure, through allowing ourselves to become more known. So that's a little mini lecture. And then I say, okay, now go back into your pair and see whether or not you want to make some new choices.

(00:16:33):
And then of course, that's all I always say, confidentiality is a very important aspect of all this work. So in the pair conversations, in the group conversations, I call them the Vegas Rule. What happens if Vegas stays in Vegas. And so I don't ask for any specifics, but I'll ask them for, was there a qualitative difference between the first and second conversation? And they inevitably say, "Oh my God. The first conversation is the conversation I have in the bar all the time with somebody." And the second conversation was a little more uncomfortable, but I sure feel a lot more known, and I think I know my partner a little bit more and now they've had a little taste of what it's going to be like.

Lenny Rachitsky (00:17:18):
Amazing example. So what I want to do with the rest of our chat is basically go through many of the lessons and insights and lectures that you give in this course. Obviously, they're not going to be able to practice the way they would practice in a class. But before we get in there, let's actually talk about, so there's this course at Stanford, and you also now have a program called Leaders in Tech where anyone can participate. They don't have to be going to Stanford Business School. Talk about what this is and how people can participate if they want to go deeper on the stuff we're going to talk about.

Carole Robin (00:17:47):
So Leaders in Tech is a nonprofit that two of my co-founders and I started, I guess in January of 2018. We started it with a program called The Fellows Program, which is a 10-month program that starts with a four-day retreat that's like Touchy Feely on steroids because Touchy Feely is a quarter long class, and then it continues on a monthly basis for a day or half a day a month to get the rest of what we might call Carole Robin curriculum because I also taught a course called High Performance Leadership. I also called Taught Leadership Coaching and Mentoring. So there were other things besides Touchy Feely. Then what happened was that our fellows who went through our first couple of cohorts said, because the Fellows Program is open to founders, either current or previous founders or co-founders of a company that has not gone public.

(00:18:51):
And what we're trying to do is we're trying to influence the cultures of the future of Facebook's and Google's of the world, not the current ones. And so that program has a more limited number of people that can apply to it. However, one of the things that happened was we got a lot of people who said, but I'm not a founder and I still want that. And we also had fellows who went through the program that said, what about my people, my chief people officer, my VP of engineering? So then we'd started just a four-day version of the Touchy Feely, which anybody could apply to. I mean, actually not anybody. You do have to be a manager of some kind, and you do have to be in tech for now.

(00:19:37):
So that's where people get the real on the ground experience. They all get a copy of the book, and of course, if you don't want to go through the program or it's not the right time or you want to start somewhere else, you can start with the book. But if you just buy the book and you read it and you put it back on your shelf, you're not going to learn anything. Don't waste your money. If you're going to buy the book, buy at least one other copy and give it to somebody with whom you actually want to develop a stronger relationship and read it together and do the activities at the end of every book, and then you start to get a taste of what it's like to go through the course.

Lenny Rachitsky (00:20:18):
So just to close the loop there, how do people learn more and apply and join this program?

Carole Robin (00:20:21):
Www.leadersintech.org.

Lenny Rachitsky (00:20:25):
Awesome. And around the time this episode comes out, there's a deadline roughly around that time?

Carole Robin (00:20:31):
There is. Around when this episode comes out. So we do these four-day retreats all year, so there's no deadline to apply for that, but if you're interested in the ten-month program that starts with the four-day retreat and then has all that additional stuff, then deadline for that is May one. Actually, it might even how many days? It might be April. I don't remember how many days there are in April, but it's either the last day of April or May one.

Lenny Rachitsky (00:21:00):
April 30th.

Carole Robin (00:21:01):
Yeah, there you go. So it's probably April 30th. So check it out. You have to be nominated in order to apply. Don't let that stop you. If you look at it all and you decide you want to apply, just apply and say, Carole, I listened to Carole Robin on this podcast, you told me to apply.

Lenny Rachitsky (00:21:23):
Okay, great.

Carole Robin (00:21:25):
Don't waste time trying to find somebody to nominate you.

Lenny Rachitsky (00:21:27):
How amazing. Okay. Or you're going to be flooded with applications.

Carole Robin (00:21:31):
Again, from your mouth to God's ears.

Lenny Rachitsky (00:21:35):
Okay, so let's get into a lot of the stuff that you teach. So you mentioned progressive disclosure, so that might be a good place to start. What's the lesson there? What is it that people get wrong? Why is that important?

Carole Robin (00:21:46):
So first of all, when we disclose, we make ourselves more vulnerable, and vulnerability and disclosure tend to be reciprocal. If I hold my cards really close, you're going to hold your cards even closer. So one of the things to learn to do is to experiment. And what works with one person isn't going to work without somebody else necessarily because every relationship is its own fabulously interesting and always unfolding dynamic is to experiment with allowing myself to be a little bit more known, and then seeing what happens and whether or not you reciprocate. Now, a really important concept we teach that you and I have talked about before is called the 15% rule. And what that is that we all have a comfort zone. Imagine a circle in the middle called the comfort zone, this picture's in the book, and that we don't think twice about what we say, and then there is a danger zone, which is a circle way on the outside.

(00:22:57):
So these are concentric circles if you're not watching the video, and I never in a million years say that or tell you that, but there's this really important circle in the middle, which is called the learning zone. In academia, they have to have fancy words for very easy concepts, it's called the zone of proximal development. But basically it means that's where you learn. And you have to step outside your comfort zone in order to learn anything, and especially in order to create a deeper connection with somebody. However, my students used to say, "But Carole, the minute I step outside my comfort zone, how do I know I'm not in my danger zone?" I hear this learning zone, but how do I know I didn't go too far? So we came up with the 15% rule. So step a little bit outside your comfort zone. If you step a little bit outside your comfort zone, you're very unlikely to freak yourself or the other person out.

(00:23:53):
But you'll know, you'll feel it a little bit, you'll be like, okay, I feel just a little uncomfortable saying this, but I think I'm going to try. And then depending on how you respond, then we settle into a new comfort zone, a slightly larger circle, which is our comfort zone with each other. Then we can go 15% beyond that, and that's how we learn and grow and deepen our relationship. The same thing by the way applies to feedback when we get into that later. So we have to step outside our comfort zone in order to deepen and strengthen relationships.

Lenny Rachitsky (00:24:27):
What are some examples of stepping outside your comfort zone, disclosing what is it that you find people maybe aren't disclosing enough of or areas they should disclose? Is it challenges they're having in their life?

Carole Robin (00:24:38):
Well, of course, context matters. It depends on who I'm talking to. And by the way, disclosure, I want to underscore a concept that we also very much teach, which is appropriate disclosure. If I'm the VP of marketing and I get up in front of the troops and I say, "Well, third month in a row we've lost share and I have no idea what's happening or why or what to do about it, and I'm feeling pretty crappy about myself, I'm not even sure I should be your VP of marketing." That might be vulnerable and disclosure, but it is not appropriate Vulnerability and that it's not what we're talking about. The flip side of that is that I get up in front of the troops and I pretend nothing is happening. That doesn't build my credibility either. So I can get up and say, okay, probably no secret to most of you, that's the third month in a row, we've lost share.

(00:25:39):
And man, I wish I could stand up here and tell you I know exactly what's happening and I know exactly what we should do about it, but I don't, and I have never needed you all more. Now, who would you rather follow? So I think in business, and for a very long time, leaders were socialized to, first of all, leave all feelings in the parking lot. I've got an anecdote I often tell about my very first job, which I'm happy to tell you if you want. And there's no place for vulnerability or for sharing feelings. Are you kidding, feelings in the workplace? Now, I ask you, how do you inspire anybody with no feelings? How do you motivate anybody with no feelings? How do you become seen as a real person with no feelings? Why should somebody who is a robot who is robotic follow you? And the answer to that sometimes in the valley especially is because they're going to follow you for a while because you've got a really great idea. And the minute they've got another choice, man, they are out of there.

Lenny Rachitsky (00:26:50):
And so a lot of this connects to this broad piece of advice you always give people is just and focus on vulnerability. You spend a lot of time teaching people just the power vulnerability, which is not intuitive. A lot of people try to move away from showing vulnerability. There's this quote I saw somewhere that "A willingness to be vulnerable makes you not less influential as a leader." Can you just talk about why that is?

Carole Robin (00:27:15):
Yeah, you asked me whether or not I held any contrarian views and I said, yeah, that's one of them. I actually think that a leader who is willing to be appropriately vulnerable is a stronger leader. And so I'll give you this short example of what happened to me because it encompasses a lot of what we've been talking about. So in 1975, I went to work for the largest industrial automation company in the world as the first woman in a non-clerical job. I was a sales engineer, and yes, I am old, but the dinosaurs were not roaming the earth. And the first thing I learned was you leave feelings in the parking lot, whatever you do, you never talk about your feelings or express feelings in the workplace. It's unprofessional. I was like, okay. I got pretty good at it. In fact, I got very good at it.

(00:28:13):
Ironic, given that I eventually became known as the queen of putty-feely at Stanford. But at the time, I got very good at it. I'm not a career academic. I've had six different careers. And 10 years later I'm at an off-site and I've been promoted many times, I'm now running a $50 million region. I've got a half a dozen guys that work for me. And yes, ladies, if you're listening, I did finally fix that, but at that point, I still hadn't quite fixed it. And we're sitting there and I had an idea, it doesn't matter what it was, but I got a little excited about it and I got crickets and I got a little more excited and I got crickets. I was like, "Come on you guys, this could be really cool. Why can't you see how cool this could be?" And one of my guys leans in, looks at me and says, "Carole, is that like water in the corner of your eye? Oh my God, are you going to cry?" And then he says, "Are you human after all?"

(00:29:09):
Are you human after all? And then I burst out crying and I tore up our agenda and I said, "You don't think I'm human?" I don't think there is anything more important for us to spend our off-site talking about than that. And we spent the next two days talking about who we were, why we were there, what we wanted, what was important to us, how we could help each other. To this day, I believe that was the day I became a leader. To this day I know for a fact any of them would follow me anywhere.

Lenny Rachitsky (00:29:40):
To help people build this muscle and start to practice this to try 15% disclosure, try to be a little more vulnerable. Is there any other examples or just tidbits you can share of like, here's something you should start doing more and more?

Carole Robin (00:29:54):
Let's start with you can start admitting mistakes, especially when everybody knows you made one, you actually lose a lot more credibility by ignoring it. And you can start again 15% by experimenting with sharing what's going on for you, particularly with regard to feelings a little bit more often. So there's a recent course is called Touchy-Feely emphasis on the feely and not the touchy. And that's because so much of our ability to develop this competence comes down to the appropriate use of feelings. That's why a vocabulary of feelings, how sad is it? We had to develop a vocabulary feelings because that's how hard it is for people to even access what they're feeling. So there's a vocabulary feelings in the syllabus, in the course, in the appendix of the book. Every member, every person who ever goes through a leaders in tech program gets it. And it starts with allowing yourself to be known, not just in terms of facts and... Feelings give meaning to facts.

(00:31:12):
Let me give you another example. If I tell you I went ziplining, well, that's interesting. Maybe you learned something about me. Maybe you start to make up all sorts of interesting stories about me. But if I tell you I went ziplining and I was terrified, but I went because I felt coerced by my family and I didn't want to be left alone back and then miss out. Well, you learned a lot more about me, didn't you? One of my most satisfying moments at the very first Leaders in Tech retreat we ever did was of a former student of mine who had taken Touchy Feely 15 years before, and who said, based on everything I learned 15 years ago from Carole, I couldn't imagine what I would learn if I came back. So I'm back. And he said, but Carole or no Carole, I will not sit around for four days talking about how we're all crushing it. I will leave.

(00:32:15):
I was like, "Oh," I was so proud. And now there are times when a leader does have to stand up and say, yeah, we're crushing it. So another really, really important thing that people don't understand is that all of this is very nuanced and very context dependent, and most people unanswered Tell me what to do when X happens. Well, did X happen with this person or this person? What relationship do you have with them in the first place? Are there 20 people in the room of 250? Is this being recorded? There's just so many different things. Who's going to have access to it? There's so many things that you have to consider, and especially today, I know I'm old and this will sound predictable, but I am not a social media fan. I think it has done more to destroy strong relationships and to destroy people's ability to even learn or think about what it takes to have a great relationship.

(00:33:26):
Anyway, we could do a whole podcast on that. I have a former Leaders in Tech fellow who sent me this fantastic, here's another great example, sent me an email and maybe he called me, I don't remember. It doesn't matter. He said, "So I had my all-hands meetings are every Monday morning, on Friday, I found out we had missed a major deadline on a product release, and I spent the entire weekend just furious, pissed off, wanting to fire a lot of them," and he said, "And then on Sunday afternoon, I remembered part of what you taught us was that anger is often a secondary emotion and often under anger is either fear or hurt. And then I realized, oh yeah, I'm actually feeling pretty scared here, that nobody is as worried about this as I am."

(00:34:24):
And so he said, "So on Monday morning, instead of getting up and blasting them all as I was prepared to do, I got up and I said, so, gang, I am deeply worried and afraid that I'm the only person here who is as concerned about this missed deadline as I am and what it's going to mean to our customers." And he said, "I have never had my troops rally to fix something faster." So appropriate use of feelings is something most people don't know how to do. They don't even know how to access the feeling. I told this particular anecdote about anger being a secondary emotion at a very big workshop a number of months ago, and a woman walked up to me and said, "Wow, thank you so much. I've never understood that my husband Carries so much fear and so much hurt because he only ever leads with anger. It never even occurred to me something else might be going on." And anger is a distancing emotion, whereas hurt, fear, sadness, loneliness, happiness, joy are all connecting emotions. So those are kinds of things people learn when they come through our programs.

Lenny Rachitsky (00:35:47):
Oh, man, you're blowing my mind already. I can see why marriages are saved by a lot of these things you teach. That's a really profound point you're just making there that anger is a secondary emotion. Really what's going on is you're afraid or you're hurt. Is there anything more you can add there because this feels very important?

Carole Robin (00:36:06):
That is normally what's going on, except we've all been socialized not to be vulnerable, especially in business and naming any of those other things makes us feel vulnerable. So somehow being angry doesn't make us feel vulnerable. That's the okay emotion, as long as you express it in an appropriate way, but it's a distancing emotion. What a disservice to everybody in business. What a disservice to professional learning, to not help people understand that anger is a distancing emotion and that there are other emotions that are appropriate and that are connecting.

Lenny Rachitsky (00:36:46):
This connects so beautifully to your first point we talked about of being vulnerable and disclosing more and how I completely see how if you were just to share, I'm afraid of this, how that brings people closer to you and feels like they will trust you more versus you not sharing that.

Carole Robin (00:37:03):
Right. It connects to something else. You and I talked about one of the biggest gifts I think people get out of taking Touchy Feely or going through the Leaders in Tech program or even reading the book is that they learn that they hold some mental models, some beliefs and assumptions. If I do this, that will happen, or if I don't do this, this will happen. And those are beliefs and assumptions that we develop very early in our careers like I did. Whatever you do, you leave your feelings in the parking lot. And it served me really well initially. If I'd burst out crying two months into the job, I'd have never ended up running a $50 million region and then it over served me, and then it cost me because I never had a reason to update it.

(00:37:56):
Because I never realized I was paying a cost for continuing to hold that belief that drove my behaviors. And mental models, then we developed them very early and they're grooved and we need new experiences in order to even believe that they're maybe subject to testing. Gee, I will forever be grateful to this fellow who said, "Oh my God, are you human after all?" I was like, how did this ever happen that I became seen as not human? Again, we go back to some of the stuff we talked about earlier, which is that leaders, if a leader doesn't show up with a willingness to update their mental models and their beliefs, they're certainly not going to inspire anybody else to do that.

Lenny Rachitsky (00:38:59):
I'm glad you got here because this is exactly where I was going to go next, is this mental model challenge we run into where we develop these mental models early on and then they end up hurting us later in life. Are there common mental models people have that hurt them as they grow? Or is it very particular independent on people's experience?

Carole Robin (00:39:17):
I mean, there are some that are pretty tried and true. I mean, the first one is, if I tell you more about me, you'll take advantage of me. Or if I am vulnerable with you or disclosing, you'll think I'm weak. And inevitably, somebody has had a time in their life where that has been true, and maybe it was true a lot, but then they decide that's the only outcome that's ever possible as opposed to part of growing up and becoming more mature is differentiating and being more discretionary in who we open up to and how we open up to them. It's like I have a colleague who often says, we have to think about these things as dials, not switches. It's not an all or I don't tell you everything or nothing. I don't share every single feeling I've got or none. It's a dial and you move it at 15% rate.

(00:40:22):
Another mental model people hold, and this becomes a huge learning for people who go through our programs, is people think if I give you feedback, it's going to ruin the relationship. It's going to weaken the relationship. Whoa, that's really common. Even though everybody's always wanting, I want more feedback, I want to know how it can be better, but everybody believes that giving feedback is going to create a problem. And that's because most people have in fact been on the receiving end of feedback poorly given or they've given feedback in a not very good way. They've stepped in piles of doodoo, yes. And it does not mean feedback ruins relationships. It means feedback the way you've always seen it done or done. It ruins relationships pretty important. And then one of the things that we arm people with, I think one of the most powerful pieces of learning that people get is learning how to give feedback in a way that is going to build relationships as opposed to, and it's going to build a relationship.

(00:41:29):
If you see that my reason for wanting to give it to you is that I'm invested in you and in us it's similarly, we hold mental models about expressing what we call pinches, which are just those little things that people do. Then we're just like, eh, I'm not going to make a big deal out of it. I'm not going to say anything, but mental model is, eh, it's a small thing. The problem is, if I'm doing something that's mildly irritating and you don't tell me, then what am I going to do?

Lenny Rachitsky (00:42:01):
You're doing it.

Carole Robin (00:42:01):
And then are you going to get less irritated or more irritated?

Lenny Rachitsky (00:42:02):
More irritated.

Carole Robin (00:42:04):
Yeah. Now, if I get less irritated or it doesn't change, then you're right. I shouldn't say anything. But if I have the wherewithal to notice, this is why we talk about two antenna, which I'll come back to notice that I'm getting more and more activated, more and more irritated, then it's really important for me to say something. And by the way, address it while it's still small and then it won't get big. That's why we call it talk about a pinch before it becomes a crunch, and then it becomes a much bigger deal. But most of the time we say it's not worth it. So I always tell students, okay, substitute the pronoun, substitute the word it for I, you, we. I'm not worth it, you're not worth it. We're not worth it. And then ask yourself again whether it's worth raising.

Lenny Rachitsky (00:42:56):
This episode is brought to you by the a16z Podcast. Every week on this podcast, you get to hear from product leaders and growth experts from some of the world's most impactful companies, whether it's Airbnb, Slack, Figma, or Stripe. But what will the next wave of companies look like? One firm might have a clue, and recent Horowitz invested in all four of the companies I just mentioned and their flagship podcast, the a16z Podcast features conversations with the very founders and technologists shaping our future. Recent episodes feature folks like Marc Andreessen, longtime builders like Adam D'Angelo from Quora and Mark Pincus from Zynga, even some voices from the government like the CIA's first-ever chief technology officer, Nand Mulchandani. From drones to DNA to deep learning, you can eavesdrop on the future with the a16z Podcast. I want to talk about how to give feedback well, but I think it might be helpful to talk about this concept that you call the three realities and the net, because I think that sets up a lot of this.

Carole Robin (00:44:00):
Yeah. And in fact, it is fundamental to giving feedback.

Lenny Rachitsky (00:44:03):
Well, awesome.

Carole Robin (00:44:04):
So they're very related. You were right on the money. And you know what, let me just take a moment and talk. I mentioned the two antennae, and this is in the book, but we're all equipped with two antennae. One is tracking what's going on for me, my internal antenna. The other one is trying to pick up signals on what might be going on for you. And first of all, recognizing those two antennae exist. Second of all, learning how to hone our ability to pick up subtler and subtler signals make us more interpersonally competent. That's also why I'm a big believer in meditation and awareness. So, anyway, if we now fast-forward to your question about how to give feedback well, which has to do with understanding the three realities. It starts with in any exchange between two people, there are three realities. There is my intent, how I see the world, my background, my history, there is what I do or say or don't do, verbal or nonverbal.

(00:45:09):
So my reality is reality number one, my behavior, verbal or nonverbal is reality number two. And whatever happens on your end is reality. Number three, the impact of what I've said or done, how you see things, your background. So there's these three distinct realities. And the trouble we get into when we don't recognize that those three realities exist is we don't understand that we are only privy to two out of the three. So I know what's going on for me, and I know what I did. I have no idea what happened on your end. You know what I did and how it impacted you. So your two are... The only one we share is the one in the middle in common, the behaviors right now, we draw a metaphorical net between reality number one and reality number two to help people understand. And anybody who's ever taken Touchy Feely in no matter which context knows the saying, "Stay on your side of the net."

(00:46:28):
Meaning stick with the two realities you know because we get in trouble the minute we start thinking we know the other person's reality. Right? So I've told this anecdote many times, it might even be in the book, but I come home... I'm sorry, my husband comes home after a very long day in the valley. He was an executive. I've got two little kids, infant and a 2-year-old. I've been waiting for him to come home. I come running into the front room. In those days, by the way, we still had newspapers. He's reading the newspaper and I say, "Oh my God. Oh my God, you're home. I can't wait until I tell you what happened tonight. I can't believe what happened. Why are we living in Palo Alto, Jesus Christ? I don't want to raise kids in Palo Alto. It's a terrible town. I wish that new nursery school, it hasn't even opened. It's already closed. Oh my God."

(00:47:18):
And then he says, "mm-hmm, great. "So then I say, you're not listening. And by the way, people have been taught iMessages, I feel that you're not listening is exactly the same thing, it doesn't have a single feeling word in it. I don't know whether he was listening or not. I'm over the net. I'm in his court unless I'm in his head. I don't know whether he was listening or not. But then he says, yeah, I was listening. You're all worked up. You went to that new nursery school. Actually it's more like this. Yeah, you're all worked up and you went to that new nursery school hasn't even opened. You're all worked up. Now I get a little bit more activated and I say, "How can you not care?" First of all, he didn't say, I don't care, did he? I don't know whether he cares or not. And by the way, "How can you be so insensitive?"

(00:48:13):
And I feel that you don't care and I feel you're being insensitive are not feelings. They're attributions and imputed motives, and that's where we make our biggest mistakes when it comes to feedback. And what that does is it makes the other person defensive. So calling my husband insensitive is the most insensitive thing in the world because he's one of the most sensitive people on the planet. So it wasn't until I learned to stay on my side of the net and say, so when I speak and I'm all worked up about something and the only thing I get back from you are either a grunt or an affectless repetition of what I just said, that's reality number two, anybody watching the video would say, that's what happened, I don't feel heard. He can't say, yeah, you do. And when I don't feel heard, I feel hurt and I feel distanced.

(00:49:11):
And the reason I'm telling you that is because I can't be here for you in the way I want to be when I feel that way. So the formula is when you do insert behavior, I feel pull out the vocabulary of feelings and I'm telling you this because, or I'm hoping the outcome of you knowing this is. And so then what happened is he said, "Well, if you want my undivided attention, then you've got to give me some time to unwind when I get home." What a reasonable request. I said, "Well, how much time do you need?" He said, "I don't know, half an hour." "I was like, half an hour?" I've been counting the minutes. How about five minutes? We settled on 15. And by the way, that is the purpose of feedback. When it's constructive feedback, move into a problem-solving conversation, don't change the other person. Move into behaviors that will work better for both of you.

Lenny Rachitsky (00:50:14):
Amazing. And this structure, so the structure you just shared, and this is similar to nonviolent communication structure?

Carole Robin (00:50:14):
Yes.

Lenny Rachitsky (00:50:21):
Okay, cool.

Carole Robin (00:50:21):
It is.

Lenny Rachitsky (00:50:22):
So there's books people can read on this-

Carole Robin (00:50:23):
Right. Ours came before, but that's okay.

_[261 additional lines trimmed for context budget]_

---

### Taking control of your career | Ethan Evans (Amazon)
**Guest:** Ethan Evans | **Date:** 2024-01-14 | [YouTube](https://www.youtube.com/watch?v=GB0P0_nFPTA)  

# Taking control of your career | Ethan Evans (Amazon)

## Transcript

Ethan Evans (00:00:00):
People think invention takes all this time, but you only need two hours once a month. The thing is, once you have one good idea, it often takes years to express that.

(00:00:09):
So you had the idea to have a newsletter. I know some of the history of your newsletter. You've been working on the expression of that idea for years now. Jeff and Amazon had ideas like, "Let's have Prime shipping." Prime is still getting better and still being worked on. It's a 20 some year old idea. The Kindle, a decades old idea now still getting better. The point here is you don't need very many good ideas to be seen as tremendously inventive.

Lenny (00:00:38):
Today my guest is Ethan Evans. Ethan is a former vice president at Amazon, executive coach, and course creator focused on helping leaders grow into executives. Ethan spent 15 years at Amazon, helped invent and run Prime Video, the Amazon Appstore, Prime Gaming, and Twitch Commerce, which alone is a billion-dollar business for Amazon. He led global teams of over 800, helped draft one of Amazon's 14 core leadership principles, holds over 70 patents, and currently spends his time executive coaching and running courses to help people advance in their career, build leadership skills, and succeed in senior roles.

(00:01:14):
In our conversation, Ethan shares an amazing story of when he failed on an important project for Jeff Bezos and what he learned from that experience. We spent some time on something called The Magic Loop, which is a very simple idea that I guarantee will help you get promoted and advance in your career. We also get into a bunch of other career advice, primarily for senior ICs, any managers. We get into advice for standing out in interviews, plus some of Amazon's most important and impactful leadership principles and much more. I learned a lot from Ethan and I'm excited to bring you this episode. With that, I bring you Ethan Evans after a short word from our sponsors. 

(00:01:50):
Let me tell you about our product called Sidebar. The best way to level up your career is to surround yourself with extraordinary peers. This gives you more than a leg up. It gives you a leap forward. This worked really well for me in my career and this is the Sidebar ethos. When you have a trusted group of peers, you can discuss challenges you're having, get career advice, and just gut check how you're thinking about your work, your career, and your life. This was a big trajectory changer for me, but it's hard to build this trusted group of peers. 

(00:02:20):
Sidebar is a private, highly vetted leadership program, where senior leaders are matched with peer groups to lean on for unbiased opinions, diverse perspectives, and raw feedback. Guided by world-class programming and facilitation, Sidebar enables you to get focused tactical feedback at every step of your career journey. 

(00:02:39):
If you're a listener of this podcast, you're already committed to growth. Sidebar is the missing piece that catalyze your career. 93% of members a sidebar helped them achieve a significant positive change in their career. Why spend a decade finding your people when you can meet them at Sidebar today? Join thousands of top senior leaders who have taken the first step to career growth from companies like Microsoft, Amazon, and Meta, by visiting sidebar.com/lenny. That's sidebar.com/lenny. 

(00:03:12):
Let me tell you about a product called Sprig. Next gen Product teams like Figma and Notion rely on Sprig to build products that people love. Sprig is an AI powered platform that enables you to collect relevant product experience insights from the right users so you can make product decisions quickly and confidently.

(00:03:32):
Here's how it works. It all starts with Sprig's precise targeting, which allows you to trigger in-app studies based on users' characteristics and actions taken in product. Then Sprig's AI is layered on top of all studies to instantly surface your product's biggest learnings. Sprig's surveys enables you to target specific users to get relevant and timely feedback. Sprig replays enables you to capture targeted session clips to see your product experience firsthand. 

(00:03:58):
Sprig's AI is a game changer for product teams. They're the only platform with product level AI, meaning it analyzes data across all of your studies to centralize the most important product opportunities, trends, and correlations in one real-time feed. Visit sprig.com/lenny to learn more and get 10% off. That's sprig.com/lenny. 

(00:04:26):
Ethan, thank you so much for being here and welcome to the podcast.

Ethan Evans (00:04:30):
Lenny, thank you a ton for having me. I'm super excited to talk about some of the things we have teed up today and to help people.

Lenny (00:04:37):
The first thing I thought we could chat about is The Magic Loop. So you wrote this guest post from my newsletter sometime earlier this year. It is, I don't know if you know this, but it's currently the sixth most popular post of all time on my newsletter across 300 plus posts. Did you expect this advice to resonate the way that it did, and why do you think it resonated as much as it did?

Ethan Evans (00:04:59):
So the competitive part of me really wants to analyze spots one to five and figure out, do they have an unfair advantage that they had more time? But I was very hopeful that the advice would resonate that way, because I put a lot of work into simplifying it and making it really easy to understand and follow. So I'm very pleased it has, but I was hopeful it would do so well.

Lenny (00:05:24):
Well, I will say sometimes they keep growing, so this isn't necessarily the terminal point for the post.

Ethan Evans (00:05:28):
The final position. Yeah.

Lenny (00:05:30):
Okay. So for people that haven't read this post, or maybe for folks that have and maybe could use a refresher, let's spend a little time here. Could you just briefly describe this idea of The Magic Loop that you wrote about?

Ethan Evans (00:05:40):
Yeah, absolutely. So The Magic Loop is how to grow your career in almost any circumstance, even with a somewhat difficult manager. It does assume that you're working in some environment, normally as an entrepreneur or with a boss. But the basic idea of The Magic Loop is five steps and they're very easy.

(00:06:01):
The first one is you have to be doing your current job well. It's not possible to really grow your career if you're not considered at least performing at a solid level. Now, it doesn't mean you have to be the star on the team at this point, but what you can't have is your boss wishing that you were different. Like, "Ethan's not very good." So you have to talk to your manager and find out how you're doing and address any problems. So step one is do your job well.

(00:06:31):
Then step two is ask your boss how you can help. Speaking as a manager, and I've talked to hundreds of managers, very few people go and ask their manager, "What can I do to help you? What do you need?" And so just asking sets you apart, and it begins to build a relationship that we're on the same team, that I'm here as a part of your organization to make you successful, not just myself. 

(00:06:53):
Step three is whatever they say, do it. So you dig a big hole. If you say, "What could I do to help you?" And they say, "Well, we really need someone to take out the tray sheets day," and you're like, "Oh, I didn't mean that. I wanted exciting work. I don't want to do sort of this maintenance work or whatever." So do what they ask, help out even if it's not your favorite work. 

(00:07:14):
Once you've done that though, and maybe you do that a couple times, the fourth step is where the magic comes in. You go back to your manager and say, "Hey, I'm really enjoying working with you. I'm wondering is there some way I could help you that would also help me reach my goal?" And whether that goal is to change roles or get a raise or get a promotion, you say, "My goal is I'd really like to learn this new skill. Is there something you need that would also help me learn this new skill?" And the reason this works is managers help those who help them. It's just human nature. We all do that.

(00:07:52):
Generally, they're very open to meeting you halfway and saying, "Sure, I need this. We can rearrange it. We can find a way to meet your goals over time." Now for step four to work, you do have to know what is your goal, so you have to be clear on what it is you want. Well, that part's up to you. 

(00:08:15):
And then step five is the easiest step of all. It's just repeat. So like lather, rinse, repeat with your shampoo. Step five is once you're working with your manager towards your goal and discussing where you're going, and you're helping each other, the magic of the loop is just go around and around.

Lenny (00:08:31):
I was going to ask you, why is it that you call it The Magic Loop? Also, we kind of dived right in, but what is the goal of this? I guess it's pretty clear maybe at this point of this helps you advance in your career, but whatever you want to share along those lines.

Ethan Evans (00:08:43):
Yeah, okay. Very fair. So I called it The Magic Loop because I pioneered it with my audience a few years ago. And it works so well, that people were writing back in and saying, "How do I turn this off? I'm in over my head now. My boss has asked me to do all these cool things, and I feel like I can't catch up, and I've already been promoted once and I need time to digest it." And it just seemed like it worked like magic. It worked in almost every circumstance. 

(00:09:15):
There are of course exceptions where you have very exploitative managers who are like, "Oh, it's great. You're working harder, keep doing that, and they won't do anything for you." But those are rare. And then the purpose, yeah, to help you get satisfaction in your career. A lot of people are unhappy with their jobs. Many people want to move up a level or get paid more. Not everyone. Some people want to change what they're doing, they're bored. This is a path to all of that, because it's forming a partnership with your leadership to say, "Look, I'll help you, but I need you also to help me." And most good managers are very open to that.

Lenny (00:09:52):
When we were working on this, one of the pieces of feedback I had was I feel like I could just tell my manager, "Hey, I want to grow my career. What can we work on to help me get there?" And your feedback was like, most managers are not that good and not that thoughtful about their employee's careers. Can you just talk a little bit about that? People may be hearing this and be like, "Why do I need to do this? This seems like a lot of work."

Ethan Evans (00:10:15):
If you have a great manager, you may not need to do nearly as much formality. They may have given you good feedback, so you don't need to ask for feedback. They may have offered you opportunities to step up, and you've said yes to some and maybe no to others. That's fantastic. I designed The Magic Loop for the people who either don't know what to do or their manager is either not that good or just very busy.

(00:10:37):
Remember, lots of managers have great intentions to help their employees, but they get busy with their own lives, their own work, all the things they're focused on, even also their own career. The manager is often busy thinking about their own needs, and so they mean to get to you next week, and next week drifts on for a year.

Lenny (00:11:00):
What has come up since this has come out that you would want to either add to, or tweak, or help people better understand? I imagine there's some criticism. I imagine there's a lot of, "Yes, yes, yes. This really works."

Ethan Evans (00:11:12):
Two things I'd love to clarify. The first is many people ask me, "Why do I have to do this? Shouldn't my manager notice what I'm doing? Shouldn't my manager help with my career? Shouldn't my manager be planning for me?" And what I say about that is what your manager should do and $4 will get you a cup of coffee at Starbucks. 

(00:11:36):
The point of this loop is it's in your control. It is true that a good manager would do all those things I just mentioned, but not all managers are good and some of them need some help. And the thing I would just say about The Magic Loop is it's in your control.

(00:11:52):
And so you can be upset that your manager isn't perfect, but move on from that and take control of your own situation. That's the first thing I'd say. The other big extension I would make is look, if you are a manager or a leader of any type, you can initiate The Magic Loop from your side, so you can talk to your employees and say, "Hey, what are your career goals? Would you like to form a partnership where you step up to new challenges and I help you get to your goals?"

(00:12:26):
I had a lot of success forming this kind of partnership with my employees, where as they saw growth and success, they really leaned in and like, "This system works. You're actually investing in me now. I'll work extra hard." And I'm like, "Yes, and we can grow your team or grow your opportunity," and it was very win-win.

Lenny (00:12:46):
To give people a little bit of social proof, you mentioned some of the folks you've worked with on this. Can you share some stories, or stats, or anything to help people understand how helpful this ended up being to folks you've worked with?

Ethan Evans (00:12:58):
Yeah, absolutely. I'll tell one story from each end of the spectrum. And what I mean there is entry-level people and then high level executive leaders. I had an entry-level person write me back and say, "Look, when I learned about The Magic Loop, I was at a company and not doing very well. I started applying it. They offered me a $30,000 raise and a bigger job. And I turned it down because I got hired at this other company that was offering me even more, and I went there. And they've promoted me also," and he was one of the people who wrote in and said, his exact words were, "A year ago I was made redundant." So he is in the UK, redundant is their word for laid off. "A year ago I was made redundant. I got this first job and I got an offer for an increased salary, and then I got the second job and I got an increase when I joined that was even bigger." And he was in that situation of, "Mow I need to sort of slow down and digest all of that."

(00:14:05):
On the complete other end, one of my best people I ever worked with joined my team at Amazon as what we would call an SDE II, which in Amazon is a level five employee. He grew with me kind of following this process to a senior engineer. Then he switched to management and ran a small team. Then he became a senior manager and he relocated with my organization. He opened a new office in another city, was eventually promoted to director running his own office of a couple hundred people. And this was over the course of about eight years. He went from a mid-level engineer to an executive with a team of 800 people. Now he was a very hard worker, but over this eight years we just saw all this progress.

(00:14:56):
And then eventually he moved on. He founded his own startup, sold that, and now works as an executive vice president at one of the major online banks. And so his career in some sense has exceeded mine, but during that eight year span, he just grew so much. And this is the process we followed.

Lenny (00:15:19):
Wow, those are excellent examples. What levels does this help you with? At what level is this most useful, and then does it kind of taper out it? I don't know if you get to VP level, do you still try using Magic Loop?

Ethan Evans (00:15:33):
So I think it works anywhere from the start of your career to pretty far into it. I think at my level, I finished my career as a vice president at Amazon. It does peter out in the sense of the active. And what I mean by that is you're still doing the same thing, but you don't have to talk about it. Your managers are expecting you to step up and recognize challenges. They're expecting you to ask for resources when you need them, and you don't sort of have this level of explicit conversation around, what can I help you with? They're expecting you to anticipate what's needed.

(00:16:09):
So in the newsletter we did together, I wrote about how over time, you go from asking your manager, "How can I help?" To suggesting to your manager, "These are some things I see that seem like they need to be done. Would you like me to do them?" To just seeing what needs to be done and sort of keeping your leader in the loop and saying, "Hey, I noticed that we have this problem. I fixed it. I noticed we have this opportunity. I've started program against it." I think at the executive level, it's much more you being proactive and just keeping your leader in the loop.

Lenny (00:16:44):
I think in the post, the way you described this step is this is advanced mode. Don't jump straight to this. Don't just start suggesting things, because you may get it wrong.

Ethan Evans (00:16:53):
Yeah, well, it's all a matter of rapport and trust. A huge part of career success is how much trust you have, mutual respect with your leadership. When they're confident that you're going to make the right decisions, they're confident to let you go. But yeah, when you're brand new or you're new to a manager, if you just jump in, you may either not work on the things they value or even find yourself working across purposes, and that isn't the right place to start.

Lenny (00:17:19):
Awesome. Okay. Just to close out this conversation. You touched on this, but why is it that you think this is so important and effective? Why do you think this works so well? People may not recognize, "I see this is the key to this."

Ethan Evans (00:17:31):
Well, I think it's two things. First, I mentioned how rare it is for managers to be offered help. If you're a manager, you'll recognize this. If not, feel free to talk to any manager, whether your own or somebody else. Ask them how much they worry and how much they feel overwhelmed and wish someone would give them a hand. Management can be a lonely job, because you feel like you're responsible for everything. So having an ally, it's just a huge weight off people's shoulders.

(00:18:01):
And then I think a lot about social engineering. The social engineering's here is just the simple, "You help me, I'll help you." It doesn't have to be exploitative, it's just we help those people who help us, and that's built into human survival. 

(00:18:18):
And I think this loop works so well because it's just leaning a little bit into that behavior. So many relationships with managers are oppositional. You tell me what to do, and I'm kind of like a kid in high school who's trying to figure out how do I skip as many classes as possible and turn in as little homework and still get by with a D? That relationship won't build your career.

(00:18:45):
Some people approach their jobs as my goal is to do the least I can and still collect my paycheck. That's an approach if you're okay with where you are. It's not what I coach though. I assume people want to grow.

Lenny (00:19:02):
Okay, so maybe it's just as a closing question, for people that are listening and want to start putting this into practice slash are stuck in their career and are just like, "Okay, I see. Here's something I can do." Could you just again summarize the loop briefly?

Ethan Evans (00:19:15):
Sure. Step one, make sure you're doing your current job well. The way I explain this is when you go to your manager and ask, "What could I do to help?" You don't want their answer, even if they don't say it quite so bluntly to be, "Do your F-ing job." You need to be doing that already. So be doing a good job.

(00:19:34):
And unfortunately, a good job is in the eyes of your manager in this case. You may think I'm doing great work, but if your manager doesn't, they're the ones you need to build as an ally here. 

(00:19:46):
Once you have that, go ask how you can help, do whatever you're asked, and then go back to your manager and suggest or ask, "I would like to meet this goal. Can I keep helping you? What could I take on that you need that would also help me meet this goal?" And that's where you start to try to bring your two sets of aims together. What do you need done, how can I get to my goal? And let's do those things together.

(00:20:11):
And then you just repeat this loop. You build trust, you build the relationship. And with all good managers, and even a lot of moderate managers, they appreciate the help so much, they really lean into that.

Lenny (00:20:23):
I think there's two really important elements of this that you haven't even mentioned necessarily, that I think are part of the reason this works so well. One is this forces you and your manager to identify the gaps that are keeping you from the next level, which it's often vague, and then you get to a performance review, and then your manager's like, "Ethan, you're still not good on this and this and that," and you're like, "You never told me that that's the things you're looking for for me to get promoted." So I think there's this implicit, here's what you need to work on to get to the next level, which I think is part of step four. 

(00:20:53):
And then you actually did touch on this that it's important to share your goal to your manager. Here's what I want. I want to get promoted. A lot of times they don't know that and you helping them understand, "Here's what I want, help me get there." It goes a long way. So there's a lot-

Ethan Evans (00:21:06):
Managers often fall into the trap. They chose to become managers, so they assume one of two things about you. They either assume that you want to keep doing exactly what you're doing forever, just maybe make a little more money.

(00:21:16):
So you're an artist, you want to keep drawing forever. You're a lawyer, you want to keep writing contracts forever. Or they assume that, "Hey, I became a manager. I'm very proud of my career. That must be what you want."

(00:21:29):
And these assumptions are natural, right? We tend to view by default that our path is great and everyone would want to be us. Now of course, some good managers don't do that. But if you clarify and express your goals, you remove that ambiguity.

Lenny (00:21:45):
I actually had a period in my career where I specifically did not want to get promoted. I was very happy where I was, and I just wanted to keep doing this awesome IC role. Is that something at all you see where people are just like, "I'm good. I don't need to get promoted," and then is this helpful in that in any way or is it not as big a deal?

Ethan Evans (00:22:02):
So first, I reached a point in my career where I was no longer pursuing promotion either, and I wanted to do other things. So I've lived that myself and I've used the same loop, but I used it to go do what I wanted to say, "This is now what I want, and how do we get there? How do we create a role where I'm adding value appropriate to my level, but I'm doing this other work that's fun?" I moved into gaming and I really wanted to do that.

(00:22:25):
Second, I think it is still helpful because there's something you want probably. Maybe you want to work on different kinds of projects or maybe you want to work with a different higher performance team. Or maybe you want to rebalance your life and say, "Hey, I love what I'm doing, but how can I be a star performer for you but within these boundaries?"

(00:22:47):
So if you truly have the perfect job just as it is, you may not need The Magic Loop. But I know so few people if you're like, "Nope, there's absolutely nothing I could improve about my role."

Lenny (00:23:00):
Yeah, I think that your point about your goal doesn't have to be promotion. It could be work on a different part of the org, try something totally... Maybe transition to a new function that could be part of your goal. Awesome. 

(00:23:09):
Okay, so along the same lines of career progression, you work with a lot of senior manager types, kind of the level of L7 and one M2-ish, and you share with me that one of the most frustrating parts of their job in that specific portion of their career is they get stuck at that level and they don't move up, and it becomes really annoying, and they're not sure how to break out of that. What advice you share with folks like that, that may be listening?

Ethan Evans (00:23:36):
Yeah, so it's common to get stuck there, and there are a few reasons for it. First, there are a lot of senior managers. If you think of your average director, they may have six to eight reports. How many more directors are needed? So there's a choke point.

(00:23:52):
Second, that choke point is worse in the current economy, and in the past maybe a lot of companies, Amazon, Google, apple, etc., were growing very rapidly. And so it wasn't just you were waiting for some other director to leave. The teams were getting bigger.

(00:24:07):
I experienced this at Amazon, where over a nine-year period I went from managing six people to 800. And so I went from a senior manager all the way to a vice president, and I described I was, in some sense just riding the elevator. The elevator was going up, and as long as I managed to stay on it, I was going to arrive at vice president.

(00:24:29):
But the other thing that causes people to get stuck is the difference between a senior manager and a director is how you lead and the work you're doing. And you can get as far as senior manager by being really strong in your function and being really good at getting things done. As a director, and as a VP beyond that, it becomes much more about influence, coordination with others, and letting go of being in all the details yourself. And so senior managers really have to change some behavior.

(00:25:03):
I often reference the book by Marshall Goldsmith, What Got You Here Won't Get You There. Not only because it's a great book classically on this problem, but because the title tells the story. All the great traits that got you to this one level won't get you to the next level where you're more expected to be thinking in strategic terms, thinking longer term.

Lenny (00:25:26):
So to someone that may be in that role today and they're not moving up, is there anything they can do? This point about just there's no roles for you, there's only so much you can do there, is the advice just wait until an opportunity arrives? Is it run this Magic Loop until something happens? Is there anything you can do?

Ethan Evans (00:25:42):
I would be honest with people and say some patience is required. At this level, there is some notion of, do we need a director? Do we need a vice president? Do we have a challenge at that level that needs that person? And so promotions at this level, I often teach have two components. The first component is can I eat and do that job? Am I qualified? Do I have the skills? But the second piece is, do we have such a job that needs that?

(00:26:09):
However, there is a lot you can do. A lot is in your control. And what is in your control is to start practicing those next level skills. Start working with your leadership on, where can I take on a strategic project? How can I become more of an inventor? I teach some about how to sort of systematically be inventive. It's not pure magic. Edison said it's 1% inspiration and 99% perspiration. You can learn the 99%, and the 1% isn't as hard then. So you start showing those next level traits. And as I describe it most succinctly, how do you make yourself the person who will be chosen out of the eight?

(00:26:51):
And you can be chosen, there are several ways to move up. Your boss can leave or be let go. They can be promoted to another role. But another way is I coach now, and I have several clients recently. I was just talking to a client yesterday, her two peers were let go. They were all the same level. Her two peers were let go and she was given their teams. And she expressed that her boss had been told, "You have too many senior managers for the size of your organization. We need to do some change in the organization, clean house, and put all your people under the folks who have potential."

(00:27:32):
Well, obviously she must be one of those people, because she still has her job and has more people and more to do. And unfortunately, her peers are shopping for new employment. So be that person, and that's where The Magic Loop comes in. Be that person.

Lenny (00:27:48):
I was just talking actually to a senior PM leader who pointed out that with this kind of lean environment of a lot of flattening of orgs and a lot of layoffs, that this is becoming increasingly hard. Exactly what you're describing. There's just less spots, because companies are running more lean, and so you just kind of have to wait. 

(00:28:06):
I think part of this advice you just shared, which is classic do the job before you have the job makes all the sense in the world. Because once people see that you can do it, obviously they'll feel a lot more comfortable putting you in that position.

Ethan Evans (00:28:18):
And they'll be looking. I always remind people, as a leader, I want the best people under me I can have. It's not that I don't wish to promote you. If you think about my job, this helps people, right? I have selfish motivation to promote you. A lot of people think, "The bosses there holding me down." Well, maybe some bosses are, but why wouldn't I want stronger, more capable direct reports? Why wouldn't I want people under me who can do more of my job? Frankly, that's the only way I can do less of my job.

Lenny (00:28:47):
Plus this pressure you're always getting from your reports. So like, "Hey, I'm ready to get promoted, because this time"... You mentioned this word inventiveness, and I was just listening to Jeff Bezos on Lex Fridman, and I don't know if you heard this, but Jeff Bezos described himself most as an inventor more than anything else that he does. Is that something that you think about? Is that influenced by Jeff Bezos any way, that idea of being an inventor as a leader?

Ethan Evans (00:29:13):
I'll say a couple things about that. First, I know you talked to my old boss, Bill Carr, who wrote Working Backwards. What I don't know is if he shared with you that after he published it, he actually realized there was a better title. He wishes that he had called the book The Invention Machine, because what Jeff was trying to do with Amazon was create the most inventive company, the company that would systematically out-invent others. And so while Working Backwards is a great title, Bill and Jeff think they should have called it the Invention Machine.

(00:29:47):
When I joined Amazon, I did not think of myself as an inventor, but I saw that we had these leadership principles think big and invent and simplify that pushed on that. And I said, "I'm in trouble. I don't know how to do this." And I sat down and thought about that. What am I going to do? It seems like that's required. And I figured out how to become systematically inventive. So I now hold over 70 patents as one benchmark of inventiveness, and they were all created during my 15 years at Amazon. 

(00:30:22):
And the way I did that, inventiveness actually isn't that hard. I teach about this. And to invent systematically, first you do need to be somewhat of an expert in whatever area you want to invent. So Lenny, if you and I say let's get together and we're going to invent cancer drugs, we have the problem that neither of us, as far as I know is a biologist, a doctor. We don't have the right background, we don't know what we're doing. So we would just be fumbling around I guess with a bathtub full of chemicals hoping. It's probably not going to work out that well. So you have to be something of a knowledgeable expert.

(00:30:56):
But then the second thing people don't do is they don't spend dedicated time actually thinking. They feel like, "Invention is just going to come to me." When I want to invent, I get away from all my devices. I go in a room with the problem I have, and I force myself to actually concentrate on what do I know and how can I invent? And the most straightforward way to invent is not to somehow come up with something completely new, but instead to put together two things that exist. 

(00:31:28):
And so my example of this, I have a patent I talk about a lot for a drone delivery for Amazon, but the drone doesn't fly from the warehouse. Instead, a truck with no top drives slowly around the neighborhood, and the drones go back and forth from the truck. As opposed to the driver stopping at every house, you can have four or six drones hitting everything in the neighborhood. 

(00:31:55):
And the way I came up with this idea is one day I was thinking about drones and delivery, but I loved military history. And so I was thinking also about an aircraft carrier and I was thinking, is there a way to have an aircraft carrier for drones? And from that, it was very quick for the light bulb to go on and say, well, what about a truck? 

(00:32:17):
And so I have this patent, and we haven't seen this become reality yet. I'm waiting for my idea to become part of Amazon's drone delivery system, but I think ultimately it will.

Lenny (00:32:32):
That is badass. I'm imagining returns come back to the truck. We're using that rope thing that just captures them with that little hook.

Ethan Evans (00:32:42):
Yeah. Well, there's no reason... Same thing. When you want to return something as opposed to taking it to the UPS Store or whatever, you just put it on your porch, and then on your phone, on your app, maybe you take a picture of it so that the drone can recognize the box or you put it in a designated spot, and you push a button and the drone takes your return away. Yes, there's no reason.

Lenny (00:33:03):
Can't wait for that. And it takes your dog backs in it sometimes, part of it.

Ethan Evans (00:33:09):
My dog's too heavy, thank you.

Lenny (00:33:11):
My dog's not. There's an owl in our backyard that we sometimes worry he is going to come grab our dog on. This idea of invention, this is really interesting. I didn't plan to talk about this, but for someone like say a PM on a team that wants to get better at invention, innovation, big thinking, is there a practice you find helpful here? Is it block off two hours, get a pen and paper, and just think about the specific two adjacent things working together?

Ethan Evans (00:33:34):
So that's part of the process, is put in dedicated time. The interesting thing I would say is you don't need that much time. Two hours is great, but you only need two hours once a month. People think invention takes all this time. The thing is once you have one good idea, it often takes years to express that.

(00:33:52):
So you had the idea to have a newsletter. I know some of the history of your newsletter. You've been working on the expression of that idea for years now. Jeff and Amazon had ideas like, "Let's have Prime shipping." Well, Prime is still getting better and still being worked on. It's a 20 some year old idea. The Kindle a decade's old idea now still getting better. 

(00:34:16):
So the point here is you don't need very many good ideas to be seen as tremendously inventive. Like Elon Musk, Tesla, he can kind of dust off his hands and be like, "I am now an Edison-like inventor." So he keeps doing it, but you don't need that many inventions.

Lenny (00:34:36):
This touches on something else Jeff Bezos shared on the podcast that most of his innovation and work is in the optimizing phase. It's not the here's the idea, it's the making it cheaper, and better, and faster. And that's where most of the good stuff comes from. In this point of Tesla, Elon had this idea, and now the hard work is actually making it scalable and cheap enough for people to use, not just an electric car.

Ethan Evans (00:34:59):
With the idea of Jeff saying that invention is really a lot of the incremental and optimization, I completely agree with that. To invent well, you need a base idea, but then there's so much of the work is making that idea real.

(00:35:15):
And again, Prime is a great example of this. The Amazon Prime program was a great example of, okay, we want fast free shipping. We want this program. That was a one-time idea that they did build, but now Prime has expanded. First it was two-day in the US, then one-day in the US, now it's same day in the US. But also they added Prime Video, Prime Music, Prime Gaming. There's actually something like 25 things you get free with Prime. Most people have no idea, because you get free photo storage and this ongoing list. And all of that is that incremental optimization to make it better, better, better, better. And of course Jeff's goal, which you probably heard him say, was to make Prime a no-brainer, to where you would be irresponsible really not to be a member.

Lenny (00:36:06):
I know you have an awesome Jeff Bezos story that I want to get to, but before we do that, one more question along this line of career advice and progression. So I read somewhere that you've interviewed over 2,500 people over the course of your career. And so kind of going back to the beginning of a career, or at least getting a job, what have you found is most helpful in standing out as a candidate when you're interviewing, and essentially getting hired? What advice do you have for people that may be going through an interview process right now?

Ethan Evans (00:36:33):
There's a lot of evidence that suggests that the number one and two factors in any interview are appearance and enthusiasm. And it doesn't mean you have to be beautiful, but show up somewhere looking like you're interested in the job, not in your pajamas. And most importantly, be enthusiastic. People want to work with people that want to work with them. So if you seem very judgmental of the company and like you have to sell me on it, you're going to turn them off. I look at every interview of whether or not I really want this job, I might've decided I don't want the job. I still want the offer.

_[411 additional lines trimmed for context budget]_

---

### Why AI evals are the hottest new skill for product builders | Hamel Husain & Shreya Shankar
**Guest:** Hamel Husain & Shreya Shankar | **Date:** 2025-09-25 | [YouTube](https://www.youtube.com/watch?v=BsWxPI9UM4c)  

# Why AI evals are the hottest new skill for product builders | Hamel Husain & Shreya Shankar

## Transcript

Lenny Rachitsky (00:00:00):
To build great AI products, you need to be really good at building evals. It's the highest ROI activity you can engage in.

Hamel Husain (00:00:05):
This process is a lot of fun. Everyone that does this immediately gets addicted to it. When you're building an AI application, you just learn a lot.

Lenny Rachitsky (00:00:12):
What's cool about this is you don't need to do this many, many times. For most products, you do this process once and then you build on it.

Shreya Shankar (00:00:18):
The goal is not to do evals perfectly, it's to actionably improve your product.

Lenny Rachitsky (00:00:23):
I did not realize how much controversy and drama there is around evals. There's a lot of people with very strong opinions.

Shreya Shankar (00:00:28):
People have been burned by evals in the past. People have done evals badly, so then they didn't trust it anymore, and then they're like, "Oh, I'm anti evals."

Lenny Rachitsky (00:00:36):
What are a couple of the most common misconceptions people have with evals?

Hamel Husain (00:00:39):
The top one is, "We live in the age of AI. Can't the AI just eval it?" But it doesn't work.

Lenny Rachitsky (00:00:45):
A term that you used in your posts that I love is this idea of a benevolent dictator.

Hamel Husain (00:00:49):
When you're doing this open coding, a lot of teams get bogged down in having a committee do this. For a lot of situations, that's wholly unnecessary. You don't want to make this process so expensive that you can't do it. You can appoint one person whose taste that you trust. It should be the person with domain expertise. Oftentimes, it is the product manager.

Lenny Rachitsky (00:01:09):
Today, my guests are Hamel Husain and Shreya Shankar. One of the most trending topics on this podcast over the past year has been the rise of evals. Both the chief product officers of Anthropic and OpenAI shared that evals are becoming the most important new skill for product builders. And since then, this has been a recurring theme across many of the top AI builders I've had on. Two years ago, I had never heard the term evals. Now it's coming up constantly. When was the last time that a new skill emerged that product builders had to get good at to be successful?

(00:01:41):
Hamel and Shreya have played a major role in shifting evals from being an obscure, mysterious subject to one of the most necessary skills for AI product builders. They teach the definitive online course on evals, which happens to be the number one course on Maven. They've now taught over 2,000 PMs and engineers across 500 companies, including large swaths of the OpenAI and Anthropic teams along with every other major AI lab.

(00:02:07):
In this conversation, we do a lot of show versus tell. We walk through the process of developing an effective eval, explain what the heck evals are and what they look like, address many of the major misconceptions with evals, give you the first few steps you can take to start building evals for your product, and also share just a ton of best practices that Hamel and Shreya have developed over the past few years. This episode is the deepest yet most understandable primer you'll find on the world of evals. And honestly, it got me excited to write evals, even though I have nothing to write evals for. I think you'll feel the same way as you watch this.

(00:02:41):
If this conversation gets you excited, definitely check out Hamel and Shreya's course on Maven. We'll link to it in the show notes. If you use the code LENNYSLIST when you purchase the course, you'll get 35% off the price of the course. With that, I bring you Hamel Husain and Shreya Shankar.

(00:02:58):
This episode is brought to you by Fin, the number one AI agent for customer service. If your customer support tickets are piling up, then you need Fin. Fin is the highest-performing AI agent on the market with a 65% average resolution rate. Fin resolves even the most complex customer queries. No other AI agent performs better. In head-head bake-offs with competitors, Fin wins every time. Yes, switching to a new tool can be scary, but Fin works on any help desk with no migration needed, which means you don't have to overhaul your current system or deal with delays in service for your customers.

(00:03:31):
And Fin is trusted by over 5,000 customer service leaders and top AI companies like Anthropic and Synthesia. And because Fin is powered by the Fin AI engine, which is a continuously improving system that allows you to analyze, train, test, and deploy with ease, Fin can continuously improve your results too. So if you're ready to transform your customer service and scale your support, give Fin a try for only 99 cents per resolution. Plus, Fin comes with a 90-day money-back guarantee. Find out how Fin can work for your team at fin.ai/lenny. That's fin.ai/lenny.

(00:04:05):
This episode is brought to you by Dscout. Design teams today are expected to move fast, but also to get it right. That's where Dscout comes in. Dscout is the all-in-one research platform built for modern product and design teams. Whether you're running usability tests, interviews, surveys, or in-the-wild fieldwork, Dscout makes it easy to connect with real users and get real insights fast. You can even test your Figma prototypes directly inside the platform. No juggling tools, no chasing ghost participants. And with the industry's most trusted panel plus AI-powered analysis, your team gets clarity and confidence to build better without slowing down. So if you're ready to streamline your research, speed up decisions, and design with impact, head to dscout.com to learn more. That's dscout.com. The answers you need to move confidently. Hamel and Shreya, thank you so much for being here, and welcome to the podcast.

Hamel Husain (00:05:04):
Thank you for having us.

Shreya Shankar (00:05:05):
Yeah, super excited.

Lenny Rachitsky (00:05:07):
I'm even more excited. Okay, so a couple years ago, I had never heard the term evals. Now it's one of the most trending topics on my podcast, essentially, that to build great AI products, you need to be really good at building evals. Also, it turns out some of the fastest-growing companies in the world are basically building and selling and creating evals for AI labs. I just had the CEO of Mercor on the podcast. So there's something really big happening here. I want to use this conversation to basically help people understand this space deeply, but let's start with the basics. Just what the heck are evals? For folks that have no idea what we're talking about, give us just a quick understanding of what an eval is, and let's start with Hamel.

Hamel Husain (00:05:49):
Sure. Evals is a way to systematically measure and improve an AI application, and it really doesn't have to be scary or unapproachable at all. It really is, at its core, data analytics on your LLM application and a systematic way of looking at that data, and where necessary, creating metrics around things so you can measure what's happening, and then so you can iterate and do experiments and improve.

Lenny Rachitsky (00:06:22):
So that's a really good broad way of thinking about it. If you go one level deeper just to give people a very, even more concrete way of imagining and visualizing what we're talking about, even if you have a example to show would be even better, what's an even deeper way of understanding what an eval is?

Hamel Husain (00:06:36):
Let's say you have a real estate assistant application and it's not working the way you want. It's not writing emails to customers the way you want, or it's not calling the right tools, or any number of errors. And before evals, you would be left with guessing. You would maybe fix a prompt and hope that you're not breaking anything else with that prompt, and you might rely on vibe checks, which is totally fine.

(00:07:11):
And vibe checks are good and you should do vibe checks initially, but it can become very unmanageable very fast because as your application grows, it's really hard to rely on vibe checks. You just feel lost. And so evals help you create metrics that you can use to measure how your application is doing and kind of give you a way to improve your application with confidence. That you have a feedback signal in which to iterate against.

Lenny Rachitsky (00:07:44):
So just to make very real, so imagining this real estate agent, maybe they're helping you book a listing or go see an open house. The idea here is you have this agent talking to people, it's answering questions, pointing them to things. As a builder of that agent, how do you know if it's giving them good advice, good answers? Is it telling them things that are completely wrong?

(00:08:04):
So the idea of evals, essentially, is to build a set of tests that tell you, how often is this agent doing something wrong that you don't want it to do? And there's a bunch of ways you could define wrong. It could be just making up stuff. It could be just answering in a really strange way. The way I think about evals, and tell me if this is wrong, just simply is like unit tests for code. You're smiling. You're like, "No, you idiot."

Shreya Shankar (00:08:29):
No, that's not what I was thinking.

Lenny Rachitsky (00:08:31):
Okay. Okay, okay, tell me. Tell me, how does that feel as a metaphor?

Shreya Shankar (00:08:35):
Okay. I like what you said first, which is we had a very broad definition. Evals is a big spectrum of ways to measure application quality. Now, unit tests are one way of doing this. Maybe there are some non-negotiable functionalities that you want your AI assistant to have, and unit tests are going to be able to check that. Now, maybe you also, because these AI assistants are doing such open-ended tasks, you kind of also want to measure how good are they at very vague or ambiguous things like responding to new types of user requests or figuring out if there's new distributions of data like new users are coming and using your real estate agent that you didn't even know would use your product. And then all of a sudden, you think, "Oh, there's a different way you want to kind of accommodate this new group of people."

(00:09:24):
So evals could also be a way of looking at your data regularly to find these new cohorts of people. Evals could also be like metrics that you just want to track over time, like you want to track people saying, "Yes. Thumbs up. I liked your message." You want very, very basic things that are not necessarily AI-related but can go back into this flywheel of improving your product. So I would say, overall, unit tests are a very small part of that very big puzzle.

Lenny Rachitsky (00:09:56):
Awesome. You guys actually brought an example of an eval just to show us exactly what the hell we're talking about. We're talking in these big ideas. So how about let's pull one up and show people, "Here's what an eval is."

Hamel Husain (00:10:06):
Yeah, let me just set the stage for it a little bit. So to echo what Shreya said, it's really important that we don't think of evals as just tests. There's a common trap that a lot of people fall into because they jump straight to the test like, "Let me write some tests," and usually that's not what you want to do. You should start with some kind of data analysis to ground what you should even test, and that's a little bit different than software engineering where you have a lot more expectations of how the system is going to work. With LLMs, it's a lot more surface area. It's very stochastic, so you kind of have a different flavor here.

(00:10:47):
And so the example I'm going to show you today, it's actually a real estate example. It's a different kind of real estate example. It's from a company called Nurture Boss. I can share my screen to show you their website just to help you understand this use case a little bit, so let me share my screen. So this is a company that I worked with. It's called Nurture Boss, and it is a AI assistant for property managers who are managing apartments, and it helps with various tasks such as inbound leads, customer service, booking appointments, so on and so forth. It's like all the different sort of operations you might be doing as a property manager, it helps you with that. And so you can see kind of what they do. It's a very good example because it has a lot of the complexities of a modern AI application.

(00:11:40):
So there's lots of different channels that you can interact through the AI with like chat, text, voice, but also, there's tool calls, lots of tool calls for booking appointments, getting information about availability, so on and so forth. There's also RAG retrieval, getting information about customers and properties and things like that. So it's pretty fully fleshed in terms of an AI application. And so they have been really generous with me in allowing me to use their data as a teaching example. And so we have anonymized it, but what I'm going to walk through today is, okay, let's do the first part of how we would start to build evals for Nurture Boss. Why would we even want to do that?

(00:12:36):
So let's go through the very beginning stage, what we call error analysis, which is, let's look at the data of their application and first start with what's going wrong. So I'm going to jump to that next, and I'm going to open an observability tool. And you can use whatever you want here. I just happen to have this data loaded in a tool called Braintrust, but you can load it in anything. We don't have a favorite tool or anything in the blog post that we wrote with you. We had the same example but in Phoenix Arize, and I think Aman, on your blog post, used Phoenix Arize as well. And there's also LangSmith. So these are kind of like different tools that you can use.

(00:13:29):
So what you see here on the screen, this is logs from the application, and let me just show you how it looks. So what you see here is, and let me make it full screen, this is one particular interaction that a customer had with the Nurture Boss application, and what it is is a detailed log of everything that happened. So it's called a trace, and it's just the engineering term for logs of a sequence of events. The concept of a trace has been around for a really long time, but it's especially really important when it comes to AI applications.

(00:14:12):
And so we have all the different components and pieces and information that the AI needs to do its job, and we are logged all of it and we're looking at a view of that. And so you see here a system prompt. The system prompt says, "You are an AI assistant working as a leasing team member at Retreat at Acme Apartments." Remember, I said this is anonymized, so that's why the name is Acme Apartments. "Your primary role is to respond to text messages from both current residents and prospective residents. Your goal is to provide accurate, helpful information," yada, yada, yada. And then there's a lot of detail around guidelines of how we want this thing to behave.

Lenny Rachitsky (00:14:56):
Is this their actual system prompt, by the way, for this company?

Hamel Husain (00:14:58):
It is. Yes, it is.

Lenny Rachitsky (00:14:58):
Amazing. That's so cool.

Hamel Husain (00:14:59):
It's a real system prompt.

Lenny Rachitsky (00:15:01):
That's amazing because it's rare you see a actual company product's system prompt. That's like their crown jewels a lot of times, so this is actually very cool on its own.

Hamel Husain (00:15:08):
Yeah. Yeah, it's really cool. And you see all of these different sort of features that are different use cases, so things about tour scheduling, handling applications, guidance on how to talk to different personas, so on and so forth. And you can see the user just kind of jumps in here and asks, "Okay, do you have a one-bedroom with study available? I saw it on virtual tours." And then you can see that the LLM calls some tools. It calls this get individual's information tool, and it pulls back that person's information. And then it gets the community's availability. So it's querying a database with the availability for that apartment complex.

(00:16:01):
And then finally, the AI responds, "Hey, we have several one-bedroom apartments available, but none specifically listed with a study. Here are a few options."

(00:16:12):
And then it says, "Can you let me know when one with a study is available?"

(00:16:16):
And then it says, "I currently don't have specific information on the availability of a one-bedroom apartment."

(00:16:23):
User says, "Thank you."

(00:16:25):
And the AI says, "You're welcome. If you have any more questions, feel free to reach out." Now, this is an example of a trace, and we're looking at one specific data point. And so one thing that's really important to do when you're doing data analysis of your LLM application is to look at data. Now, you might wonder, "There's a lot of these logs. It's kind of messy. There's a lot of things going on here. How in the hell are you supposed to look at this data? Do you want to just drown in this data? How do you even analyze this data?"

(00:17:07):
So it turns out there is a way to do it that is completely manageable, and it's not something that we invented. It's been around in machine learning and data science for a really long time, and it's called error analysis. And what you do is, the first step in conquering data like this is just to write notes. Okay? So you got to put your product hat on, which is why we're talking to you, because product people have to be in the room and they have to be involved in sort of doing this. Usually a developer is not suited to do this, especially if it's not a coding application.

Lenny Rachitsky (00:17:47):
And just to mirror back, why I think you're saying that is because this is the user experience of your product. People talking to this agent is the entire product essentially, and so it makes sense for the product person to be super involved in this.

Hamel Husain (00:17:59):
Yeah. So let's reflect on this conversation. Okay, a user asked about availability. The AI said, "Oh, we don't really have that. Have a nice day." Now, for a product that is helping you with lead management, is that good? Do you feel like this is the way we want it to go?

Lenny Rachitsky (00:18:30):
Not ideal.

Hamel Husain (00:18:32):
Yes, not ideal, and I'm glad you said that. A lot of people would say, "Oh, it's great. The AI did the right thing. It looked, it said, 'We didn't have available,' and it's not available." But with your product hat on, you know that's not correct. And so what you would do is you would just write a quick note here. You would say, "Okay." You might pop in here, and you can write a note. So every observability application has ability to write notes, and you wouldn't try to figure out if something is wrong. In this case, it's kind of not doing the right thing, but you just write a quick note, "Should have handed off to a human."

Lenny Rachitsky (00:19:19):
And as we watch this happening, it's like you mention this and you'll explain more. You're doing this, this feels very manual and unscalable, but as you said, this is just one step of the process and there's a system to this. That was just the first one.

Hamel Husain (00:19:30):
Yeah, and you don't have to do it for all of your data. You sample your data and just take a look, and it's surprising how much you learn when you do this. Everyone that does this immediately gets addicted to it and they say, "This is the greatest thing that you can do when you're building an AI application." You just learn a lot and you're like, "Hmm, this is not how I want it to work. Okay." And so that's just an example.

(00:19:58):
So you write this note, and then we can go on to the next trace. So this is the next trace. I just pushed a hot key on my keyboard. Let me go back to looking at it.

Lenny Rachitsky (00:20:09):
And these tools make it easy to go through a bunch and add these notes quickly.

Hamel Husain (00:20:13):
Yes. And so this is another one. Similar system prompt. We don't need to go through all of it again. We'll just jump right into the user question. "Okay, I've been texting you all day." Isn't that funny? And the user says, "Please." Okay, yeah, this one is just like an error in the application where this is a text message application, sorry, the channel through which the customer is communicating is through text message, and you're just getting really garbled. And you can see here that it kind of doesn't make sense. The words are being cut off like, "In the meantime," and then the system doesn't know how to respond, because you know how people text message, they write short phrases. They split their sentence across four or five different turns. So in this case-

Lenny Rachitsky (00:21:16):
Yeah, so what do you do with something like that?

Hamel Husain (00:21:18):
Yeah, so this is a different kind of error.

Lenny Rachitsky (00:21:19):
Mm.

Hamel Husain (00:21:19):
This is more of, "Hey, we're not handling this interaction correctly. This is more of a technical problem," rather than, "Hey, the AI is not doing exactly what we want." So we would write that down too.

Lenny Rachitsky (00:21:20):
Which is still really cool.

Hamel Husain (00:21:20):
Yeah.

Lenny Rachitsky (00:21:31):
It's amazing you're catching that, too, here. Otherwise, you'd have no idea this was happening.

Hamel Husain (00:21:35):
Yeah, you might not know this is happening, right? And so you would just say, "Okay." You would write a note like, "Oh, conversation flow is janky because of text message."

Lenny Rachitsky (00:21:51):
And I like that, I like that you're using the word janky. It shows you just how informal this can be at this stage.

Hamel Husain (00:21:56):
Yeah, it's supposed to be chill. Just don't overthink it. And there's a way to do this. So the question always comes up, how do you do this? Do you try to find all the different problems in this trace? What do you write a note about? And the answer is, just write down the first thing that you see that's wrong, the most upstream error. Don't worry about all the errors, just capture the first thing that you see that's wrong, and stop, and move on. And you can get really good at this. The first two or three can be very painful, but you can do a bunch of them really fast.

(00:22:38):
So here's another one, and let's skip the system prompt again. And the user asks, "Hey, I'm looking for a two- to three-bedroom with either one or two baths. Do you provide virtual tours?"

(00:22:51):
And a bunch of tools are called and it says, "Hi Sarah. Currently, we have three-bedroom, two-and-a-half-bathroom apartment available for $2,175. Unfortunately, we don't have any two-bedroom options at the moment. We do offer virtual tours. You can schedule a tour," blah, blah. It just so happens that there is no virtual tour, right?

Lenny Rachitsky (00:23:16):
Mm-hmm. Nice.

Hamel Husain (00:23:16):
So it is hallucinating something that doesn't exist. Then you kind of have to bring your context as an engineer, or even product content, and say, "Hey, this is kind of weird. We shouldn't be telling a person about virtual tour when it's not offered."

(00:23:32):
So you would say, "Okay, offered virtual tour," and you just write the note. So you can see there's a diversity of different kinds of errors that we're seeing, and we're actually learning a lot about your application in a very short amount of time.

Shreya Shankar (00:23:55):
One common question that we get from people at this stage is, "Okay, I understand what's going on. Can I ask an LLM to do this process for me?"

Lenny Rachitsky (00:24:04):
Mm, great question.

Shreya Shankar (00:24:04):
And I loved Hamel's most recent example because what we usually find when we try to ask an LLM to do this error analysis is it just says the trace looks good because it doesn't have the context needed to understand whether something might be bad product smell or not. For example, the hallucination about scheduling the tour, right? I can guarantee you, I would bet money on this, if I put that into chat GPT and asked, "Is there an error?" it would say, "No, did a great job."

(00:24:34):
But Hamel had the context of knowing, "Oh, we don't actually have this virtual tour functionality," right? So I think, in these cases, it's so important to make sure you are manually doing this yourself. And we can talk a little bit more about when to use LLMs in the process later, but number one pitfall right here is people are like, "Let me automate this with an LLM."

Lenny Rachitsky (00:24:55):
Do you think we'll get to a place where an agent can do this, where it has that context?

Shreya Shankar (00:24:58):
Oh, no. No, no, no. Sorry. There are parts of error analysis that an LLM is suited for, which we could talk about later in this podcast. But right now, in this stage of free form, note-taking is not the place for an LLM.

Lenny Rachitsky (00:25:13):
Got it. And this is something you call open coding, this step?

Shreya Shankar (00:25:14):
Yes, absolutely.

Lenny Rachitsky (00:25:17):
Cool. Another term that you used in your posts that I love and that fits into this step is this idea of a benevolent dictator. Maybe just talk about what that is, and maybe, Shreya, cover that.

Shreya Shankar (00:25:27):
Yeah, so Hamel actually came up with this term.

Lenny Rachitsky (00:25:29):
Okay, maybe Hamel cover that, actually.

Hamel Husain (00:25:33):
No problem. And we'll actually show the LLM automation in this example, because we're going to take this example, we're going to go all the way through.

Lenny Rachitsky (00:25:40):
Amazing.

Hamel Husain (00:25:41):
And so benevolent dictator is just a catchy term for the fact that when you're doing this open coding, a lot of teams get bogged down in having a committee do this. And for a lot of situations, that's wholly unnecessary. People get really uncomfortable with, "Okay, we want everybody on board. We want everybody involved," so on and so forth. You need to cut through the noise. And a lot of organizations, if you look really deeply, especially small, medium-sized companies, you can appoint one person whose tastes that you trust. And you can do this with a small number of people and often one person, and it's really important to make this tractable. You don't want to make this process so expensive that you can't do it. You're going to lose out.

(00:26:36):
So that's the idea behind benevolent dictator, is, "Hey, you need to simplify this across as many dimensions as you can." Another thing that we'll talk about later is when it goes to building an LLM as a judge, you need a binary score. You don't want to think about, "Is this like a 1, 2, 3, 4, 5?" Like, assign a score to it. You can't. That's going to slow it down.

Lenny Rachitsky (00:26:59):
Just to make sure this benevolent dictator point is really clear, basically, this is the person that-

Lenny Rachitsky (00:27:00):
Make sure this benevolent dictator point is really clear. Basically, this is the person that does this note-taking, and ideally they're the expert on the stuff. So if it's law stuff, maybe there's a legal person that owns this, it could be a product manager. Give us advice on who this person should be?

Hamel Husain (00:27:16):
Yeah. It should be the person with domain expertise. So in this case, it would be the person who understands the business of leasing, apartment leasing, and has context to understand if this makes sense. It's always a domain expert, like you said. Okay. For legal, it would be a law person. For mental health, it would be the mental health expert, whether that's a psychiatrist or someone else.

Lenny Rachitsky (00:27:41):
Cool.

Hamel Husain (00:27:42):
Though oftentimes, it is the product manager.

Lenny Rachitsky (00:27:44):
Cool. So the advice here is pick that person. It may not feel so super fair that they're the one in charge and they're the dictator, but they're benevolent. It's going to be okay.

Hamel Husain (00:27:52):
Yeah. It's going to be okay. It's not perfection. You're just trying to make progress and get signal quickly so you have an idea of what to work on because it can become infinitely expensive if you're not careful.

Lenny Rachitsky (00:28:07):
Yeah. Okay, cool. Let's go back to your examples.

Hamel Husain (00:28:09):
Yeah, no problem. So this is another example where we have someone saying, "Okay. Do you have any specials?" And the assistant or the AI responds, "Hey, we have a 5% military discount." User responds, and it switches the subject, "Can you tell me how many floors there are? Do you have any one-bedrooms available or one-bedrooms on the first floor?" And the AI responds, "Yeah, okay. We have several one-bedroom apartments available." And then the user wants to confirm, "Any of those on the first floor and how much are the one-bedrooms?" And then also, it's a current resident, so they're also asking, "I need a maintenance request."

(00:28:56):
You could see the messiness of the real world in here, and the assistant just calls a tool that says transfer call, but it doesn't say anything. It just abruptly does transfer call, so it's pretty jank, I would say. It's just not-

Lenny Rachitsky (00:29:13):
Another jank.

_[843 additional lines trimmed for context budget]_

---

### How to see like a designer: The hidden power of typography and logos | Jessica Hische
**Guest:** Jessica Hische | **Date:** 2024-10-20 | [YouTube](https://www.youtube.com/watch?v=tLLqE6Ia8-U)  

# How to see like a designer: The hidden power of typography and logos | Jessica Hische

## Transcript

Jessica Hische (00:00:00):
Most people are better at understanding the feelings and sensations that typography and logos give us than they give themselves credit for, because what we are as people are endless absorbers of patterns, and information, and all this kind of stuff as we move throughout the world. We don't take time to sit and digest it, but it's still coming in and getting logged, and so even as a non-designer, I think you can look at examples of logos where something's not quite right and be like, "Something's not right here, I just don't know how to name it." But I think a good exercise is just like looking at fonts that are available in the world and asking yourself, "What feeling does this give me?"

Lenny Rachitsky (00:00:47):
Today, my guest is Jessica Hische. Jessica is a design legend, and it was such an honor to both have her on this podcast and also to work with her on a refresh of my newsletter and podcast logo and brand, which is launching around the same time as this episode comes out. Jessica is a lettering artist specializing in typographical work for logos, film, books, and other commercial applications. Her clients include Wes Anderson, the United States Postal Service, Apple, Nike, Tiffany and Company, The Gap, and Penguin Books, and her work has been featured in design and illustration annals, both in the U.S. and internationally. She's helped create logos for Philz Coffee, Eventbrite, and Mailchimp, is a best-selling children's book author, and if you live around the Bay Area, you've seen her work all over the city without knowing it. In our conversation, Jessica shares the process that she went through to update my logo and brand for my newsletter and podcast.

(00:01:38):
What specific elements of a logo and brand impact how you feel about that brand, why a good enough logo is just buying for a long time for most startups, and when it makes sense to refresh your look, also some really clever productivity tips, design advice, and a bunch of really fun stories. Jessica is a master at what she does and I am excited to spread the Jessica Hische gospel. If you enjoy this podcast, don't forget to subscribe and follow it in your favorite podcasting app or YouTube. It's the best way to avoid missing future episodes, and it helps the podcast tremendously. With that, I bring you Jessica Hische. Jessica, thank you so much for being here. Welcome to the podcast.

Jessica Hische (00:02:19):
Happy to be here.

Lenny Rachitsky (00:02:21):
I thought it'd be good to start with asking you just to describe what it is you do, because you're very atypical of the kinds of guests I have in this podcast and you also have very unique skillset.

Jessica Hische (00:02:32):
Yes. Well, I will describe what I'm most prominently known for, because I'm a person who just does a lot of things, but the thing that I do the most professionally is custom typography, like bespoke lettering pieces. That translates to working for all kinds of things. Sometimes it's for film and television. I've done movie titles and things like that and television credits and stuff. Sometimes it's book covers. Actually, a lot of times, it's book covers, and then a big part of my business is doing logos, and logo refreshes, and things like that, so basically being the person who knows all the things so you don't have to have that person on staff when it comes to typography.

Lenny Rachitsky (00:03:17):
This episode is brought to you by the Enterprise Ready Conference, a one-day event in San Francisco bringing together product and engineering leaders shaping the future of enterprise SaaS. The event features a curated list of speakers with direct experience building for the enterprise, including leaders from OpenAI, Vanta, Checkr, Dropbox, and Canva. Topics include advanced identity management, compliance, encryption, and logging, essentially at complex features that most enterprise customers require. If you're a founder, exec, product manager, or engineer tasked with the enterprise roadmap, this conference is for you. You'll get detailed insights from industry leaders that have years of experience navigating the same challenges that you face today. Best of all, it's completely free. Since it's hosted by WorkOS. Spots are filling up quickly, make sure to request an invite at enterpriseready.com. That's enterpriseready.com. I am excited to chat with Christina Gilbert, the founder of OneSchema, one of our longtime podcast sponsors. Hi, Christina.

Christina Gilbert (00:04:21):
Yes. Thank you for having me on, Lenny.

Lenny Rachitsky (00:04:23):
What is the latest with OneSchema? I know you now work with some of my favorite companies like Ramp, Vanta Scale, and Watershed. I heard that you just launched a new product to help product teams import CSVs from especially tricky systems like ERPs.

Christina Gilbert (00:04:39):
Yes. We just launched OneSchema FileFeeds, which allows you to build an integration with any system in 15 minutes as long as you can export a CSV to an SFTP folder. We see our customers all the time getting stuck with hacks and workarounds, and the product teams that we work with don't have to turn down prospects because their systems are too hard to integrate with. We allow our customers to offer thousands of integrations without involving their engineering team at all.

Lenny Rachitsky (00:05:01):
I can tell you that if my team had to build integrations like this, how nice would it be to be able to take this off my roadmap and instead use something like OneSchema, and not just to build it but also to maintain it forever.

Christina Gilbert (00:05:12):
Absolutely, Lenny. We've heard so many horror stories of multiday outages from even just a handful of ad records. We are laser-focused on integration reliability to help teams end all of those distractions that come up with integrations. We have a built-in validation layer that stops any bad data from entering your system, and OneSchema will notify your team immediately of any data that looks incorrect.

Lenny Rachitsky (00:05:32):
I know that importing incorrect data can cause all kinds of pain for your customers and quickly lose their trust. Christina, thank you for joining us. If you want to learn more, head on over to oneschema.co. That's oneschema.co.

(00:05:47):
Part of the reason I was excited to have you on this podcast is that I was lucky enough to get to work with you on a refresh of my logo and brand for my newsletter and my podcast, which I'm very, very excited about. It's actually going to be launching right around the time of this episode going live, so this is, in part, a celebration of the new look, and logo, and brand. I thought it'd be an awesome excuse to bring you on the podcast and give an inside glimpse into the process of updating a logo and a brand. Partly, because I think it'll just be people are like, "What the hell? How did this change? Where did this come from? Why this versus that?" and also just for people that are thinking about this for their own product or business, to understand the process and understand when it might be right for them, when it might be not right for them. Broadly, how does that sound?

Jessica Hische (00:06:32):
Sure. Yeah, of course. Everybody that starts a company knows that they need a logo. That is a big thing. Some people start a company and think the logo is going to drive the culture of the company, which I don't think that that is true. I think that the product itself, and the team you build, and the people you put together are the thing that should be driving things forward, but I do think the logo and the brand assets can generate a lot of both internal and external excitement and just tell people what to expect from the thing that they're about to engage with. Some people say don't judge a book by its cover. I'm the opposite, where any book... The cover of the book should be giving you incredible insight into what is on the interior of the book and setting the tone and setting the vibe so that when you open the book and read the book, it's a symbiotic thing where you're like, "Oh, I understand what I was getting into. This got me excited about starting it," and whatever. It keeps that ball rolling.

(00:07:34):
But with the refresh work that I do, a lot of people start companies and they have a certain amount of money. If they're bootstrapping it, they have less. If they get venture money, they have a little bit more. But what they don't want to do is spend venture money on a massive brand exploration when you're still in the hiring process, you're still trying to get early stage engineers, and all that kind of stuff, and so I am sort of a weird contrarian in this way in the brand world where brand people are like, "Brand is everything. You need to take a significant investment in brand because that guides the vision of the product or whatever." But I think being a bit more of an insider within the tech world, I understand that sometimes people start companies and have an intention to do something.

(00:08:24):
While they're doing that, they're building the team, they're doing cool stuff, but then the company has to pivot for one reason or another, whether a competitor immediately comes out with a thing that you're doing or the technology that you're doing gets postponed, whatever's going to feed into it, and so if you invest super heavily on the whole brand vision from the jump, sometimes it's like throwing away money if you have to pivot. What I love about the work that I do is that I understand that a lot of people have to just have something to put on decks and have something to put on a holding page or whatever, and internal teams are totally capable of doing that early work. But then if it does become successful, you don't want to get locked into whatever it is that you had to throw together before an investor meeting or something.

(00:09:15):
I come in then to take the existing vibe and smooth it out, address any of the concerns that came up. A lot of times, it's really utilitarian stuff like this doesn't scale well, or this thing falls apart in this context, or we never had a good avatar version of it, or whatever. Sometimes it's really specific utilitarian fixes, and sometimes it's just about growing it up and sophisticated it without losing what was there in the first place that people got excited about.

Lenny Rachitsky (00:09:46):
To give people maybe a couple more tactical piece of advice here, what's a sign that it's maybe time to do something with your logo and brand from you took a first pass, it's good enough, your wife made one, your husband made you a logo, it's like, "Well, this is great. Let's just go with this"? What's a sign that maybe it's time to, "Okay, we should actually at least uplevel this, not necessarily hire studio but takes something to make it better"?

Jessica Hische (00:10:10):
Well, one of the things is if, suddenly, you're starting to deal with the greater rollout of the look and feel of the brand. If you, at first, just basically had a really beta website and small version of the app but you're about to do a new one that kind of updates and expands it, that could be a really good time to roll it out. One of the things that a lot of people do when they're starting a company is that they'll make a logo with a font that is really popular or widely available or free. If you are about to, let's say, print a bunch of swag for new hires, or you're hosting a conference, or whatever is that event where suddenly you're going to physically invest money in making stuff with your brand on it, that might be a good time to do it.

(00:11:03):
One of the reasons why I tell people why having a custom logo or custom typography can really matter is that if you're using something that's available to everyone, the chances of someone else coming in and copying you are very easy and high. You might be one of these lucky companies that just out the gate is outrageously successful, but with success comes people climbing up behind you trying to copy your success. If that success is very easily copyable and people will try to trick your customers into coming their way by repeating the things that you're doing, including the branding, one way to avoid that is by doing something that's more customized when it comes to the logo and brand.

Lenny Rachitsky (00:11:47):
Let's actually talk about the work that you put into updating my logo and brand. From that, we can spin off into lessons and insights that you have along these lines. Overall, just high level, what's the process you go through to go through a logo refresh?

Jessica Hische (00:12:01):
Sure, of course. Super high level, it's figuring out what the goals are. Some people have a goal where they want their customers to not notice it at all. They're like, "Oh, everybody loves this, but I, the person with a design eye, can see all the problems with it. I need to roll something out that fixes the problems that I see but that no one else really notices." If that's one approach to it, which is a very close in refresh where we're just trying out little things to make it feel custom or fixing things that have come up when stress-tested, whether it looks crazy when you scale it and looks like horsey and heavy-handed when you scale it too big or when you scale it too small, you lose a lot of legibility and stuff like that. That's what would be a really close in exploration.

(00:12:53):
For other people, they might have bigger goals where they're about to pivot the company to try to attract a different audience or things like that. They might have this really cool and successful group of people that are super users but they're trying to expand it, so then it's about, "What can we do to shift the vibe to make it include these new people without excluding our core folks?" My first round is figuring out what that scope is, how experimental, how broad are we going to go for that first round, because then everything else cascades from there. If we go really broad for the first round, then we're in this process of narrowing down the scope as we go down. Whereas if we go really close in for the first round, we're already just talking about really technical stuff.

(00:13:45):
This is another thing that is a little bit unique to me that not everybody else does, but I will hand off files to clients to try in situ really early on if what we're trying to do is solve utilitarian issues. Most other people are like, "No, you don't get the files until we do the final because I don't want you to run away with this or whatever." There has to be some trust with it. Yeah, figure out what we're trying to accomplish and always keep those goals in mind, and then scope out the process based on what those goals are. For each round, we're addressing different things. First round might be about just capturing the overall look and trying things as broad as we can, and then the next round is, "Okay. Well, now we have generally a look that we want, but is it the right weight? Is it the right letter height? Do we have enough details?" That kind of thing, and then we get narrower and nerdier as we go along in the process.

Lenny Rachitsky (00:14:45):
Awesome. In the experience of my podcast logo, I was an ass. At the end of it, I'm like, "No, this isn't right," because we narrowed, narrowed, narrowed, and then I was like, "No, I'm not feeling it," and then we unnarrowed it and went back. [inaudible 00:15:02]

Jessica Hische (00:15:01):
That is not an uncommon experience though. This is why there's a few things that I'm not opposed to that I know people that are very opposed to. I'm not opposed to Frankensteining options together. To me, it's like I'm giving you a menu of all the things that we can do, but I'm like a chef that puts a menu together where you can combine different appetizers and different mains and they all still make sense. Some people will give you a menu and if you try to do that, it's insane and it tastes terrible. But for me, everything that we're doing, I feel like, mixes and matches fairly well, and I will let you know if it doesn't.

(00:15:37):
It's not unreasonable for clients to go down one path and then use that as validation or confirmation that something that we did much earlier was actually the right way. In that way, if a client ever asks me to do something, they're like, "Let's see it in purple or whatever." There's designers that will really fight you tooth and nail, because they're like, "That is wrong, that is wrong," but I know that some people just have to see it before they can let it go. Sometimes you have to walk down a path before you're able to understand what the right thing to do was all along.

Lenny Rachitsky (00:16:10):
That's exactly how it felt for me. Going back to the goals, the different goals people have when they're exploring this, you mentioned there's just little problems with the logo, it doesn't scale, it doesn't print well, that kind of thing. Another goal is people are pivoting and they want to change the vibe and the feel of the brand. What other goals might appear that you've come across why people want to do this?

Jessica Hische (00:16:31):
Well, sometimes there's a legibility issue that is really glaring once you see it and then people are too close in to notice it, so this is why it's like you have to show it to a lot of people that aren't familiar with looking at those letter forms and stuff. The one that came to mind immediately when you asked was when I did a refresh for Jeni's Ice Cream, which is a really amazing ice cream brand based out of Columbus, Ohio. They had a few utilitarian things in mind. They had this long J that created this pocket of white space underneath it that made it really hard to design with, but then the biggest glaring one was that they decided to make the apostrophe of the Jeni's over the I as a cool thing. It made the word look like it said penis because of the way that J was drawn.

(00:17:18):
It had a little loop at the top of the J. That was the most specific fix that I've ever had to do, is make the logo not say penis anymore. But yeah, I feel like a lot of it is misreads. When you think about logos and things like that, you want it to be something that, at a super fast glance, people can read it right away. That doesn't mean that everything has to be simple, but it just means that everything has to be incredibly legible, especially when you're starting a new company or you have a less recognizable brand. Because eventually, you become a household name and then people can look at just the color and recognize that it's you or whatever, but it takes a long time building equity before you get that. Until that, it's really important that the legibility is just super tight.

Lenny Rachitsky (00:18:04):
Let's talk about actually thinking through my new logo and brand. Can you just talk about what you had in mind as you started to explore directions and we started narrowing what was the mindset, and the approach, and the vibe?

Jessica Hische (00:18:18):
I think with the Lenny's brand, you are what I would call a person that has a beloved fan base that we don't want to exclude or offend by shifting gears super crazily. When I was looking at redoing your brand, it was about what's here that we can look at that we should keep or at least explore keeping in order to make sure that it still feels the same on the other side? We didn't want to do anything super drastic. Some of the things to think about in that way are you have this sort of handwritten approach to a lot of the parts of your brand, and I was like, "How do we do something that feels handwritten, that feels like it could jive with the handwriting scripts and stuff that you were using but might feel a little bit more refined moving forward and in a way that could blend with illustration so that the illustration and the letter forms all feel like they were created by the same hand?"

(00:19:17):
A lot of times for me, when it comes to doing an exploration like that, it's about how do we make sure everything feels like it was created at the same time and not that we just tacked on new things? Figuring out how to blend everything together is important. Color was also a big one. I feel like people's color stories are something that is an immediate read, where if you blur your eyes and look at a brand, you can recognize it by its color, and so I think keeping your color similar was a big one too. But then I really wanted to try different approaches with the typography, doing stuff that was a little bit more clean, doing stuff that still had a bit of a funky edge to it. Because your original Lenny's podcast type had this sort of cut papery, a little bit off kilter vibe to it, and so trying to capture that but with new cleaner typography. That was really fun.

(00:20:13):
I was using a typeface Degular as part of it, which has a nice wonkiness to it. It was drawn by a friend of mine, James Edmondson, who I love all of his stuff. That was one of the things that we approached because I felt like it could capture the look of the original cut paperesque type. But yeah, it was really fun to try a lot of different things and work within the iconography that you had used on a few stuff with the microphones, and also marshmallows, and campfires, and all kinds of things, and just trying to make a more unified system.

Lenny Rachitsky (00:20:47):
Are there things that didn't work as you tried to explore this process that you recall, that might be worth sharing of just like, "Oh, that was a cool concept," but didn't quite work the way you thought?

Jessica Hische (00:20:57):
With working on this refresh, so much of it was thinking about what are the immediate uses of the logo. Your logo, you have these very specific uses that you need it for, the avatar for the podcast being a top one. Some people, they'll design a logo and then have these illustrative versions of the logo that get rolled out for other things, but I feel like those layouts were actually more important than even just having a basic letterhead-esque logo. That's an interesting way to approach it, because usually, it's like let's start simple, and expand, and make it crazier. I felt like we had to always keep those things in play, so it was designing brand assets while also designing the logo. Sometimes you'll see examples of that in early explorations just because it makes it feel real to the client.

(00:21:53):
That's why there's always the tote bag. You got to put the logo on the tote bag and then it feels real or whatever. But in our case, we had really specific uses that needed to get explored very early on, and that made it a slightly different process for me. In terms of things that didn't work though, I think just trying to work on the level of detail within the illustration and type so that it could shrink down, because we knew... One of the primo examples of the logo being used was on that podcast avatar, and some of the versions felt a little bit too detailed to shrink down that much. Trying to get the balance of that, having the illustrations feel illustrative and whole, but not when they scaled up, to feel too simple like we just pulled it from a icon library. I think that was an interesting challenge.

Lenny Rachitsky (00:22:46):
One of the impetuses for changing my logo, for motivating me, is my wife's a designer, as you know. She's got strong opinions about my logo, and she always was making fun of my logo saying it looked like a clip art fireplace that anyone just could just plug and play. She's always like, "Oh, that's so bad. You got to change it." That was one of the motivators. I think I might have shared that when we started working together.

Jessica Hische (00:23:08):
Yeah. Also too, I think that really talks to what I was saying earlier about making sure that everything feels like it was created together rather than it feeling like these disparate elements, because I think you can have an icon or logo that is quite simple as long as the rest of the brand matches that simplicity. I think, for you, the biggest thing was making the illustration and the typography just feel like it came from the same universe instead of feeling like these separate elements. When I first moved to San Francisco, I moved to San Francisco from New York, and as a New Yorker, I had an apartment with a full kitchen that never got used. I just ate at restaurants and did takeout for the seven years or whatever that I lived in New York.

(00:23:51):
Learning to cook, at first, when you learn how to cook, it's like you're making a pot full of ingredients and none of the ingredients are actually working together. But then the more you do it and the more you understand how these things are meant to blend and cook at different temperatures at different times, you start having this cohesive dish rather than hot ham water. You know what I mean? I feel like a lot of clients come to me and they have hot ham water when it comes to their brand. It's just about turning that into a soup, something that feels real.

Lenny Rachitsky (00:24:25):
I want to take a quick tangent. Most people listening to this podcast are not designers. They're product managers, founders, engineers, and folks building product. I've always wanted to see the world through the eyes of a designer, because there's so much I don't see and there's so much that affects how I think about something that I don't understand when I look at a logo, and so I thought it might be helpful just to spend a little time helping people see a designer a little bit. Let me just ask you this question. When people look at a logo or a brand, what are just elements that make it what it is, that make you feel the thing you want to feel that we may not recognize?

Jessica Hische (00:25:06):
I think most people are better at understanding the feelings and sensations that typography and logos give us than they give themselves credit for, because what we are as people are endless absorbers of patterns, and information, and all this kind of stuff as we move throughout the world. We don't take time to sit and digest it, but it's still coming in and getting logged. That's why if you see something funky in the world, you're like, "That's weird. I don't like that. I don't know why I don't like it, but I know I don't like it." I think even as a non-designer, you can see that in typography. The whole being able to recognize patterns thing, I talked about this a bit at config, it's like it's a safety thing. Looking into the world, your eyes can spot that thing that's a little bit off, and that thing that's off feels not safe to you.

(00:26:08):
It's thinking about when we look at a meal and there's a thing on the plate that looks like it's moldy or something like that. You understand that doesn't look right to me, this doesn't smell right to me. Your body knows it before your brain knows it. Even as a non-designer, I think you can look at examples of logos where something's not quite right and be like, "Something's not right here, I just don't know how to name it." But I think a good exercise is just looking at fonts that are available in the world and asking yourself, "What feeling does this give me?" and just write them down. It doesn't matter, just give yourself permission to say whatever is happening in your mind, the first thing. Don't overanalyze it. Just look at it and be like, "That feels calm to me. That feels exciting to me. That feels whatever to me."

(00:27:00):
The more you do that, the more you can start seeing similarities in the ones that feel exciting, and the ones that feel calm, and the ones that feel whatever, and then get into analyze mode of, "Oh, these 10 things that I said feel calm are a lighter weight, have more generous spacing, have rounded edges, have rounder bowls to the letter forms." You just start seeing commonalities between the things. It's just about seeing them all together to understand what those similarities are. I think anybody can do that. I mean, you're not going to have the language of the leg of the R and the tittle of the I. Don't worry about that. You don't have to know typography language to think about it, but anybody's capable of doing that. It can be really fun to just stop and ask yourself and notice.

Lenny Rachitsky (00:27:58):
This is great. I want to go actually a little deeper. What I'm hearing is look at something, tap into the feeling you feel when you look at it, actually pay attention to it because there's wisdom in that. The specific things that you pointed out that impact that feeling are, you mentioned, spacing between the letters, the edges. I imagine there's just the color of it. What other specific elements impact the way someone feels when they look at a logo?

Jessica Hische (00:28:26):
There's the width of the letter, so if it's really narrow versus really wide. I always think about the width, the weight, [inaudible 00:28:35] facing, sort of detailed treatment of things, whether things are very hard and jagged or soft and how soft it is. Sometimes we just add a tiny bit of softness so that it just feels printed. You can take a typeface like Helvetica, just the one everybody knows. But if you take Helvetica and just ever so slightly round the edges just a little bit, all of a sudden, you have this typeface that feels more vintage, or softer, or whatever, because we're perceiving it, we would perceive it if it were printed on paper versus perceiving it as this hard geometric piece of technology that we're viewing. You know what I mean?

(00:29:20):
I know that it's because when you look at stuff printed on a page, it bleeds into the paper a little bit, which means that that softness reminds us in our bodies of a thing that we've seen that was printed. It's cool to sort of walk back your feelings also. You'll look at something and go, "That feels like this," and then ask yourself why does it feel like that. It might be because you saw it on a flyer for a band when you were 22, and it brought out that feeling in you of what it felt like to be 22 at that thing. That's a very specific feeling to you, but it can inform your decisions about design, because you can be like, "Oh, I'm not that much of a special snowflake." Other people might have that same reaction but have different experiences that are adjacent to that reaction.

(00:30:08):
It's cool because you are reverse-justifying decisions. I think that's a really fun exercise to do, is to Song Exploder your intuition. You make a decision intuitively or look at something and intuit what you feel from it, and then really try to dive in, "Why do I feel that way? What could this have reminded me of that made me feel that way?" You have to be just so forgiving and loose with yourself as you do it because then you'll get into some really weird stuff, and that's really great inspiration juice for picking other things.

Lenny Rachitsky (00:30:51):
I love the exercise that you gave. The one you gave earlier is look at a bunch of fonts, look at your font folder. Is that a place to go, just open up your font folder and just go through them?

Jessica Hische (00:30:55):
Totally. Look at your font folder, or go to MyFonts, or a place where there's a ton of fonts, and just search for something. Search for sans serif or whatever. Search for a really basic category of fonts. Serif, sans serif, script, whatever, the top level edge of stuff, and then just page through, page through, page through. Screenshot stuff that you like and make a folder full of screenshots, and then you can take those screenshots and start categorizing them. "Oh, this one feels feminine. This one feels masculine, this one feels aggressive. This one feels whatever." Just take some notes on it. Then, you ask yourself, "Well, why did I feel that way?" You're like, "Oh, well, this feels feminine because it reminds me of wedding invitations," and wedding invitations feel inherently bridey versus groomy. All of a sudden, you're like, "Okay. Well, now I know that if I'm going to use a script for something, this zone of script feels very wedding, so maybe I avoid that for this brand that's actually a cutting edge food packaging company or whatever because it feels too aligned with that industry."

(00:32:10):
Stereotypes are real and trends are real, and what can sometimes happen is some industry usurps an entire style for a period of time. If you use anything within that style, it's like you're aligning yourself to that industry. I mean, everybody that does branding, one of the things that they do is they analyze the competitors of the company. You just look at a landscape of what are all the competitors doing, what is their visual vibe, and do I lean into that or do I avoid that? If I lean into it, then I'm immediately getting this... Everyone that looks at it understands I'm a FinTech company because I look like a FinTech company. If my whole thing is I'm trying to be divergent from that, I'm trying to show how different I am from the status quo, then you use it as a reactionary thing of I want to do nothing like that and do something really different so people understand this isn't just another FinTech company.

Lenny Rachitsky (00:33:09):
Yeah. I'm thinking green. The color green has to be a part of the logo if you want to be a FinTech company.

Jessica Hische (00:33:13):
Yeah, exactly. If you're trying to be weird, then you're like teal.

Lenny Rachitsky (00:33:19):
On this topic of seeing a designer, is there any other tip, just before we move on to a different topic, of just how someone could learn to see a designer a little bit more?

Jessica Hische (00:33:30):
Yeah. Another thing to notice, because I'm assuming you're... Product people, I feel like a lot of product people end up having some engineering background, whether or not they're engineers themselves. They have to interface with the engineers and they build stuff, and so they come at it from a data standpoint. I can always tell when there's an engineer that has suddenly got taken an interest in type design and is now a type designer, because everything is very, very regular. You can draw a grid on top of everything and the lines all line up perfectly. You see lots of reverse justification of that when people are making logos and they have a more engineering background. The thing to notice, that is interesting within type, is that, yes, you're absolutely following rules, but you're breaking those rules quite often to correct for optical tricks.

(00:34:24):
If you look at a geometric sans serif, for instance, that's like a category of sans serifs, and they're meant to have a lot of really strong geometry, be very regular, like most of the sans that you think of that you're like, "I'm in love with this over the last 10 years," like geometric sanses. But when you really start to examine them, you notice that there's all these little things that people are doing to make them look perfectly geometric even though they are not mathematically perfectly geometric. That's another thing that you can do, whether you're doing it in Figma or doing it somewhere else, is just type out a couple of lowercase letters. Lowercase specifically are really good for analyzing this because they're smaller than the uppercase letters. You usually have to accommodate for the weight a little bit differently. You'll notice that when strokes combine, so say I have an A and I'm combining the lower bowl of the A, I'm going to get a little thinner as I come into that.

(00:35:23):
Or say I have a two-storey A, so a two-storey A is the one that's like this and then a bowl, you might notice that this vertical of the A there, the bowl actually eats into that stroke a little bit to erase a little bit of that added weight that would've been perceived optically had you kept everything perfectly regular. It's weird because you end up creating something that's perfect and then have to make it not perfect in order to make it be perceived as perfect. That's another just fun thing to start noticing. You notice it a lot more on typefaces and typography that is heavier in weight, because when things are heavier weight, you're constantly managing these really inky moments where things join together and you have to subtract a bunch of weight from that so that it doesn't get perceived as this dark mark where the letter is happening.

(00:36:22):
I think about lowercase Rs and lowercase Ns, where the shoulder of the N or the R comes out, sometimes the top of the R is actually narrower at the top than it is on the bottom, and that's to try to subtract some of that weight in there where that join happens. Anyway, that's just a fun thing to notice that you have to do that. Once you start seeing it, you start seeing where it happens more often, and the answer of why it happens is because correcting for this optical weight issue. You're like, "Oh, man. Now I have x-ray vision. I can see all these weird things I've never seen before." It's very fun. But anyway, a lot of people, when they first start out doing typography, whether you're an engineer or whether you are a designer, they don't account for that. I can always just look at something and see whether someone is truly an expert at typography or whether this is a fun hobby for them when they're pretty fresh at it.

Lenny Rachitsky (00:37:20):
The exercise here is open up Figma, start typing, and make it really big so you can basically see the font really zoomed in.

Jessica Hische (00:37:20):
A single letter.

Lenny Rachitsky (00:37:27):
A single letter.

Jessica Hische (00:37:28):
Make a single letter the whole page and then just draw some vertical lines, or do the thing where you draw a little circle or whatever and see if that circle is the same size at the point where two strokes join together or the point where the stroke is just vertical and on its own. You'll notice that there's differences even in typography that's meant to look extremely rigid and geometric.

Lenny Rachitsky (00:37:53):
I want to come back to my logo just to close the loop there. Talk about just the final result that we landed on, and why you think that was the final answer, and what you think people might get from it, whatever comes to mind.

Jessica Hische (00:38:07):
Yeah, totally. I think the final answer or the final logo, we took it down a lot of different paths trying to see where it was going to land, but ultimately ended up keeping it pretty close to home and really focusing on that asset of the fire. We tried so many different versions of it's microphones with the fire and marshmallows with the fire, et cetera. It was just about sometimes the simplest solution is the correct solution. I don't know, I just feel like in terms of what we were doing for blowing it out and making it really cohesive, we went a lot of directions where there might've been multiple versions of the logo depending on the scale and ultimately ended up in a place where it's much more in line and consistent across the bar.

Lenny Rachitsky (00:38:55):
Yeah. Maybe what I'll do is we'll share, I don't know if you're comfortable with that, we could just share all the iterations somewhere that we went through.

Jessica Hische (00:39:01):
Oh, yeah. I love sharing iterations. It's very fun. Yeah.

Lenny Rachitsky (00:39:03):
Okay, cool. Sweet. The biggest issue we had with the podcast logo specifically is, originally, I was thinking the mic made sense to differentiate it from the newsletter. At the end of it, it's just like, "Why do we have this freaking mic in there? It just feels strange." That's where we revisited the whole idea and killed the mic and went back to a different version of the fireplace but with marshmallows.

Jessica Hische (00:39:24):
Yeah, I love the marshmallows.

Lenny Rachitsky (00:39:26):
Awesome. I love it. That was my wife's... My wife loves the marshmallows also, and it's so versatile. We can use it in so many different ways.

Jessica Hische (00:39:33):
Indeed.

Lenny Rachitsky (00:39:34):
I guess maybe one last thread there is we explored handwritten typography that you created from scratch, and then there's the block letters. Maybe just thoughts on those two options and benefits and why go one direction versus another?

Jessica Hische (00:39:51):
The handwritten one, I really liked because I felt like we could bring the line quality of the illustration into the handwriting, but then the only problem with the handwriting is that if you want to blow that out, if you want to include other handwriting throughout the rest of the brand, finding something that matches that perfectly without creating a custom typeface is a whole thing. I really like being able to combine the two, where we have this broader visual vocabulary that we can pull from, because you're going to have headlines, you're going to have subheads, you're going to have all these other uses for typography moving forward. If you just have one thing to pull from, it's a lot harder to work with. It's just nicer when you have a few elements to play with. It's like having a wardrobe. If you have only one shirt and one pair of pants, there's only so many things that you can do. But if you have all these things that work together and can recombine, then you can blow out a brand system much more easily.

Lenny Rachitsky (00:40:44):
Yeah. That's one of my favorite things about working with you, is you create all these different variations of ways to use it. It doesn't always have to be the handwritten one. It could be the blocky outlining one. It's not like, "This is it, don't change anything about this. This is the only way to use it."

Jessica Hische (00:40:57):
Well, part of that is because some brands, if you have a massive company, like hundreds of employees generating hundreds of things, sometimes having too many assets can overcomplicate stuff. Because unless you have a really, really well-written brand book outlining how to do everything that people are adhering to very closely, you can get the assets running rampant throughout all of the stuff and being used incorrectly. But because you're not a massive company and you have creative control over the things that are happening and can help direct that, we can be much more playful with the assets and give you the ability to use things in different ways. It depends on how much you trust all the people that are handling your assets.

(00:41:43):
I am of the mind that you shouldn't need a 500-page brand book in order to direct how the brand is used moving forward, and that if you do, the brand might be quite complicated, or there might be even just parts of the logo that make it difficult to work with. My goal always when designing a logo is to design a logo that's so easy to use that you don't have to be an extremely skilled designer to design well with it. That's my number one goal, because I know not everybody is going to be at a stage where they have an internal brand team or a designer that's a rock star designer that can work with really complicated assets and make them look good.

(00:42:28):
I just want the assets to teach you themselves, by just how they exist, how to use it. You, as a person that has any taste whatsoever, and hopefully people that you're hiring for any job at your company have some degree of taste. If you hire an engineer, they have to have taste about how that happens. If you hire a marketing person, they have to have taste about how that happens. They should be able to look at that and intuit most of the way that you should be able to use it without being explicitly told, "Do not do this."

Lenny Rachitsky (00:43:01):
This episode is brought to you by Merge. Product leaders, yes, like you, cringe when they hear the word integration. They're not fun for you to scope, build, launch, or maintain, and integrations probably aren't what led you to product work in the first place. Lucky for you, the folks at Merge are obsessed with integrations. Their single API helps SaaS companies launch over 200 product integrations in weeks, not quarters. Think of Merge like Plaid, but for everything B2B SaaS. Organizations like Ramp, Dorado, and Electric use Merge to access their customer's accounting data to reconcile bill payments, file storage data to create searchable databases and their product, or HRAS data to auto provision and deprovision access for the customer's employees. Yes, if you need AI-ready data for your SaaS product, then Merge is the fastest way to get it. Want to solve your organization's integration dilemma once and for all, book and attend a meeting at merge.dev/lenny and receive a $50 Amazon gift card. That's merge.dev/lenny.

(00:44:08):
This is a good circle back to something you touched on earlier that I wanted to come back to, which is a lot of companies, in your opinion, put too much weight on the power of brand, and rebranding, and how much a brand can fix their problems. Can you just again share your perspective on just how this might be an issue for people where they almost overemphasize the power of brand? [inaudible 00:44:32]

Jessica Hische (00:44:31):
Yeah. Well, there's different companies. There's companies where brand is literally everything, where they're doing something that's not crazy innovative in the first place and the brand is the thing that is the whole company, and that's fine. That's a completely valid way to do stuff. It's like people that can take in information and recommunicate it in a way to another audience or whatever that they hit that audience in a way that the original information couldn't do. Think about all of the people who write books on psychology, and medicine, and all kinds of stuff that write it for a broader popular audience. They're the ones reading the medical papers, they're the ones digesting all of this really huge complicated data, and turning it into something that normal people can read. I think that there are companies that are doing that. They're taking a thing that is not innovative or isn't like they're not the only ones doing it, but they're repackaging it in a way that takes that and makes it so accessible to so many people.

(00:45:34):
In that case, brand can matter immensely, where the brand really is the thing that shows people that the thing that you're doing has value. But for a lot of people, the brand should be somewhat invisible so that the thing itself becomes the star. If we think about the experiences that we have using products, sometimes there's products where there's a ton of fun and delight built into how you use it, and that can happen through brand through design choices and things like that. Sometimes the delight is the fact that nothing is getting in your way as you're using that product. You just figure out what's your ethos of your company. Is the whole thing about doing a thing well, doing it simply, and making sure that everything gets out of the way of that experience, or is it like we're trying to generate this delightful thing or we're trying to open it up to a new audience or whatever? Depending on whatever that goal is, brand can have a different place in that equation.

Lenny Rachitsky (00:46:42):
I think some of this unique perspective on the power brand and the need for it in tech companies comes from you're not like a tech person. You work with tech people, you work with tech companies. Do you feel like that has an impact on the way you think and the value that you bring to companies to give them this very outside perspective?

Jessica Hische (00:47:01):
I mean, most of the folks that I know that work in brand or traditional graphic design, print design, they don't necessarily have a lot of insight into how building companies works. They're not friends with a ton of startup founders and things like that. I've just had been in this very fun position being like everyone's token creative person in The Bay for a while, and this been for a long time. I remember speaking at a Silicon Valley event that was women in Silicon Valley in 2009 or 2010, something like that. It's interesting to be a person that has never actually, themselves, worked at a tech company but felt so involved and understanding about how all of that works. My partner, the reason why we're in The Bay is because he got hired by Facebook back in 2011. We were olds at the time. Take that with a grain of salt, don't judge me.

(00:48:02):
Because we were 28, 29 coming over to work at Facebook and everybody there was 23, 24, and so all the people that we ended up meeting in The Bay were more people our age that were moving on to start companies and things like that. We just got to see that perspective so clearly of what it is to branch out on your own, to fundraise, to do all this stuff, to pivot, to do little experimental apps and see where that goes to get acquired as a team versus getting acquired as a technology or whatever. I've been able to see that in a way that I feel like a lot of people that do my job don't get to see. That makes me very sympathetic and towards what it is to want to build a brand as a founder. I understand that you have limited resources and those resources aren't necessarily going to get devoted to doing a $200,000 brand exploration, because when you have $500,000 of money at all for a year to try to get things going, you certainly should not spend half of that money on branding.

(00:49:12):
That's my opinion, but that's not to say that brand can't be important and can't come in at some level or can't be thought of as a partnership between you and someone else where... There's this whole idea of fractional leadership now, which I feel like hasn't really infiltrated my world as much, but I don't know why it hasn't, because most people don't have internal comms teams, internal brand teams, until the company is very mature. The idea that you could bring someone in who is a real expert in whatever it's that they do, just as you need them, and they just get consultant equity kind of thing, that should be more present because people don't necessarily need to have internal brand teams for the first six months to a year of when they're doing stuff unless they grow really significantly.

Lenny Rachitsky (00:50:05):
Along these lines, you have a pretty unique way of pricing your work. For people that may want to explore this with you, share whatever you can about just how you think about pricing and ideally even an order magnitude of pricing so they're like, "Oh, okay. We should actually do this."

Jessica Hische (00:50:21):
To go back to the process how it's always about figuring out what people are trying to accomplish, so a lot of my process scales depending on how broad of an exploration that we're doing. The way that I treat it is I treat my branding work not dissimilarly to how I treat commercial lettering, which is atypical. Brand people, what they typically do is because the client has to own the assets outright at the end no matter what, they tend to do is bill everything that the client owns everything as you are moving along. It's all sort of work for hire, but the idea being that it's a buyout of everything that is being made. What that means is that when a branding agency is pricing stuff for you, they're taking the buyout rights and baking it into every round of work so that every round gets more expensive because you are owning all of the work.

(00:51:14):
What I do is I treat it much more like a commercial lettering project where I say, "You have to own the rights to this eventually, but hey, let's break that out and let's keep the creative process lighter and less expensive so then we have more room to explore. So then if some stakeholder comes in last minute and blows everything else up and we need to start over, you haven't already paid to own everything that we've created, you just pay to own the thing that we create in the end that gets chosen." I really like the idea of keeping the creation process more flexible and to try to scale to what people need versus having a really rigid way of approaching everything. Sometimes people will bring me on really early in the process, where if they have an internal team or if they're working with an external agency or something, they want me there from the get out.

(00:52:04):
Some people are like, "We have no money and we are going to try to do this as much as we can inside of our business, but then can we hire you at the end to make it look good? If we can get everybody bought in and get it 80% of the way there." Depending on what people's budgets are, I have different ways of working, or just depending on what their needs are. Because my whole thing too is I don't want to step on anybody's toes, because sometimes companies have these really amazing designers that are working in-house, and it sucks as a designer who started at a company and thinks that you might be able to get a chance to work on what is considered the most important asset in terms of the brand and they just farm it out to someone else instead of letting you touch it. To me, that's a recipe for anything that I create to be immediately killed, because someone inside is going to be like, "It's time to shine," and then they're just going to kill all my work.

(00:52:56):
I'm always like, "How do we collaborate? How do I make it so that I'm an asset to you? Not that I'm trying to step on your toes, not that I'm trying to take over what's the cool juicy work from the people who inside that are really excited to do it." I just want them to feel as bought in as I can be. But yeah, it becomes interesting. I feel like I get told by branding people that I'm too inexpensive because they're like, "Oh, for what you do, it should be 60 or $70,000 at a minimum to do all this kind of stuff." I'm like, I feel like the majority of the projects that I do end up being between 25 and 35, but depending on how you bring me in, it can be less if it's just as a consultant. It's not out of the realm of possibilities to hire a proper crazy expert at stuff. It's not like you're thinking about sinking half a million dollars into the brand. That's a very different experience.

Lenny Rachitsky (00:53:51):
Awesome. Thank you for sharing that. Before we get into other stuff that you do, because like you said at the top, there are many other things you do outside of this specific time of work, is there anything else you think might be helpful or important for people to know about working with you on a logo refresh or just thinking about logos and this whole space we talked about?

Jessica Hische (00:54:11):
The best thing is just seeing what's there and really being able to understand what's not working about it and what your goals are. Like I said with that reverse justification of intuition, I think if you know that the logo is not quite where you want it to be, just spend a couple days asking yourself why. "What is the thing about this that bothers me?" Don't get specific. Don't be like, "The way the R is," or whatever. Maybe that's the thing that we talk about down the line, but always think big picture before you think minutiae, because sometimes people think that... They'll throw a bunch of minutiae stuff at me, but it's because they haven't really stopped to think about what is the overall thing that's bothering them.

(00:54:59):
You just never get there if you're always trying to address detail before you address the big picture stuff. You have to just always start super top level, and really ask yourself very broad questions about why you think it's not working, and then go tighter, and tighter, and tighter, and be like... It really could be like, "This C has always bothered me," and then we can get real specific about that when I'm doing the refresh. But I think I also need to understand the overall reason why we're doing this, not just the little bugaboo that bothers you specifically but might not bother anybody else.

Lenny Rachitsky (00:55:35):
Yeah. When I was thinking about this, it feels like my whole feeling was this could be better. That's all it was for me initially, is just like I feel like it could be a lot better. I imagine that's enough for some people, just like, "I think this could be better," and then here's things [inaudible 00:55:50].

Jessica Hische (00:55:49):
Yeah. That's definitely enough for some people, because I think sometimes... I think you, specifically, had a very clear vibe going on with all of your brand stuff. Some people, it's totally like a mishmash grab bag of random trends and there's no real voice that's coming through. But I feel like you've been doing this for a while, and when you see everything together, there's definitely a very clear vision and vibe that you get from everything. I always tell people that having terrible vision can be your best asset when it comes to logo and brand, because it allows you...

(00:56:30):
Just take your glasses off if you have terrible vision, and look at it, and get the feeling when you can't see the detail. You have to be looking at it with blurred eyes. What is the overall look of this thing? Just trying to get as broad and noodly as you can with it instead of it being about those really specific one by one stuff. I think when you blur your eyes on your brand, there was a really clear cohesiveness to it already. It was just about massaging it into a more consistent professional-looking place.

Lenny Rachitsky (00:57:07):
Well, I can't look at the old logo anymore now that I've gotten this thing coming together, so I'm really excited for this to come out. Just a couple closing questions. One is you have a lot of other stuff going on that is not just typography and logo refreshes. You have children's books, you do lettering for classics, you have a store in Oakland. Talk about all these other things you got going on in case it might interest people.

Jessica Hische (00:57:32):
Sure. Yeah, of course. I'm based here in The Bay, as we've talked about, and I have a studio in Downtown Oakland. My studio is like Barbie's creative Dream house, where the top floor is my office, that's where I'm right now, and then the bottom floor on one side is a workshop. I do a lot of printmaking. I went to college at a school that was very focused on interdisciplinary work, and I feel like I bring a lot of manual analog processes into my work a lot. I find it really important to make physical things as a part of my creative process, so I do a lot of printmaking. On the other side downstairs is a brick and mortar store. I've always wanted to have a brick and mortar store because, as an artist, I think having people have a physical connection to your work can be really important.

(00:58:20):
I think one of the reasons why people hire me to do things for them is because one of the gifts of working with someone that is a real nerd professional about whatever it is that they do is that they bring you along the journey and give you the language to talk about the thing through their eyes and through their experience. To me, the funnest thing for me is actually telling clients and teaching them about all the things that we are doing along the way so then they go out into the world as a newly-minted type nerd and can communicate all of these things to other people. That connection is just really important. The connection to the work, the story behind it, I feel like that's one of the ways that we create lasting work, is understanding that the work exists because there's a story behind it. Things, if they're just created because of the aesthetics or they're just created quickly or whatever, it's really easy to discard them because there's not a story behind it.

_[153 additional lines trimmed for context budget]_

---

### Lessons from one of the world’s top executive recruiters | Lauren Ipsen (Daversa Partners, GC)
**Guest:** Lauren Ipsen | **Date:** 2022-11-03 | [YouTube](https://www.youtube.com/watch?v=v3pofqabzhs)  

# Lessons from one of the world’s top executive recruiters | Lauren Ipsen (Daversa Partners, GC)

## Transcript

Lauren Ipsen (00:00:00):
Regardless of whether or not you're hiring, you should always be keeping a pulse on the market. That is the most important thing. And I think that should be the case for both candidates and folks that are hiring. Like, you never want to put yourself in a position where you have no idea what good looks like, whether that's from a company standpoint or from a candidate standpoint. So, both parties should always be having a good understanding of which companies are thriving, which individuals are building great things and are well known commodities in their organizations and get great references.

Lauren Ipsen (00:00:35):
Oftentimes, I encourage founders to simply chat with what good looks like and get a really good sense of what benchmark candidate profiles could be, and who knows where that person will be in a year or what have you, but staying really, really close to really great people and using them from an advising capacity or getting them ingrained in some type of involvement in the product prior to actually having that specific need, I think, is really important.

Lenny (00:01:05):
Welcome to Lenny's Podcast. I'm Lenny and my goal here is to help you get better at the craft of building and growing products. Today my guest is Lauren Ipsen.

Lenny (00:01:15):
One of the most important skills for founders and senior product leaders to develop is the ability to hire great people. You won't be able to build the best company or the best product if you can't hire the best people. And Lauren is one of the most experienced and successful people in the world when it comes to hiring product leaders. She's placed over 80 senior product leaders across tech companies and has worked with some of the biggest companies out there. When I asked a bunch of really smart product leaders who their favorite product recruiter was, Lauren's name came up a ton.

Lenny (00:01:46):
In our conversation, we get super tactical about what founders need to do to find the best product talent, what product managers should be doing during their career to give themselves the most opportunity, and we also touch on what recruiters themselves often get wrong when trying to attract great talent. This episode is rich with actionable advice for basically everyone, and I am really excited to bring it to you. With that, I bring you Lauren Ipsen.

Lenny (00:02:12):
Who has an opinion on internal tools? Internal tools are something you probably don't think about until you have to, or it probably didn't even occur to you to think about them, but if you work at a big company, you probably have a bunch of one-off custom apps or dashboards that are laser focused on just one job to be done for one specific team or just one role, and they're always such a huge pain to build and maintain. And that's why I'm such a big fan of Retool, and why I think Retool is so popular. Retool allows teams as small as just one person to build a suite of custom internal apps in a fraction of the time that you think it takes. The productivity gains of custom apps is now within reach, not just for large enterprises, but for small teams as well. And as you scale your company, Retool scales with you.

Lenny (00:02:57):
Snowflake saves about 26 hours a week of manual spreadsheet work with custom internal apps built on Retool. Amazon uses Retool to handle GDPR requests. Thousands of teams at companies like Coinbase, DoorDash, and NBC collaborate around custom-built Retool apps to operate with greater efficiency.

Lenny (00:03:17):
Maybe you've thought about using Retool before but just haven't, and I'm here to tell you that now teams of up to five can build unlimited Retool apps for free. Get started today at retool.com/lenny.

Lenny (00:03:31):
Today's episode is brought to you by Miro. Creating a product, especially one that your users can't live without, is damn hard, but it's made easier by working closely with your colleagues to capture ideas, get feedback, and being able to iterate quickly. That's where Miro comes in. Miro is an online visual whiteboard that's designed specifically for teams like yours.

Lenny (00:03:53):
I actually used Miro to come up with a plan for this very ad. With Miro, you can build out your product strategy by brainstorming sticky notes, comments, live reactions, voting tools, even a timer to keep your team on track. You can also bring your whole distributed team together around wire frames where anyone can draw their own ideas with the pen tool or put their own images or mock-ups right into the Miro board. And with one of Miro's ready made templates, you can go from discovering research to product roadmaps to customer journey flows to final mocks.

Lenny (00:04:24):
Want to see how I use Miro? Head on over to my Miro board at miro.com/lenny to see my most popular podcast episodes, my favorite Miro templates. You can also leave feedback on this podcast episode and more. That's M-I-R-O.com/lenny.

Lenny (00:04:45):
Lauren, thank you for being here. Welcome to the podcast.

Lauren Ipsen (00:04:47):
Thanks so much. It's great to be here.

Lenny (00:04:50):
I've been meeting to do an episode on product hiring and recruiting product people for a while, and when I asked a bunch of smart friends who should I have on to talk about this stuff, your name came up a bunch, and so I'm really happy that we're finally doing this.

Lauren Ipsen (00:05:03):
Me too. Absolutely. I'm grateful that you asked.

Lenny (00:05:07):
So, to help listeners get a sense of just your background and kind of the journey you've been on to get to where you are now, can you just spend maybe just a minute kind of talking through the wonderful things you've done in your career and what you're doing now?

Lauren Ipsen (00:05:19):
Yeah, absolutely. So I started my career in broadcast. I originally thought that I wanted to be on the news and quickly realized I didn't necessarily want to be the face of sadness. There's a ton of that happening in the world.

Lauren Ipsen (00:05:36):
So, made a pivot pretty early on and thought, where could I use the communication skills that I've been working so hard on and do something that's impactful in a big way, but maybe just with a little bit of a different angle? And stumbled across executive search.

Lauren Ipsen (00:05:52):
Exec search is not really something that people major in college by any means or think they're going to end up doing. So it was something I found fascinating. I had applied to all of these different companies like Twitter and Snap and Pinterest, hadn't heard back from any of them, was a name in the resumes, and thought, well, how cool would it be if I could work with all of them and have an opportunity to play a part from a different lens?

Lauren Ipsen (00:06:18):
So, got into exec search, was really focused early on on consumer mobile build outs. Was doing a lot of work with Twitter and Reddit and TaskRabbit and Nextdoor, Postmates, you name it, and that was probably 85% of the work I was doing. And then as, on the agency side, 15% was obscure, autonomous helicopters selling into the Department of Defense and then retail and robotics, and it was all very fascinating, but for me, who has wicked bad ADD, it was amazing to be able to be so stimulated by so many different industries and feel like I couldn't really master this.

Lauren Ipsen (00:06:57):
So, long story short, had an awesome career at Daversa Partners, which is a boutique executive search firm, and thought I was going to be there potentially forever. Was tapped by a awesome individual, Abe Shafi, who was founding a company. I had been doing a lot of work for them. They were a client of mine at the time. Placed a couple great hires and they said, "We're either going to kick off a head of talent search with you or you can come over and join us." So I was the first recruiting hire over there, built out the talent function in its entirety.

Lauren Ipsen (00:07:27):
I definitely think there was a part of me that I loved the operating experience, learned a ton, worked side by side some amazing people, but was really missing working with founders, and lots of them, and keeping a pulse on the market. So General Catalyst tapped me most recently, and been working here for the past couple months, and it's been great thus far, and I'm specializing in our consumer and crypto investments.

Lenny (00:07:51):
Awesome. The fact that you're at GC now makes me think about another recruiter that I know who's awesome, Austin Brizendine, and it's interesting that a lot of the best recruiters seem to be heading to VC funds, and I'm curious why that is happening. Is it like a comp thing? Is it other things that are pulling everyone away into funds?

Lauren Ipsen (00:08:09):
It's a great question. It's definitely not a comp thing. I'll say that. I think it could be a stability of life thing. Search is incredibly volatile, and you have to hustle so hard, and as soon as you have three wins, you've got four more things to execute on. And so, there's aspects of it that can be tough, especially in a market like right now where you really do have to chase business and you can't be selective about what you take on, so you could be pitching things that maybe you don't necessarily believe in in its entirety or what have you. So, I think that's one component.

Lauren Ipsen (00:08:39):
In house is obviously difficult right now as well for talent leaders. It's really scary to take a bet on one single company right now and know what it's going to look like six months from now. And so I think those things combined might be the reason for an influx in folks leaning more towards venture, and I think it's just timeliness.

Lenny (00:08:59):
Got it. That makes sense. The downside is it's hard for people to find awesome recruiters because once you're in a fund, you're just going to help those startups. And so we're going to talk later about just how do people find awesome recruiters, what do you look for? But there's roughly three things I want to spend our time chatting through today. One is for founders and hiring managers, just how to find the best talent and what they could do to be successful finding the best talent. Two is for product leaders and PMs, how to give themselves the most opportunity from the flip side. And then third is just for recruiters, what do they often do wrong? How do they miss out on the best product talent? Does that sound good?

Lauren Ipsen (00:09:37):
Yeah, that sounds great.

Lenny (00:09:39):
Okay, cool. So on the hiring front, just diving in, say you are a founder, you're someone really early at a company, and maybe you've got a couple PMs and you're starting to think about we need to hire a really senior product leader or first senior product leader.

Lauren Ipsen (00:09:52):
Sure.

Lenny (00:09:53):
What do you find is often the biggest mistake that founders make when they're trying to hire their first senior product leader?

Lauren Ipsen (00:09:59):
I think especially for founders that haven't hired for this caliber of talent in the past, it's really easy to be distracted by shiny objects and look at huge names. You want to find the CPOs of Google and YouTube or what have you because that seems like it would be such an incredible opportunity for brand recognition. And to an extent, it is. But the fact of the matter is, oftentimes those individuals are pretty far from the work and have a great team of executors that they've put into place that are actually the ones that are in the weeds.

Lauren Ipsen (00:10:33):
And so I think that's the biggest mistake I see people make, especially on the hiring front where they have limited resources, and maybe they're an early stage company and trying so hard to bring in big names is not always the best way to go about it because the fact of the matter is they need to go then hire a team. So, I think looking for someone that's a little closer to the work, maybe someone that can step up into that type of role and do so in a way where some days they might actually be operating like a PM and then other days they might be able to build from a leadership perspective, that's more of the DNA that people should be targeting.

Lenny (00:11:09):
Do you think the source of the issue with that going wrong, that they no longer can do that work as well because they've been shielded away from the tactical day to day? Or is it that they're not as hungry as they used to be and they're just like, "I already, I'm a YouTube 10 year super success, I don't need to prove myself anymore," and they're just not as hungry, or something else?

Lauren Ipsen (00:11:29):
I mean, I'm not going to sit here and say that all senior leaders aren't hungry. I think that there's some folks that really lean into the work in a different way and miss that, and often go to startups because they crave building.

Lauren Ipsen (00:11:41):
So it's not necessarily that, but I do tend to lean towards folks that have a chip on their shoulder or have something to prove and want to build a name for themselves. All of that to say, there's a reason that a lot of those people got to where they are, and some of the best talent are some of the senior folks, but just maybe not necessarily the best talent for where this company is today. Right? It could be great for 10 years down the road, but the past five years of that individual's career could have been far more focused on camaraderie, team building, operational components, performance reviews, and then aspects of product vision, which just might not be the innovative AB testing type of profile that you typically look for in these pre-IPO companies.

Lenny (00:12:27):
Got it. I imagine there are stories that you can share about people you've placed like that, that have not worked out. If you can share one, that'd be awesome. But maybe a side question is the general advice, just don't assume someone that's been successful at a big company with a fancy background is going to be great. Sometimes they work out, but not always, is that kind of the takeaway?

Lauren Ipsen (00:12:46):
Yeah. I would say the general advice is who is going to be best for this specific role at this specific time, not necessarily who is the best talent in the world or in the market. Those are two very, very different questions to ask. And I think early on in my recruiting career, I was often just trying to recruit these whales of executives to try and prove myself and say, "Oh, I got this person to entertain this opportunity, how sick is that?" But naturally, that's not necessarily the person that actually can move the needle. And so you need to think very specifically. Just because this is the best talent, that doesn't necessarily mean they are the best talent for this role today.

Lenny (00:13:25):
So to double down on that, say you are hiring and you know you're going to start hiring a senior product leader, what is it that you suggest founders nail down and iron out when they're kicking off the hiring process, either on their own or with a recruiter? Like, in the job description, what else do they have to get right to find the right person?

Lauren Ipsen (00:13:48):
It's a number of things, like product leaders can come in a lot of different flavors, and so I think it's trying to determine where this person should major and minor, where they should spike. Is this someone that's going to really lean into the design efforts? Is it someone that actually kind of needs to just operate like a very senior PM and continue to build out a team? Is this someone that really should be focused on product vision for the long haul? And then thinking more holistically about how to build the rest of the team.

Lauren Ipsen (00:14:12):
There's so many different ways in which you can hire for a product leader. So I think it's trying to work a little bit backwards and think about, what is the actual outcome that we are trying to solve for with this hire? Or are we just hiring a head of product because we feel like we need to hire a head of product? That's so often what I see is the board's telling me we need to hire a head of product and I don't necessarily think that we do, or I'm not exactly sure what we need in this role. And so whenever you're starting a search in that regard, it's kind of doomed from inception, so you need to get incredibly granular on the front end around, what is this person going to be coming in to do? What's their mandate? And if we think about someone that's just absolutely hitting it out of the park and crushing it, what does that look like? So I think just trying to be really specific on the front end.

Lenny (00:14:59):
I love that last piece, just what does success really look like for this person? On the first piece, that's exactly the same advice I give founders when they're looking for a PM is like, what do you concretely need them to do day to day? Not like, "We need someone to help us with product." And that often helps illuminate, okay, I see. We need someone to help us ship more consistently. We need someone to help us hire engineers. Yeah, just make a list. What are they going to do in the first month or two or three?

Lauren Ipsen (00:15:24):
Yeah, yeah, absolutely. I think those are the most important things. Like the 90 day plan is something that's overused but so necessary, and that's just the tip of the iceberg. So that component and then, okay, a year from now, what should this person be doing? Two years from now? Do we want them to grow up into a CPO role? Do we think about that in a different way? How are we thinking about the product direction today, 12 months from now, 18 months from now, through IPO? I think it's really difficult to think about things that way, and so often you're thinking about the task in front of you and just trying to iterate quickly, but that is the type of thought process that needs to be happening from the CEOs and founders.

Lenny (00:16:06):
Are there archetypes of PMs, if you just bucket like here's the three maybe most common types of product leaders that founders hire, because there's an infinite list of skills and things they could do, but just to make it even simpler, like here's probably one of these three you're looking for. Do you have something like that in your head?

Lauren Ipsen (00:16:25):
Yeah, there's platform product leaders or folks that are kind of more indexed on the infrastructure components. There's folks that are typically focused on core product, or consumer product if it's on the consumer side of things. And then you'll have folks that are really indexed and that can include UX individuals, design folks. And then there's also typically specialists, so individuals that are really hyper-focused on growth or monetization or what have you. Those are the three buckets that I would say I see most often.

Lenny (00:16:57):
Do you feel like founders sometimes pick one of the wrong buckets and that's a common mistake, or is it generally it's the wrong bucket but then maybe it's not the right spikes of skills within that bucket?

Lauren Ipsen (00:17:08):
Well, it's kind of twofold. I think sometimes people just bring in a head of product to do everything, so that's probably not the best way to go about things. I think that ends up being a unicorn, which you hear often in the search world, and it becomes really difficult to hone in on what good looks like.

Lauren Ipsen (00:17:25):
And so I think again, it just comes back to having a clear org chart on the front end and determining, are we hiring someone specifically to build out our walled garden ads approach, or are we hiring someone to run product marketing, or are we hiring someone to help from a product perspective to build a better core user experience? Those are all very, very, very different roles, and if you bring in one person to try and do it all, the facts of the matter is, they're going to have to bring in some key lieutenants to help them, so is that something you want to do or are you more focused on bringing in someone imminently to help on the ad side of things, and then we can find that head of product to help them out down the road?

Lauren Ipsen (00:18:05):
That's kind of the way in which I would architect it and think about it, is what's most imminent, and what do you actually need to hire for today, as opposed to just hiring for the sake of hiring and bringing in that leader.

Lenny (00:18:17):
And part of the discussion there is maybe they grow up into this head of product long term, maybe not. Maybe we just need someone to ship the ads platform, right?

Lauren Ipsen (00:18:24):
That's exactly right. Yeah.

Lenny (00:18:26):
You mentioned this title of head of product. There's also VP of product, CPO. There's all these titles and I feel like people sometimes use them interchangeably, don't know which one to use when they're putting out a job description. Do you have any kind of heuristic rule of thumb of just, here's how to think about when to use each of these titles? Or is it not even a big deal for, say, a founder, hiring the first senior product leader?

Lauren Ipsen (00:18:48):
It's a great question and it definitely leads to confusion across the board because I'll have candidates come to me and they'll say, "I'm only looking for a CPO role," when I'm working with a startup where, maybe on the venture side of things, we've actually advised that startup to not hire C-level executives at this point. And so that naturally could eliminate a candidate that could be amazing for the role.

Lauren Ipsen (00:19:15):
Similarly, people could feel that way if it's a VP of product role, but that in their mind is the most senior product leader within the organization running everything from end to end who they intend to be the CPO down the road but are not in a position where they're ready to hire C-level execs. So, it's tough, and it really depends on the organization and the way in which they're thinking about org charts and leveling.

Lauren Ipsen (00:19:39):
A lot of startups at this point are almost allergic to C titles or VP titles or are just more title agnostic than I've seen in the past, so you see a lot more of these head ofs, and that is sector leaders, up until probably C plus or D stage. And then you get to D and E, and you'll see more of the VP, director, CPO type. And I think that is the way that people should be thinking about it is, if I'm joining a company very, very early days and it's called head of engineering, that is intended to be the most senior engineering leader within the organization.

Lauren Ipsen (00:20:19):
The fact of the matter though is there's good reason sometimes why they're not throwing out that C title, and could this person be layered down the road? Potentially, because maybe the talent they need right now is different than, just as we had spoken to earlier, is different than the talent that they might need from a massive CPO in two and a half years.

Lauren Ipsen (00:20:37):
And so I think that rubs people the wrong way because they want a bit of a promise that things are going to, if they join at this stage, that they'll be in it for the long haul and be that chief product officer that takes a company through an IPO. But companies are so dynamic and things change so quickly.

Lauren Ipsen (00:20:52):
So, I guess a long-winded way of me saying there's different breadth and depth to each role, but I think for the most part, unless it's a very siloed company with multiple different VPs, if you're coming into a company and they say this is the head of product, the VP of product, or the CPO, that all means the same thing dependent on the stage, for the most part.

Lenny (00:21:14):
And the other takeaway there, which is awesome, is if you're early stage startup, probably just start with head of product. Keep it simple.

Lauren Ipsen (00:21:19):
Yeah.

Lenny (00:21:19):
Don't over promise. Everyone understands.

Lauren Ipsen (00:21:22):
Yeah. Because the last thing you want to do is have to demote someone. But once you get to a place of having C-level executives, like that's not going to do any good, but maybe you start as a head of product, and then as the company continues to grow, you lean into the growth side of things more, and so you become that head of growth or an SVP of growth. Like things iterate and change, but you just never know and you can't predict the outcome of a company on the front end, so.

Lenny (00:21:49):
A lot of people listening to this probably aren't hiring senior product leaders right now, but plan to and will in the future, and so I wanted to ask, what should founders do when they know they will hire a head of product, say, in the next year, that could set them up for success down the road? What should they be doing ahead of time?

Lauren Ipsen (00:22:07):
Regardless of whether or not you're hiring, you should always be keeping a pulse on the market. That is the most important thing. And I think that should be the case for both candidates and folks that are hiring. Like, you never want to put yourself in a position where you have no idea what good looks like, whether that's from a company standpoint or from a candidate standpoint. So, both parties should always be having a good understanding of which companies are thriving, which individuals are building great things and are well known commodities in their organizations and get great references.

Lauren Ipsen (00:22:43):
Oftentimes, I encourage founders to simply chat with what good looks like and get a really good sense of what benchmark candidate profiles could be, and who knows where that person will be in a year or what have you, but staying really, really close to really great people and using them from an advising capacity or getting them ingrained in some type of involvement in the product prior to actually having that specific need, I think, is really important.

Lenny (00:23:11):
That sounds awesome and it makes sense. How do you, as a founder, do some of that? Do you just ask folks like, "Hey, who are some of the best product leaders you know? I just want to chat with them. I'm not hiring, just want to kind of meet people who are awesome." Is that the behavior you suggest or is there something else you can do?

Lauren Ipsen (00:23:27):
That's a great way to go about it. And by simply saying, "No agenda, I'm not trying to hire you tomorrow, I just want to know great people." And to be totally honest, people feel flattered by that, typically. Most of the time if you've been referred to someone and heard nothing but great things about them and you really don't have an agenda other than wanting to pick their brain, people are like, "Huh, well, this is different from the day to day. This is fulfilling," and people want to pour their knowledge, especially into companies that they believe in.

Lauren Ipsen (00:23:58):
So, I think more often than not you'll find that, not just products leaders, but executives across the board are actually really inclined to do so and want to help out because it's a little bit different from the day to day monotony of their work life.

Lenny (00:24:12):
So, the advice is keep track of companies who are killing it, who you might be able to kind of poach from in the future, and keep a list and keep warm contact with folks that are awesome.

Lauren Ipsen (00:24:22):
Yeah.

Lenny (00:24:23):
It reminds me of a founder that ... or basically all founders who are really good at hiring and how far ahead they plant seeds and how they just play the long game with the best people they meet, and they just kind of keep the conversation going until they finally convince them to join a year or two later. Do you find that the same thing?

Lauren Ipsen (00:24:41):
Yeah, 1000000%. Yeah. When I was on the executive recruiting side of things at Daversa, the VP of engineering candidate that we ultimately landed, it was a seven month game of courtship, and let's bring him in to help out from an advising capacity, let's ask him how he would think about structuring this organization, let's talk to him about the best talent that he would recommend that we're spending time with. No question that's invasive, but more so just collaborative and exciting, and you'll find that the founder and that leader will build a different level of rapport and trust by not going through a formal interview process and having it feel transactional. And then with that, magic can happen and you can land incredible people.

Lenny (00:25:24):
This all sounds like a lot of work and a lot of time. Do you have guidance on how long it should take to find, like for a early stage startup, say series A or B, to find someone awesome, and/or how much time founders should spend a week, just best practice, on hiring for someone like this?

Lauren Ipsen (00:25:41):
I mean, I think if you're going to search, so if you're looking for this individual, it's really so case to case. There's searches that I've been in that I call three people. I know they're amazing for this. I tell the founder and CEO, "These are the three people you should chat with, hire one of them." And it's that easy.

Lauren Ipsen (00:25:59):
There's others where it's a lot of trying to figure out what the person's actually looking for. If there's some ... they hit it off from an emotional standpoint. There's so many different things that come with it. I would say from a timing perspective, it's not a hard number. It's more of just put yourself in the room with great people. If you have a tremendous amount of respect for someone, continue to harvest that relationship and ask what good looks like. Find excuses to continue to touch base with people that are important in your network. If you remembered that they'd mentioned that they were going to some event and you think that you might want to hire them down the road, in a non-creepy way, show up to that event. These are things that I'm constantly doing, and I think that founders can do a better job of, but just make yourself known and relevant. And then when you reach out and timing is right, it won't feel so obscure or so transactional.

Lenny (00:26:55):
That reminds me of a time at Airbnb where we had these meetups for engineers every month or so where it was a tech talk, and then all the engineers get a target engineer that is coming to the event. Like they get their profile and their picture, and their job is to make sure they have a great time and try to convince them to join someday.

Lauren Ipsen (00:27:14):
Oh my god. But it's so real. It's similar to college trips where you're trying to get recruited for a sport, and you have to ensure that you're, yeah, just continuing to give people the best experience possible and staying top of mind for people.

Lenny (00:27:28):
Yeah. But I can see it being creepy, They have no idea there's this person assigned to them, but it works. It worked great. It was a great tactic.

Lauren Ipsen (00:27:35):
That's awesome.

Lenny (00:27:36):
I want to come back to a question that I asked, but I feel like you'll have an actual more concrete answer to the specific piece of just, what's your guidance per week how much time you should be spending on hiring broadly, and specifically, heads of product, if that's any different. Do you have any just advice? Because I imagine it's always spend more time than you think. It's going to take a lot of time.

Lauren Ipsen (00:27:54):
Yeah, it's definitely spend more time than you think. If you are in an actual search, then you should devote all of your time to it. I know that sucks to hear, but you should be really carving out concrete time.

Lauren Ipsen (00:28:05):
The thing that's tough though is you could spend, I guess what I was trying to say is you could spend one hour on something or you could spend 10 hours on it, but it's more so around, are you doing things to be impactful during that period of time? Are you actually doing things that are going to move the needle? Are you just blindly reaching out to people on LinkedIn? Because that's not going to be the way in which you're going to find the best of the best. Some of the greatest talent, they're not even on LinkedIn.

Lauren Ipsen (00:28:32):
And so I think it's building a really strong network in advance, and then once you actually get to a place where you need to hire that person, calling all of those amazing people that you've built relationships with and saying, "Now tell me who your favorite person is and who the best person you've ever worked with is, and could you put me on a thread with them?" How are you going to differentiate yourself from the rest of the market? So it's less in my mind like a quantitative number of hours and more of, how are you doing things differently than the rest of the market?

Lenny (00:29:01):
I want to pull on this thread. Okay, so spending one hour versus 10 hours, and your point about how you could spend one hour and get as much done maybe in those 10 hours. What sorts of behaviors and actions should folks take to make use of, say, that one hour in hiring? How do you not waste your time?

Lauren Ipsen (00:29:18):
I mean, I would say I've got probably five of my all time favorite product leaders in the world that I tap whenever I'm kicking off a search. And they know that whenever life brings them to an opportunity where they are going to start looking or want to lean into board opportunities, that I'm going to set them up and shout their name from the rooftop, so oftentimes they're willing to point me in the right direction of great people, make those introductions, what have you, and I'm going to know simply because of how great they are that they would never put me in touch with someone that wasn't equally as qualified.

Lauren Ipsen (00:29:55):
So that I think because the quality is there, so I'm not just blindly guessing on quantity, spending a ton of time on LinkedIn, and then having to call unknown entities and ask for back channel references when they also might not even feel comfortable sharing the dirt. You know? So it comes back to rapport and people that you have around you that you know you can trust and tap into and ensuring that you're spending the time in the right areas.

Lenny (00:30:19):
Got it. Yeah, so tapping your network makes a lot of sense. If you don't have that yet, I guess is it worth spending time on LinkedIn just cold messaging people as a founder, and any tips there for just cold outreach that you think work for a founder doing it versus you who are a professional at it?

Lauren Ipsen (00:30:37):
I do think it is worth that time. If you see someone that looks amazing, hell yeah, reach out to them, spend time with them, why not? And oftentimes, again, people are excited to see, oh, this CEO and founder wants to pick my brain, doesn't look like they're coming at me to try and recruit me, but rather just to have an open-ended conversation. For sure, and they can sense. You can definitely sense that type of interaction and feel comfortable with it, whereas sometimes the walls immediately go up when someone senses that they're trying to get poached.

Lauren Ipsen (00:31:08):
And so it's I think something that's worth them doing for sure. It's just if you are going to look for a key executive and are on a time crunch, I don't necessarily think the best use of your time is blindly reaching out to executives when you don't necessarily have the expertise in knowing which companies were thriving during that period of time, which organizations were great, and which were a little bit weaker within companies. All of those things are just the inner workings of the recruiting atmosphere and technology, and I think tapping people, if you don't have the network, talk to a great recruiter or just spend some time kind of doing some research on who's great. You can ask investors or board members in your companies as to who you should be targeting. So there's always got to be one or two people that can at least point you to another three or four.

Lenny (00:31:56):
That reminds me of a tactic Gokul shared on this podcast about one of his best tricks, as you think of, if you're hiring salespeople, instead of looking for who are just the best salespeople, you look for the company that is known for being really good at sales and then you go find people there and try to poach their lieutenant types, not maybe necessarily their head of sales. Do you think that's a good move?

Lauren Ipsen (00:32:17):
Yes and no.

Lenny (00:32:18):
Ooh.

Lauren Ipsen (00:32:19):
Yes and no, because I think if the organization is incredibly good at sales then the majority of the folks are probably amazing, but you always have weak links, and just because someone has a brand on their resume at the right time, I think oftentimes CEOs and founders will do this thing where they kind over generalize, well, Amazon's Prime team at this time was amazing or something like that. It's like, they definitely could have been, but just as any other company, there's going to be people that are breakout, top 1% type individuals, and then other individuals that get to ride the wave and reap the benefits of being at the right place at the right time. And I think that's a good starting place, but then also spending time getting a little bit deeper on who the best people are within that organization. But yeah, we always start with market mapping, so determining who the best companies are within a specific area, and then I just encourage everyone to take that a layer deeper.

Lenny (00:33:16):
Got it. It just comes back to your previous piece of advice. Don't assume someone that has an awesome logo is going to be great, but sometimes they are. There's a question I should have asked you at the beginning that I'm going to ask now. How many folks have you placed? How many companies have you worked with? And then also, is there a story of just your favorite person that you've placed/company you've helped hire that comes to mind?

Lauren Ipsen (00:33:36):
Yeah. So both great questions. I've placed probably 85 executives over the course of my career, and then lots of entry level employees when I was in house, and also some great key leaders. Yeah, probably 85 searches that I've opened and closed, so that's been incredibly fulfilling work. It's also fun too on the exec side of things because you hire the VP of engineering at Postmates and you see firsthand the product change. You know, you watch those types of things happen before your eyes, which is, it's fulfilling stuff, it's really cool.

Lenny (00:34:09):
And delivery gets there faster too.

Lauren Ipsen (00:34:10):
Yeah, yeah, exactly. So, that's really fun. Favorite placement of all time has got to be the VP of engineering that I placed at IRL, which is, to be completely honest, a big reason that I joined that company. Alex Strand is his name, and he is just the most incredible, atypical, high emotional intelligent engineering leader. Still super technical, but has managed teams in the hundreds, built out Amazon Prime Day, then went on to build the core messaging platform at Snapchat, called him and he was like, "Why would I ever leave? There's a ton of financial incentive and a team that I've built that all loves me."

Lauren Ipsen (00:34:54):
And we got him to ultimately make a move in two weeks, which is just kind of unheard of, although it ended up being like a seven month push for a start date, but that was a big reason why I went and joined IRL was to work side by side him and help build out his team and learn to get more deep on the technical side of things. Because oftentimes, on the executive level, as mentioned before, can be sometimes more of that people management type role. And so he just felt like this unique hybrid of an individual, and yeah, you close searches and you cross your fingers and hope for the best and feel very good about it, but he was one where I had all the confidence in the world that I could not have done better. You know? I just felt so great about that. So, that was a good one.

Lenny (00:35:38):
I don't know how much more confidence you could instill in a candidate joining a company than the recruiter also then joining the company.

Lauren Ipsen (00:35:44):
Yeah, I did say that to him. I kept saying, "And if I were to go in house, I swear this is probably the company I would do it for." And then about four weeks after he signed, I'd texted him and I said, "Well, you'll never guess."

_[360 additional lines trimmed for context budget]_

---

