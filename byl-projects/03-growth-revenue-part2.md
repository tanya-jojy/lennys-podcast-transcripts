# BYL Brain: Growth & Revenue â€” Part 2 of 2
_See Part 1 for topic index | Updated: 2026-02-23 01:10 UTC_

---

## TRANSCRIPTS (continued)

### The paths to power: How to grow your influence and advance your career | Jeffrey Pfeffer (Stanford)
**Guest:** Jeffrey Pfeffer | **Date:** 2024-06-13 | [YouTube](https://www.youtube.com/watch?v=etVCBDRXUH8)  

# The paths to power: How to grow your influence and advance your career | Jeffrey Pfeffer (Stanford)

## Transcript

Lenny Rachitsky (00:00:00):
We're going to be talking about how to grow your power.

Jeffrey Pfeffer (00:00:02):
The reason why you should pay attention to this is because it leads to a lot of good things, salary, getting promoted, being happy in your career, being less stressed.

Lenny Rachitsky (00:00:11):
You're not describing how the world should work. This is just how it is.

Jeffrey Pfeffer (00:00:15):
Not only is, but how it was and how it will be.

Lenny Rachitsky (00:00:20):
The Seven Rules of Power, get out of your own way, break the rules, show up in a powerful fashion, create a powerful brand, network relentlessly, use your power, and understand that once you've acquired power, what you did to get there will be forgiven, forgotten, or both.

Jeffrey Pfeffer (00:00:32):
This is not about personality. These are skills they can be mastered.

Lenny Rachitsky (00:00:35):
People might be hearing this and they're like, "I don't want to be this person."

Jeffrey Pfeffer (00:00:38):
Well, you already have done a fabulous job of illustrating principle one. That is one way to get in our own way. If I think power is dirty, the first thing that's going to happen is I'm not going to do what I need to do to be successful in my career.

Lenny Rachitsky (00:00:52):
The opening quote to your book that I have here, if you want power to be used for good, more good people need to have power.

Jeffrey Pfeffer (00:00:58):
That's exactly right.

Lenny Rachitsky (00:01:03):
Today, my guest is Jeffrey Pfeffer. Jeffrey is a Professor of Organizational Behavior at Stanford's graduate School of Business, and teaches one of the two most popular and oversubscribed courses in all of the MBA program, called the Paths to Power. The other class, by the way, is Touchy-Feely, which we dove into last month. In his class and in his recent book, the Seven Rules of Power, Jeffrey teaches the things that you can do in your life and in your work to build your power, and through that get things done and advance in your career. As one student described the class, it's the cod liver oil of the Graduate School of Business. You know it's good for you, but you feel a little nervous about it. In our conversation, we dig into each of the seven powers, why it's important to build these skills even if you feel uncomfortable.

(00:01:48):
We talk through a bunch of examples of the power in action and the impact it has had on people's lives, why it isn't as cringey or scary as you may think. I was actually nervous to have this conversation and I ended up being a huge fan of Jeffrey and the work that he does. We end the conversation with what you can start doing today to start building your own power. This podcast is basically for anyone that wants to advance in their career, whether you're an IC or a CEO, and I'm really excited to bring it to you. If you enjoy this podcast, don't forget to subscribe and follow it in your favorite podcasting app or YouTube. It's the best way to avoid missing future episodes, and it helps the podcast tremendously. With that, I bring you Jeffrey Pfeffer. Jeffrey, thank you so much for being here and welcome to the podcast.

Jeffrey Pfeffer (00:02:36):
Thank you, Lenny. I am honored that you invited me on.

Lenny Rachitsky (00:02:40):
I'm even more honored that you decided to come on. We're going to be talking about something that makes a lot of people uncomfortable. I think it's going to make me uncomfortable. We're going to be talking about how to grow your power in life and in business. Let me just start by asking why does this stuff make people uncomfortable, and why is it still important for people to learn how to do this well?

Jeffrey Pfeffer (00:03:02):
Well, it's important because a guy named Gerald Ferris developed a scale of political skill. And he and a bunch of his colleagues over the years did a lot of empirical research that demonstrates that political skill is associated with a lot of positive outcomes, salary, getting promoted, being happy in your career, being happy in your job, being less stressed. So the reason why you should pay attention to this is because it leads to a lot of good things. The reason why it makes people uncomfortable. You said it made you uncomfortable. Maybe I should ask you the question, why does it make you uncomfortable?

Lenny Rachitsky (00:03:44):
There's a lot of things here that are probably not how people want to live their life necessarily, or want other people to act.

Jeffrey Pfeffer (00:03:51):
Yeah, so I think it makes people uncomfortable because the realities of what it takes to get power bear almost no resemblance to what you're taught in Sunday school or the mosque or wherever, how your parents raised you. They bear almost no resemblance to how we think the world ought to be. They bear a little resemblance to our aspirations. And I think we look around the world and we see people who have acquired enormous amounts of power and have used it for bad. But I tell people, I see people with hammers hitting other people on the head. That does not mean that a hammer is not a useful tool. You can take a screwdriver and stab it into somebody's belly. I have a very dear friend who we'll probably talk about later in the podcast. Laura Esserman is a breast cancer surgeon, and I tell people, Laura has a knife. She uses it to cure cancer. Muggers have knives. They use it to rob people. So I think we've confused the tool for how it has been used.

Lenny Rachitsky (00:04:56):
You also have this quote that the people who need to understand power and build their power skills are people who come from backgrounds or characteristics who would normally put them at a disadvantage.

Jeffrey Pfeffer (00:05:04):
I think that's exactly right. I will go this Sunday to Nashville Tennessee to talk to a bunch of people of color in the NFL who are trying to rise up the ranks. Stanford runs a program because the NFL is serious, I think, about trying to make more opportunity for people, underrepresented minorities. But these are folks who will not succeed unless they learn power skills, because the world is stacked against them in lots of ways.

Lenny Rachitsky (00:05:41):
This episode is brought to you by Uizard, empowering product leaders to ideate and iterate faster than ever before with the power of AI. As a product manager, I often spend hours taking screenshots and then annotating them with feedback for my team. With Uizard, I can simply upload my screenshot and Uizard's AI will turn them into a fully editable UI design, that I can then take, make tweaks to, and then share with my teams in minutes. And when I want to get really creative and explore totally new ways to improve our product experience, I can use Uizard's AI to generate new design concepts from simple text prompts, and turn them into interactive prototypes effortlessly.

(00:06:21):
There's a reason that over 2.6 million people have trusted Uizard to accelerate every phase of their product life cycle and speed up time to market. Developers can even export UI components to React and CSS to speed up their development. Uizard's drag and drop editor is super easy to use and you can collaborate in real time with your entire team. Even your CEO and customer service teams can contribute. Unlock all of Uizard's game-changing AI powered features and more with 25% off Uizard's Pro annual plan. Visit uizard.io/Lenny and use code Lenny to check out today. That's uizard.io/Lenny.

(00:07:05):
This episode is brought to you by Webflow. We're all friends here, so let's be real for a second. We all know that your website shouldn't be a static asset. It should be a dynamic part of your strategy that drives conversions. That's Business 101. But here's a number for you. 54% of leaders say web updates take too long. That's over half of you listening right now. That's where Webflow comes in. Their visual first platform allows you to build, launch, and optimize web pages fast. That means you can set ambitious business goals and your site can rise to the challenge. Learn how teams like Dropbox, IDEO and Orange Theory trust Webflow to achieve their most ambitious goals today at Webflow.com.

(00:07:51):
Another element of your course that you try to make clear in your syllabus is to teach people not to be as judgmental. You also have this whole huge bold phrase, "This class is not for everyone." Can you just talk a bit about maybe why it's not for everyone and then why being less judgmental is important?

Jeffrey Pfeffer (00:08:07):
The class is extremely popular. I have a long waiting list. It's kind of mythical. And because of the principle of social influence, we are influenced by what other people do. We're influenced in our choices of restaurants, we're influenced in our choices of music. I'm not sure I like Taylor Swift, but I certainly want to go see Taylor Swift because everybody else is. And many people sign up for the class. And one year in particular, there were some people who sat in about the third row up, directly in front of me, and they looked every day like they were having some terrible thing put up some horrible orifice or something. They looked literally in pain, and so I decided, and of course they learned nothing. If you come to the class and you're in that much psychological discomfort, you're not going to learn anything.

(00:08:55):
And so what I try to do is tell people that in order to benefit from this class, you have to be open to learning the material. And if you're not, and by the way, this would be true for any class. If you go to physics and you say, I don't like physics, I hate physics, I don't believe in physics, I don't believe in the theory of physics, whatever, you're not going to learn anything. And so I'm trying to get the class, given those huge waiting lists, I'm trying to get the class to have people in it who are in fact going to benefit from it rather than sit there and look like they're in pain. And the judgmental thing, if Matthew 7, "Judge not that ye be not judged." The Koran says, "Only Allah can judge people." The American poet, Walt Whitman said, "Be curious, not judgmental."

(00:09:46):
Mother Teresa said, "If you judge people, you have no time to love them." Judgment, if I need to build a positive relationship with Lenny because you are on my critical path in my job, and jobs necessarily entail a reasonable amount of interdependence, I get things done through and with other people. If I decide that Lenny is whatever set of bad adjectives you want to use, dumb, incompetent, immoral, whatever, my ability to build a positive relationship with you, and you remain on my critical path, becomes almost zero unless you're a better actor or I'm a better actor than I think most people are.

(00:10:32):
So therefore, you should suspend judgment in the sense that if somebody is on your critical path, the only judgment you should make is they're on my critical path. If I want to get something done, I need their collaboration and cooperation, and the fact that I may not like them is in fact irrelevant. And as I have taught senior executives over the years, it is absolutely clear to me that one of the skills that they have mastered is they have a skill that you cannot tell what they think of you. And that's important because I need your collaboration, I need your cooperation. And if I leak out in ways that say I don't respect you or I don't like you, I don't admire you, whatever, my odds of getting you to work productively with me goes essentially to zero. So that's a judgmental story. Does that make sense?

Lenny Rachitsky (00:11:21):
Absolutely. And I have a quote from your syllabus along these lines, "Not everyone we encounter in Paths to Power is someone you're going to want to emulate. This is a class about how to get things done, how to build and wield influence, and there are multiple ways to accomplish these objectives." So I've had two friends actually go through the course. One I asked about the course of the impact it had on her. She's actually a founder now. And she told me that your class was her single most favorite class at all of Stanford Business School, because it was realistic and applicable to life as a founder. Because it didn't sugarcoat business, didn't sugarcoat life. It told her how the world really works and it is helping her succeed with her startup right now.

Jeffrey Pfeffer (00:12:00):
Thank you. There's no higher praise than that. I actually don't read, I shouldn't probably say this on the public, but I don't actually read my course evaluations because the impact I want to have is not to have people like me, but to have that kind of impact. To make people more successful, more effective in their lives.

Lenny Rachitsky (00:12:22):
That is a good segue to one of your rules of power. So let's just talk about your Seven Rules of Power. I actually have your book right here and if folks want to learn more, here it is, Seven Rules of Power. This is your fourth book about power, and this is your best book about power, because it basically summarizes everything you've learned in a really cohesive way. So let me first share the Seven Rules of Power and then I'm going to dive into a few of them. Does that sound good?

Jeffrey Pfeffer (00:12:44):
That sounds perfect.

Lenny Rachitsky (00:12:45):
Okay, so the seven rules, one, get out of your own way. Two, break the rules. Three, appear powerful. Four, build a powerful brand. Network relentlessly. Use your power, and success excuses almost...

Jeffrey Pfeffer (00:13:01):
Everything, correct.

Lenny Rachitsky (00:13:02):
Okay, success excuses everything almost. Got it. Okay. So let me start with power four, which is around personal brand. And the reason I want to start here is I have a friend that's actually in your class right now, Ralph, and I saw that he started a podcast. And he told me that he did it because it was part of the homework that you give students. You call it Doing Power. Is that the name of the homework assignments?

Jeffrey Pfeffer (00:13:25):
Yeah, I have. So we give them a bunch of assignments throughout the class, which are self-reflective assignments, but their big thing for the class is they have to do power. They have to do something during the quarter to take the principles of the class, and what they're learning, and put them into practice. And that is because everything I do, I do for a reason. That is because if you don't use what you learn, the learning will disappear. So if you go to a French class and you learn French, and you never speak French, in about a relatively short period of time you'll forget everything you learn. And so I want the learning to stick with them, so I try to get them to do something with it.

Lenny Rachitsky (00:14:06):
While we're on this tangent, I wanted to talk about this whole, you have a book, the Doing Knowing Gap.

Jeffrey Pfeffer (00:14:10):
Knowing-Doing Gap.

Lenny Rachitsky (00:14:11):
The Knowing-Doing Gap. So let's just spend a little time there. I think that's really important. What are some examples of things that have come out of people doing these homework assignments, things that maybe led to something interesting?

Jeffrey Pfeffer (00:14:22):
Let me take my most extreme example. My most extreme example is probably Derek Kan, K-A-N. And Derek Kan was a little bit older when he went through the MBA program. And in 2012, his Doing Power project was to get appointed to be Mitt Romney's, it was Mitt Romney was running for president, to be head of economic policy for the Romney campaign. Believe it or not, he was offered the job. Believe it or not, he turned it down for reasons which we could go into if you want, but aren't that interesting. He wound up relatively soon thereafter as number three in the transportation department, working for Elaine Chao. And I have a picture of him when he comes to the class, I introduce him. I tell students, I give them a trigger warning and I introduce him. He wound up, at the end of the Trump administration, as the deputy director of the Office of Management and Budget. And since typical of the Trump administration, by that time there was no director. He essentially ran the $6 trillion US budget. This is six years out of business school.

Lenny Rachitsky (00:15:35):
I was thinking whether he got that during his class. Amazing. Okay, so this is a success story of Doing Power. Let's talk about this first power of building brand, and I think this is where people start to feel uncomfortable. They're going to be like, oh no, I don't want to be doing this. This doesn't feel good to me.

Jeffrey Pfeffer (00:15:51):
Why?

Lenny Rachitsky (00:15:52):
So let's talk about it.

Jeffrey Pfeffer (00:15:53):
Okay.

Lenny Rachitsky (00:15:54):
Let's see. So talk about this power of building a personal brand.

Jeffrey Pfeffer (00:15:58):
There are by definition fewer positions at the top than at the bottom. The world, we might not like this, and I know people, the late Tony Hsieh, the Zappos tried to do holacracy or whatever, but the world is essentially hierarchical. The animal kingdom is hierarchical. It is better to be at the top than at the bottom for a number of reasons. Okay. If that is true, if there are fewer positions at the top and then at the bottom, then your job to advance your career is to figure out how to get promoted. There are many ways to get promoted, but I will guarantee you one thing, no one is going to promote Lenny if they don't know who the hell you are. So it is not sufficient for them to know who you are. They have to know that you're smart and personable and have whatever skills you've got, but they have to know you.

(00:16:52):
If they don't know you, you cannot choose what is not in your head. You know that better than I do. So therefore you have to do something to differentiate yourself. You have to build a brand. So when I think of Lenny in your podcast, I think of something. Not just 25 billion podcasts. When I think of somebody in McKinsey or Bay or BCG, one of the consulting firms, a gazillion people start off as front-line consultants. What are you going to do so that somebody knows who you are? What are you going to do that creates value for the firm and for you? So my friend Keith Ferazzi, when he started at Deloitte Consulting, did not do the spreadsheet stuff that he was, by the way, not very good at and didn't like. He started the Lincoln Quality Award. He decided to try to take Deloitte's brand recognition from about one or 2% to 30%.

(00:17:54):
He was appointed, though he left before he actually took the job, to the position of partner and the first chief marketing officer at Deloitte. I have another friend, Tristan Walker, who wanted to get hired at Foursquare. He sent emails to the founder. The founder ignored him. So Tristan Walker did something I think very bold. Tristan Walker began signing up partnerships. So one day the founder says, holy God, he signed up Starbucks. Maybe I ought to hire this guy. So you have to do something that causes people to know who you are, and that is what building a brand is about.

Lenny Rachitsky (00:18:36):
It sounds very benign put that way. When people hear, I need to build a personal brand, it sounds like I need to post on LinkedIn, I need to post on Twitter, start a newsletter, start a podcast, which I know is kind of a part of it. But what are some examples you've seen of ways to build a personal brand that are effective?

Jeffrey Pfeffer (00:18:50):
So I have a friend who I bring to the class, a lovely tall Asian woman named Laura Chau, who made partner at a venture capital firm after four years, which is fast. Laura works for Canaan Partners, C-A-N-A-A-N. Laura said, I do not work for Andreessen Horowitz, I do not work for Sequoia, I don't work for Greylock, I don't work for any of these very visible large VC firms. So if I am going to get deals in the consumer space, that's her domain of expertise is consumer and consumer tech, somebody is going to have to know who Laura Chau is. Because again, you're not going to get the deal if nobody knows who you are. And so she basically did everything you talked about. She did writing. She started a podcast in which she invited influential people on. She contributed to a book. She helped people out.

(00:19:55):
She did networking dinners. She did everything she could so that people would recognize and know Laura Chau. And one of the things she did was she's tall. In general, Asian women are not tall. She decided to play up the fact that she was, in the words of some people, the tallest Asian woman they'd ever seen. She wears heels. In her heels she's six feet one inches tall. She also, when she comes to my class, many people come to my class with hoodies, God knows what. She has style and it's a unique style. So she thinks about very strategically, how am I going to dress? How am I going to look? How am I going to show up? How am I going to do what I need to do so that people know who I am? Obviously there's substance. If you have visibility without substance, people will know you're useless. But if you have substance without visibility, no one will know the substance that you got.

Lenny Rachitsky (00:20:56):
You also talk about how you can reframe this idea of not being self-promotional, but it's amplifying the impact of the team that I work with, or it's me scaling myself by sharing things I know and pointing people to it.

Jeffrey Pfeffer (00:21:11):
Absolutely, absolutely.

Lenny Rachitsky (00:21:11):
Awesome. Okay, let's move on to a different power. Let's talk about power one, which is getting out of your own way. Talk about what that means and how someone can work on that.

Jeffrey Pfeffer (00:21:20):
Well, you already have done a fabulous job of illustrating principle one by talking about how this is the topic that makes people uncomfortable. And when something makes people uncomfortable, in general, they're going to shy away from it. So if I think power is dirty, if I think power is evil, if I think power is something I want nothing to do with, the first thing that's going to happen is I'm not going to do what I need to do to be successful in my career. So that is one way in which we get in our own way. Another way in which we get in our own way, many people suffer from what is called in the psychology literature, imposter syndrome. They believe that they were the admissions mistake at Stanford. They were the hiring mistake in whatever company they're working for. That somehow they got to this job, but they don't really deserve to be there because they're surrounded by people who are smarter and better than them, and therefore they will do things.

(00:22:18):
I can't even believe that this goes on. So I will have students, not many, but a few. Now raise their hand, I'll call them, and the first things out of their mouth will be, I don't know if this comment is going to be useful. This is called pre-emptory apology. If the comment is not going to be useful, don't say it. That's number one, but number two, don't apologize. Pardon me for interrupting. Pardon me for taking up your time. Stop all this apologizing. If you got the job you're in, you probably are not only qualified, you're probably overqualified. So don't use descriptors of yourself that disempower you. Don't think of yourself as not deserving of the job that you hold, because that attitude will leak out and other people will say, if Lenny doesn't think he deserved the job, then maybe I shouldn't think he deserves the job either.

Lenny Rachitsky (00:23:19):
So basically the first power is you're shooting yourself in the foot by worrying too much about whether people like you. This friend of mine who's now the founder that I read some quotes from, she said her number one takeaway from your class is if you want to be liked, get a dog.

Jeffrey Pfeffer (00:23:32):
That's correct. That is a quote from my dear friend Gary Loveman, who for many years ran Harrah's, which then became Caesars, the casino company.

Lenny Rachitsky (00:23:43):
You also talk about this. Yeah, there's this quote I have. "Acknowledge that others are often no better than you, because that makes you feel better about the story." So for people that actually want to be liked, I like being liked. I don't know if it's a flaw in my upbringing. Is the lesson you're capped on your power if you worry about if you want to be liked?

Jeffrey Pfeffer (00:24:05):
At the end of the day, you don't want to be intentionally disliked. You don't want to violate my dear friend Bob Sutton's book The No Asshole Rule and gratuitously be an asshole. But you are hired to get a job done. It's interesting. I unfortunately am now too old. In the words of my chiropractor, I'm suffering from too many birthdays. So I have a lot of doctors who are doing a great job. Trust me, when I choose a doctor, I have a, unfortunately he retired, a neurosurgeon who did two surgeries on my spine. He's considered to be one of the top 20 in the country. If you go on Yelp, you read about his personality and his office furniture. As I said to somebody, he's doing microsurgery on my spine, I can wind up permanently paralyzed. I really don't care about his personality. I don't care about his office furniture.

(00:25:01):
That's an extreme example, but it makes the point. When you are put in a role, you are put in a role because presumably you are supposed to perform in that role. If you get appointed to be a head coach, if we make you a quarterback on an NFL team, or we make you center on a National Basketball Association team, you did not get that job because people thought you had a cute personality. You got that job because you have the skills to make your organization successful. And if you do not use those skills because you're worried about what everybody else is thinking, you are not only harming yourself, but you're harming them because you are not doing what you were hired to do.

Lenny Rachitsky (00:25:46):
So a simple way of thinking about this, which makes me feel better about. As we talk through this, I'm becoming less uncomfortable with these, which is a good sign. It's don't actively try to be disliked. Don't prioritize being liked, prioritize competence and respect.

Jeffrey Pfeffer (00:25:59):
Correct, absolutely.

Lenny Rachitsky (00:26:01):
And doing the thing that needs to be done.

Jeffrey Pfeffer (00:26:03):
Absolutely.

Lenny Rachitsky (00:26:04):
Okay, great. Let's talk about rule number two, which is break the rules. So basically again, these powers are ways to grow power, and you're saying that if you break the rules, you get more power. Talk about why that's the case, what that looks like.

Jeffrey Pfeffer (00:26:18):
Well, for many reasons, number one, when you break the rules, you stand out. You become memorable when you do something that's unexpected. And being memorable is of course important, as we've already discussed. That's number one. Number two, the rules were made mostly by the people who are favored by the rules in place. So if we were talking about business strategy, we would talk about a word that is probably now overused. We would talk about being a disruptor. That you would disrupt an industry. And how do you disrupt an industry? By doing something that is different from the other industry incumbents. In Southwest Airlines, you don't do hub and spoke. At Amazon, you provide an incredible level of customer service. Whole Foods, you don't optimize on the cost of the stuff in your store, but you optimize on fitting local taste, whatever. You would be a disruptor, you would do things that are different.

(00:27:14):
The same principle holds for you, that if you are going to be successful, you have to do what number one, plays to your strengths, to use the title of a Gallup book. Number two, you have to do things again that cause you to stand out, and you have to do things that basically will make you more successful. One of the conventional wisdoms that people I think adhere to way too much, is don't ask. Don't ask, don't ask for help. You need to show self-sufficiency. So my colleague, Frances Flynn, Frank Flynn, wrote an article with Vanessa Lake entitled, If You Need Help, Just Ask. And it turns out people overestimate how many people they're going to have to ask to get help, and it turns out that asking for help makes people uncomfortable.

(00:28:03):
I was married, if you read the acknowledgments in Seven Rules of Power. For more than 35 years I was married to Kathleen Francis Fowler, who I could send you a picture. She looked literally like a supermodel and I never have been any particularly better looking than I am now. So somebody said to me, how'd you get her to go out with you? And the answer of course is, I asked. I asked, I asked, I asked. No one, well, maybe not no one. But few people are going to go out with you if you don't ask them out. Few people are going to marry you if you don't ask them to marry. Few people are going to do anything if you don't ask. And what is the worst that could happen? If you ask and they say, no, you are no worse off than had you not asked in the first place. If you don't ask, you're not going to get it. If you ask and they say, no, you're not going to get it. Get over your ego, ask.

Lenny Rachitsky (00:28:59):
Is there another example that comes to mind of someone breaking the rules say in business, that ended up being really successful as a result, or just someone that is really good at this in business?

Jeffrey Pfeffer (00:29:07):
I can give you many examples. One of my favorite examples would be Jason Calacanis. I don't know if you know Jason.

Lenny Rachitsky (00:29:14):
Oh yeah, I know you did a case study on him. I know him well. Yeah.

Jeffrey Pfeffer (00:29:18):
So I think Jason, I think consistently breaks all kinds of rules. First of all, when he comes to my class, nobody likes him, but that's okay. Almost nobody likes him. But...

Lenny Rachitsky (00:29:30):
That's power number one. He's not worried about.

Jeffrey Pfeffer (00:29:33):
Yeah, that's number one. But also I think there's a conventional wisdom in the venture capital industry which he defies almost completely. He makes a lot of little bets, not a few big bets. He runs a very lean operation. He doesn't have a lot of partners. He doesn't have actually any partners, so he can't be fired by his partners. He began his career in journalism, which is interesting. Being a journalist is a wonderful job because you get to ask people all kinds of questions. And if you ask smart people questions, and you have some reasonable level of intelligence yourself, at the end of that process, you're going to wind up incredibly smart and incredibly insightful. And he wound up incredibly smart and insightful around aspects of the internet industry.

Lenny Rachitsky (00:30:23):
And he definitely has a lot of power. I think I am going to link to the case study in our show notes where you basically walk through how he went from nothing to a very powerful person.

Jeffrey Pfeffer (00:30:31):
And by the way, a very rich person as well.

Lenny Rachitsky (00:30:34):
Let's talk about another power, power five, networking relentlessly. What does that look like and how do you get better at this?

Jeffrey Pfeffer (00:30:42):
Well, I think there's research that shows that many people find networking dirty, and that's because I think they think about it in the wrong way. My friend John Levy, who has written a fabulous book, You're Invited. Which is a book about how to put on events that people want to come to. Is a fabulous human being. Will tell you that the first principle of networking is in fact generosity. It's generosity. What can I do for you? How can I be helpful? Who can I introduce you to? Either a company or a product or a person who can be helpful to you in your career.

(00:31:20):
But in order to be able to introduce you to someone who's going to be useful to you, I have to know people. If I don't know anybody, I can't introduce you to anybody. And the more people I know, the more likely it will be that if you say, I need to know somebody in X, I will know that person. So the broader your social network, the more people and the more things you will know. If knowledge is power, networking is a fabulous way to get knowledge about people and about ideas.

Lenny Rachitsky (00:31:53):
It's funny to hear this where it's like, yes, obviously this is right, but connecting it to if you want to become more powerful, more successful, you need to do this. I think that is a powerful point that we're all making here, is just like this actually is necessary if you want to acquire more power.

Jeffrey Pfeffer (00:32:10):
Of course. I have a funny story about networking. So we have a thing in Stanford called View From The Top, in which these fancy people come in and give a talk. And one of the people came in and give a talk and made my class instantly popular, because he's an extraordinarily wealthy person. His name is Omid Kordestani, and so I reached out to him and I said, "Omid, let's have breakfast." So I can understand because during the thing, when somebody asked them what class was the most important in his success, he said mine. So I said, "Tell me your story." He said, "Well, I graduated from Stanford. I'm Persian in background, immigrant background. An engineer, HP, all the things that would make you kind of a nerd or something." He said, "I went to work for a couple of startups that didn't do well. Then I found myself in the mid-nineties at Netscape." The browser company, big deal. Anyway, Marc Andreessen.

Lenny Rachitsky (00:33:07):
Oh yeah.

Jeffrey Pfeffer (00:33:08):
Okay. "So I found myself in Netscape in marketing and business development. I was not making any progress. One day I decided to take your class to an extreme. I decided to stop doing my job." So I said, "When people gave you projects to do or assignments or tasks and you didn't do them?" He said, "It turns out that I spent my time, not obviously at the bar, but in networking first with people inside of Netscape. And it turns out if you're well-connected to the senior leaders, they don't really care if you do your job or not. That's number one. Number two," he said, "oftentimes I can find other people to do the work I was doing. So instead of basically spending time doing my job, I networked first within Netscape." But Netscape was not that big of a company. So after a while he decided to go through, which is by the way, his job anyway, in marketing and business, he decided to basically drive through the Silicon Valley talking to people. And this is the rest of the browsers are just beginning.

(00:34:14):
Nobody knows what a browser is, nobody knows what the internet is going to become. So he's having all these fabulous conversations. All right, so he now knows basically everybody. It's 1998 and a little tiny company decides 10 engineers, typical Silicon Valley company. 10 engineers, all engineers. I have a friend who went to work for a company, run a company that had 26 people. He didn't have one sales and marketing person. Anyway, the company now has 10 engineers. They decide they need to hire their first business person. Being an analytically oriented company, they say, we're going to do this very analytically. We are basically going to ask everybody we can think of, and a few people we can't. Give us a list of the best technically oriented business people that you know. And there is of course one name that appears on every list. It is Omid Kordestani who becomes employee number 11 at Google and makes two and a half billion dollars.

Lenny Rachitsky (00:35:13):
Great success from networking. Well done. When people hear about networking, it just feels like you said, very cringe. I don't want to go to these networking events and try to pretend to talk and care about people. Do you have any advice for just how to make networking feel less cringy?

Jeffrey Pfeffer (00:35:28):
One of the exercises I give my students is I say, write a list of 10 people who, if you knew them and if they knew you, would be important for whatever you're trying to accomplish in your career. So maybe you're trying to get into biotech, you need to know 10 executives in biotech or whatever. Make a list of 10 people. Then for each person figure out how you are going to meet them. By the way, not necessarily in a network event. Maybe you want to do what John Levy does, which is hold dinners. In which by the way, the guests do the cooking, which is a very interesting idea. That of course taps into the IKEA effect. You always like something better if you participate in creating it. Maybe you want to hold dinners, maybe you want to try to reach out to them at lunch.

(00:36:21):
Maybe you want to reach out to them and say, here's an article I think you'd be interested in. Here is somebody who I think you'd benefit from meeting. So recently, I'm not a particularly good networker, though I'm better I guess than some people. I recently met Esther Wojcicki, I'm sure I killed her name. She's considered the grandmother of Silicon Valley. She's Susan Wojcicki's mother. And she is interested in depression, particularly depression among teenagers. And I said, you should know Leanne Williams, who's in the Psychiatry and Behavioral Science Department, who has done probably 300 articles on depression and has invented something which is called Precision Psychiatry. So I connected them to each other. Just send an email. I think you ought to know each other. They got together. I get credit for knowing people and connecting them, but I also benefit them. Leanne needs to raise money. Esther's got money. Esther needs to understand the cutting edge research in depression and biomarkers of depression, which is what Leanne's specialty is, so they benefit. So this is not some icky thing. This is connecting people who benefit from being connected.

Lenny Rachitsky (00:37:38):
That touches on something you teach in the book is, in your networking you want to become a broker, you want to be central. That's kind of what you want to work on. Can you talk a bit about what that means?

Jeffrey Pfeffer (00:37:47):
Well, that means you want to connect people. So many people with benefits, and many groups with benefits for being connected, and they're not connected. What does a venture capitalist do? Connects people with ideas to people with money, and takes a fee for doing it. What does an investment banker do? Connects people trying to sell businesses or raise capital with people trying to buy businesses or who have capital. What's a real estate agent do? Connects buyers and sellers of houses. There are all kinds of examples where the person's entire job is connecting people. But even for the people whose job it isn't, you benefit from knowing people. The more people you know, the more things you know. As I sometimes say in my class, if leadership management, call it what you will, is getting things done through other people, it seems like common sense that the more other people you know, the more you'll be able to get done.

Lenny Rachitsky (00:38:45):
As someone that has become central to a lot of things in the product world, and has built a large network as a result of this podcast newsletter, I can tell you a thousand percent. This creates a lot of opportunity and power, you could say in quotes. So I've seen it happen.

Jeffrey Pfeffer (00:39:00):
Of course.

Lenny Rachitsky (00:39:03):
This episode is brought to you by Heap, the product analytics solution that shows you everything users do on your digital product, website, mobile product or other digital services. We all know a great digital experience when we see it. It's intuitive, it anticipates your needs, and it makes it easy for you to do your job. If you're trying to build that kind of experience for your users, you need up to date, reliable information about what your users do in your product and why they do it. Want to know how your users behave across platforms, what keeps them coming back, what they're doing that you're not even aware of? Well, I have some great news for you. Heap captures all of this user activity for you automatically, and then gives you definitive answers to all your questions about user behavior in seconds, not weeks. With Heap, it's easy to prioritize the product investments that improve conversion, engagement and retention. Visit heap.io/Lenny to get started with a demo. That's H-E-A-P dot I-O slash Lenny.

(00:40:10):
Another lesson I guess you teach is to pursue weak ties. Talk a bit about what that means. What do you mean by that?

Jeffrey Pfeffer (00:40:14):
Well, the people to whom you are strongly tied, your family, your spouse, significant other, your friends, the people who work with you at work will probably, because they are close to you, know the same things and the same people that you do. So to the extent that you build ties with people who are more different from you in every dimension, you are more likely to learn non-redundant information, and come in contact with people that you don't already know. This again seems common sense. There's a lot of research behind it. Mark Granovetter, a sociologist who still teaches at Stanford, wrote a book entitled The Strength of Weak Ties.

(00:40:59):
And basically, no, actually, pardon me. He wrote an article called The Strength of Weak Ties. His book was called Getting a Job. And he did a study, I guess it was at that time in Boston. He did a study of job seekers in Massachusetts, and he looked at people who got jobs through applying to ads, through more formal means. And then he looked at people who got jobs through referrals from their network. And it turns out, of course, the jobs that you get through your network referrals are much better jobs. And oftentimes the best jobs that people got were referred to them by people that were not particularly close to them. Because they had a view, that's the word I was looking for, a view into the ecosystem of the Boston labor market, that the other people didn't have because they weren't in the same place as the other people.

Lenny Rachitsky (00:42:00):
I love that lesson. Let's talk about another power, which is using your power. So when people think of using your power builds more power, that's not intuitive. You talk about how it creates this self-perpetuating growth of power the more you practice power. Can you talk a bit about that?

Jeffrey Pfeffer (00:42:16):
Sure. So when you are given, it's interesting, my friend Deborah Gruenfeld, who wrote a book called Acting with Power, talks a lot about people's ambivalence to power. So sometimes people are put in a job. Herminia Ibarra who teaches now at London Business School, at one point taught in Seattle, at one point taught at Harvard, has an article in the Harvard Business Review in which she talks about a woman in a drug company who gets a promotion, and says to her colleagues, to whom she's now overseeing, that she's not sure why she got the job. She's not sure she deserves the job. Needless to say, she didn't do very well in this job because she got in her own way. But part of this is you are put in a position of power. You're put in a position of authority in order to make things happen. So to the extent you mobilize your resources and get things to happen, you'll get more resources.

(00:43:14):
People want to be associated with success. To the extent that you become successful, more people will want to work with you. To the extent that you get more stuff done, you'll get more promotions, more opportunities. Nobody's going to give you a job to do if the last five jobs like that they gave you, you couldn't get done. So the more you are able to do, which oftentimes of course requires power and influence, the more you're able to do, the more you'll be asked to do. But better yet, the more resources you'll be given to help you get things done.

Lenny Rachitsky (00:43:48):
And I think there's interestingly you teach that just showing that you have power creates more power. People see that you have this power and they start to follow your lead more, right? That's a part of this.

_[219 additional lines trimmed for context budget]_

---

### Anthropic's CPO on what comes next | Mike Krieger (co-founder of Instagram)
**Guest:** Mike Krieger | **Date:** 2025-06-05 | [YouTube](https://www.youtube.com/watch?v=DKrBGOFs0GY)  

# Anthropic's CPO on what comes next | Mike Krieger (co-founder of Instagram)

## Transcript

Lenny Rachitsky (00:00:00):
90% of your code roughly is written by AI now.

Mike Krieger (00:00:03):
The team that works in the most futuristic way is the Claude Code team. They're using Claude Code to build Claude Code in a very self-improving kind of way. We really rapidly became bottlenecked on other things like our merge queue. We had to completely re-architect it because so much more code was being written and so many more pull requests were being submitted. Over half of our pull requests are Claude Code generated. Probably at this point it's probably over 70% that it just completely blew out the expectations of it.

Lenny Rachitsky (00:00:26):
You guys are at the edge of where things are heading.

Mike Krieger (00:00:28):
I had the very bizarre experience of I had two tabs open. It was AI 2027, and my product strategy, and it was this moment where I'm like, "Wait, am I the character in the story?"

Lenny Rachitsky (00:00:36):
It feels like ChatGPT is just winning in consumer mind share. How does that inform the way you think about product, strategy, and mission?

Mike Krieger (00:00:43):
I think there's room for several generationally important companies to be built in AI right now. How do we figure out what we want to be when we grow up versus what we currently aren't or wish that we were or see other players in the space being?

Lenny Rachitsky (00:00:55):
What's something that you've changed your mind about what AI is capable of and where AI is heading?

Mike Krieger (00:01:01):
I had this notion coming in like, "Yes, these models are great, but are they able to have an independent opinion?" And it's actually really flipped for me only in the last month.

Lenny Rachitsky (00:01:12):
Today, my guest is Mike Krieger. Mike is chief product officer at Anthropic, the company behind Claude. He's also the co-founder of Instagram. He's one of my most favorite product builders and thinkers. He's also now leading product at one of the most important companies in the world, and I'm so thrilled to have had a chance to chat with him on the podcast. We chat about what he's changed his mind about most in terms of AI capabilities in the years since he joined Anthropic, how product development changes and where bottlenecks emerge when 90% of your code is written by AI, which is now true at Anthropic. Also, his thoughts on OpenAI versus Anthropic, the future of MCP, why he shut down Artifact, his last startup and how he feels about it. Also, what skills he's encouraging his kids to develop with the rise of AI. And we closed the podcast on a very heartwarming message that Claude wanted me to share it with Mike.

(00:02:00):
A big thank you to my newsletter Slack community for suggesting topics for this conversation. If you enjoy this podcast, don't forget to subscribe it and follow it in your favorite podcasting app or YouTube. Also, if you become an annual subscriber of my newsletter, you get a year free of a bunch of incredible products, including Linear, Superhuman, Notion, Perplexity and Granola. Check it out at lennysnewsletter.com and click bundle.

(00:02:22):
With that, I bring you Mike Krieger. This episode is brought to you by Productboard, the leading product management platform for the enterprise. For over 10 years, Productboard has helped customer-centric organizations like Zoom, Salesforce, and Autodesk build the right products faster. And as an end-end platform, Productboard seamlessly supports all stages of the product development lifecycle. From gathering customer insights to planning a roadmap, to aligning stakeholders, to earning customer buy-in, all with a single source of truth.

(00:02:52):
And now product leaders can get even more visibility into customer needs. With Productboard Pulse, a new voice of customer solution built-in intelligence helps you analyze trends across all of your feedback and then dive deeper by asking AI your follow-up questions. See how Productboard can help your team deliver higher impact products that solve real customer needs and advance your business goals. For a special offer and free 15-day trial, visit productboard.com/lenny. That's productboard.com/L-E-N-N-Y.

(00:03:26):
Last year, 1.3% of the global GDP flowed through Stripe. That's over $1.4 trillion and driving that huge number are the millions of businesses growing more rapidly with Stripe. For industry leaders like Forbes, Atlassian, OpenAI, and Toyota, Stripe isn't just financial software. It's a powerful partner that simplifies how they move money, making it as seamless and borderless as the internet itself. For example, Hertz boosted its online payment authorization rates by 4% after migrating to Stripe. And imagine seeing a 23% lift in revenue like Forbes did just six months after switching to Stripe for subscription management. Stripe has been leveraging AI for the last decade to make its product better at growing revenue for all businesses. From smarter checkouts to fraud prevention and beyond. Join the ranks of over half of the Fortune 100 companies that trust Stripe to drive change, learn more at Stripe.com.

(00:04:29):
Mike, thank you so much for being here and welcome to the podcast.

Mike Krieger (00:04:32):
I'm really happy to be here. I've been looking forward to this for a while.

Lenny Rachitsky (00:04:35):
Wow, I had love to hear that. I've also been looking forward to this for a while. I have so much to talk about. So first of all, you've been at Anthropic for just over a year at this point. Congrats by the way on hitting the cliff.

Mike Krieger (00:04:46):
Thank you. Not that we're tracking.

Lenny Rachitsky (00:04:49):
That's right. So let me just ask you this. So you've been at Anthropic for about a year. What's something that you've changed your mind about from before you joined Anthropic to today about what AI is capable of and where AI is heading?

Mike Krieger (00:05:04):
Two things. One is like a pace and timeline question. The other one is a capability question. So maybe I'll take the second one first. I had this notion coming in, yes, these models are great, they're going to be able to produce code, they're going to be able to write hopefully in your voice eventually, but are they able to sort of have an independent opinion? And it's actually really flipped for me only in the last month and only with Opus 4 where my go-to product strategy partner is Claude. And it has been basically for that full year where I'll write an initial strategy, I'll share it with Claude basically, and I'll have it, look at it. And in the past it's pretty anodyne kind of comments that it would leave, "Oh, have you thought about this?" And it's like, "Yeah, I thought about that." And Opus 4, I was working on some strategy for our second half of the year was the first one.

(00:05:51):
It was like Opus 4 combined with our advanced research. But it really went out for a while and it came back and I was like, you really looked at it in a new way. And so that's a thing that I've maybe I didn't feel like it would never be able to do that, but I wasn't sure how soon it'd be able to come up with something where I look at it, I'm like, yep, that is a new angle that I hadn't been looking at before and I'm going to incorporate that immediately into how I think about it. So that's probably the biggest shift that I've had is, I don't know about independence is the right word, but creativity and sort of novelty of thought relative to how I'm thinking about things. But in the timeline, one, it's so interesting because I was sitting next to Dario yesterday and he's like, "I keep making these predictions and people keep laughing at me. And then they come true."

(00:06:31):
And it's funny to have this happen over and over again and he is like, not all of them are going to be right. But even I think as of last year he was talking about we're at 50% on SWE-Bench, which is this benchmark around how well the models are at coding. He's like, "I think we'll be at 90% by the end of 2025 or something like that." And sure enough, we're at about 72 now with the new models and we're at 50% when he made that prediction. And it's continued to scale pretty much as predicted. And so I've taken the timelines a lot more seriously now. And I don't know if you read AI 2027-

Lenny Rachitsky (00:07:05):
I have, it made by heart race.

Mike Krieger (00:07:09):
And I had the very bizarre experience of I had two tabs open, it was AI 2027 and my product strategy. And it was this moment where I'm like, "Wait, am I the character in the story? How much is this converging?" But you read that and you're like, "Oh, 2027, that's years away if you're like no, mid 2025." And things continue to improve and the models continue to be able to do more and more and they're able to act agentically and they're able to have memory and they're able to act over time. So I think my confidence in the timelines and I don't know exactly how they manifest it definitely just solidified over the last year.

Lenny Rachitsky (00:07:43):
Wow. I wasn't expecting to go down that that paper was scary. And I'm curious just I guess I can't help but ask just thoughts on just how do we avoid the scary scenario that paper paints of where AI getting really smart goes?

Mike Krieger (00:07:59):
Yeah, this maybe ties into, I've been here a year, why did I join Anthropic? I was watching the models get better and even you could see it in early 2024, and looking at my kids, I'm like, "All right, they're going to grow up in a world with AI. It's unavoidable." Where can I maximally apply my time to nudge things towards going well? And I mean that's a lot of what people think about across the industry, especially at Anthropic. And so I think coming to an agreement and a shared framework and understanding of what does going well look like? What is the kind of human AI relationship that we want?

(00:08:36):
How will we know along the way? What do we need to build and develop and research along the way? I think those are all the kind of key questions. And some of those are product questions and some of those are research and interpretability questions, but for me it was the strongest reason to join was okay. I think there's a lot of contribution that Anthropic can have around nudging things to go better. And if I can have a part to play there, let's do it.

Lenny Rachitsky (00:09:00):
I love that answer. Speaking of kids, so you've got two kids, I've got a young kid, he's just about to turn two. I'm curious just what skills you're encouraging your kids to build as this AI becomes more and more of our future and some jobs will be changed and just what advice do you have?

Mike Krieger (00:09:18):
We have this breakfast feed breakfast with the kids every morning and sometimes some question will come up, something about physics and our oldest kid's almost six, but they ask funny questions about the solar system or physics or in a 6-year-old way and before we reach for Claude, because at first my instinct is like, "Oh, I wonder how Claude will do this question." And we started changing, "Well, how would we find out?" And the answer can't just be we'll ask Claude, all right, well, we could do this experiment, we could have this thing. So I think nurturing curiosity and still having a sense of, I don't know, the scientific process sounds grandiose to instill in a 6-year-old, but that process of discovery and asking questions and then systematically working right through, I think will still be important. And of course AI will be an incredible tool for helping resolve large parts of that, but that process of inquiry I think is still really important and independent thought.

(00:10:11):
My favorite moment with my kid, because she's very headstrong, our 6-year-old, she said something and I wasn't sure if it was true. It was, oh, is that coral is an animal or corals alive? I don't even remember what the details of it. And I was like, "I don't know if that's true." And she's like, "It's definitely true, dad." I'm like, "All right, let's ask Claude on this one." And she's like, "You can ask Claude, but I know I'm right." And I'm like I love that. I want that kind of level of not just delegating all of your cognition to the AI because it won't always get it right. And also it kind of short circuits any kind of independent thought. So the skill of asking questions, inquiry and independent thinking, I think those are all the pieces. What that looks like from a job or occupation perspective, I'm just keeping an open mind and I'm sure that'll radically change between now and then.

Lenny Rachitsky (00:11:02):
It's interesting. Tobias LÃ¼tke, Shopify CEO, on the podcast and he had the same answer for what he's encouraging his kids to develop is curiosity. And so it's interesting that's a common thread.

Mike Krieger (00:11:14):
The K through eight school our kid goes through had an AI and education expert come in and I had a very low bar or a very low expectation of what this conversation was going to be like. And actually I think it went over most of the people in the audience's heads because he was like, "All right, well let me take you all the way back to Claude Shannon in information theory." And I could see people's eyes going, "What did I sign up for and why am I hearing this school auditorium hearing about information theory?" But he did a really nice job I think of also just imagining there will be different jobs and we don't know what those jobs are going to be and so what are the skills and techniques and remain open mindedness around what the exact way we recombine those things. And even those will probably change three times between now and when they're 18.

Lenny Rachitsky (00:11:59):
So we're talking about timelines and how things are changing. So I've seen these stats that you've shared, other folks at Anthropic have shared about how much of your code is now written by AI. So people have shared stats from 70% to 90%. There was an engineer lead that shared 90% of your code roughly is written by AI now, which first of all is just insane that it went from zero to 90%, I don't know, a few years, something like that. Yeah, basically. I don't think people are talking about this enough. That's just wild. You guys are basically at the bleeding edge. I've never heard a company that has this high a percentage of code being written by AI.

(00:12:34):
So you guys are at the edge of where things are heading. I think most companies will get here. How has product development changed knowing so much of your code is now written by AI, so usually it's like PM, it's like here's what we're building, engineer builds it, it ships it. Is it still kind of roughly that or is it now PMs are just going straight to Claude, build this thing for me, engineers are doing different things? Just what looks different in a world where 90% of your code is written by AI?

Mike Krieger (00:12:57):
Yeah, it's really interesting because I think the role of engineering has changed a lot, but the suite of people that come together to produce a product hasn't yet. And I think for the worst in a lot of ways because I think we're still holding on some assumptions. So I think the roles are still fairly similar, although we'll now get in my favorite things that happen now are some nice PMs that have an idea that they want to express or designers that have an idea they want to express will use Claude and maybe even Artifacts to put together an actual functional demo. And that has been very, very helpful. No, no, this is what I mean that makes it tangible. That's probably the biggest role shift is prototyping happening earlier in the process via more of this code plus design piece. What I've learned though is the process of knowing what to ask the AI, how to compose the question, how to even think about structuring a change between the backend and the front end.

(00:13:54):
Those are still very difficult and specialized skills and they still require the engineer to think about it. And we really rapidly became bottlenecked on other things like our merge queue, which is the get in line to get your change accepted by the system that then deploys into production. We had to completely re-architect it because so much more code was being written and so many more pull requests were being submitted that it just completely blew out the expectations of it. And so it's like, I don't know if you've ever read, is it the goal, the classic process optimization book, and you realize there's this critical path theory. I've just found all these new bottlenecks in our system, there's an upstream bottleneck, which is decision making and alignment. A lot of things that I'm thinking about right now is how do I provide the minimum viable strategy to let people feel empowered to go run and type and build and explore at the edge of model capabilities.

(00:14:44):
I don't think I've gotten that right yet, but that's something I'm working on. And then once the building is happening, other bottlenecks emerge, let's make sure we don't step on each other's toes. Let's think through all the edge cases here ahead of time so that we're not blocked on the engineering side. And then when the work is complete and we're getting ready to ship it, what are all those bottlenecks as well? Let's do the air traffic control of landing the change. How do we figure out large strategy? So I think there hasn't been as much pressure on changing those until this year, but I would expect that a year from now the way that we are conceiving of building and shipping software just changes a lot because it's going to be very painful to do it the current way.

Lenny Rachitsky (00:15:20):
Wow, that is extremely interesting. So it used to be here's an idea, let's go design it, build it, ship it, merge it, and then ship it. And usually the bottleneck was engineering, taking time to build a thing and then design. And now you're saying the two bottlenecks you're finding are okay deciding what to build and aligning everyone and then it's actually the cue to merge it into production. And I imagine review it too is probably a part-

Mike Krieger (00:15:47):
Reviewing has really changed too. And in many ways perhaps unsurprisingly the team that works in the most futuristic way is the Claude Code team because they're using Claude Code to build Claude Code in a very self-improving kind of way. And early on in that project, they would do very line by line pull request reviews in the way that you would for any other project. And they've just realized Claude is generally right and it's producing pull requests that are probably larger than most people are going to be able to review. So can you use a different Claude to review it and then do the human almost acceptance testing more than trying to review line by line. There's definitely pros and cons and so far it's gone well. But I could also imagine it going off the rails and then having a completely both unmaintainable or even understandable by Claude Code base that hasn't happened, but watching them change their review processes definitely has been interesting.

(00:16:38):
And yeah, the merge queue is one instance of the bottom bottleneck that forms down there, but there's other ones which is how do we make sure that we're still building something coherent and packaging it up into a moment that we can share with people and whether that's around a launch moment, whether that's about then enabling people to use this thing and talking about it, the classic things of building something useful for people and then making it known that you've built it and then learning from their feedback still exists. We've just made a portion of that whole process much more efficient.

Lenny Rachitsky (00:17:06):
I heard you describe this as you guys are patient zero for this way of working.

Mike Krieger (00:17:11):
Yes.

Lenny Rachitsky (00:17:12):
I love that. Do you have a sense of what percentage of Claude Code is written by Claude Code?

Mike Krieger (00:17:17):
At this point, I would be shocked if it wasn't 95% plus. I'd have to ask Boris and the other tech leads on there. But what's been cool is so nitty-gritty stuff, Claude Code is written in TypeScript. It's actually our largest TypeScript project. Most of the rest of Anthropic is written in Python, some Go, some Rust now, but we're not like a TypeScript shop. And so I saw a great comment yesterday in our Slack where somebody had this thing that was driving them crazy about Claude Code and they're like, "Well, I don't know any TypeScript, I'm just going to talk to Claude about it and do it."

(00:17:49):
And they went from that to pull requests in an hour and solve their problem and they submitted a pull request and that breaking down the barriers. One, it changes your barrier to entry for any kind of newcomer to the project. I think it can let you choose the right language for the right job for example. I think that helps as well, but I think it also just reinforces Claude Code being that patient alpha of that where contributions from outside the team can be Claude coded as well.

Lenny Rachitsky (00:18:18):
Wow, this is, it's just continue to blow my mind all these things that you're sharing, 95% of Claude Code is written by Claude Code roughly.

Mike Krieger (00:18:27):
That's my guess. Yeah, I'll come back with the real stuff. But I mean if you ask the team, that's how they're working and that's how they're getting contributions from across the company too.

Lenny Rachitsky (00:18:35):
It's interesting going back to your point about strategy being assisted by Claude itself and your point about how a lot of the bottlenecks now are kind of the top of the funnel of coming up with ideas aligning everyone, it's interesting that Claude is already helping with that also of helping you decide what to build. So if those two bottlenecks are aligning, deciding what to build and then just merging and getting everything, where do you see the most interesting stuff happening to help you speed those things up?

Mike Krieger (00:19:02):
Yeah, I think that on that first row, I started the year by writing a doc that was effectively how do we do product today and where is Claude not showing up yet that it should? And I think that upstream part is the next one to go. It's interesting. At your conference I talked to somebody who's working on a PRD, GPT kind of ChatPRD, I think was the-

Lenny Rachitsky (00:19:24):
ChatPRD, [inaudible 00:19:24].

Mike Krieger (00:19:24):
Yeah. Can Claude be a partner in figuring out what to build? What the market size is if you want to approach it that way? What the user needs are if you look at a different way? We think a lot about the virtual collaborator on topic and one of the ways in which I think that can show up is, "Hey, I'm in the Discord, the Claude Anthropic Discord, I'm in the user Fora, I'm on X and I'm reading things and here's what's emergent." That's step one. Models can do that today. Step two, which the models probably can do today, which have to wire them up to do it is and not only are the problems here's how I think you might be able to solve them. And then taking that through to, and I put together a pull request to solve this thing that I'm seeing feels very achievable this year than stringing those things together and we're limited more.

(00:20:13):
This is why MCP is exciting to me. We're limited more around making sure the context flows through all of that so we have the right access to those things more than the model's capability to reason and propose. Now the model might not have perfect UI taste yet, so there's definitely room for design to intervene and be like, "Oh, that's not quite how I would solve the problem of this not showing up." But I would get very excited. I would give you a really small example, but we changed on Claude AI, you should be able to just copy markdown from Artifacts or code from Artifacts and we changed it so you can actually download it and export it. We changed the button to export and we got a bunch of feedback like, "How do I copy now?" And the answer is you drop it down and it's copied.

(00:20:51):
It's just mind one of those things where it's made sense, but we probably got it not quite right. That feedback was in the RUX channel. I would've loved an hour later for a plot to be like, "Hey, if we do want to change it back, here's the PR to do it." And by the way, eventually, and then I'm going to spin up an A/B test to see if this changes metrics and then we'll see how it looks in a week. If you told me that about a year and a half ago going to be like, "Ah, yeah, maybe like 27, maybe 26." But it really feels just at the tip of capabilities right now.

Lenny Rachitsky (00:21:20):
Wow, okay. You mentioned the Lenny and Friends Summit. I wanted to talk about this a bit. So you were on a panel with Kevin Weil, the CPO of OpenAI, I think it was the first time you guys did this maybe the last time for now.

Mike Krieger (00:21:32):
Yeah, we haven't done it since, not for any reason. I had a lot of fun.

Lenny Rachitsky (00:21:34):
What a legendary panel we assembled there with Sarah Guo moderating. And you made this comment actually ended up being the most rewatched part of the interview, which is that you were putting product people on the model team and working with researchers making the model better and you're putting some product people on the product experience making the UX more intuitive, making all that better. And you found that almost all the leverage came from the product team working with the researchers. And so you've been doing more of that. So first of all, does that continue to be true? And second of all, what are the implications of that for product teams?

Mike Krieger (00:22:11):
It's continued to be true. And in fact I think that if the proportion was already skewing towards having more of that embedding, I've just become more and more convinced. I didn't feel as strongly about it during the summit and now I feel really strongly about it. If we're shipping things that could have been built by anybody just using our models off the shelf, there's great stuff to be built by using our models off the shelf by the way, don't get me wrong, but where we should play and what we can do uniquely should be stuff that's really at that magic intersection between the two, right?

(00:22:42):
Artifacts may a great example and if you play with Artifacts with Claude 4, that's an actually really interesting example where we took somebody from our, we have Claude code skills, which is a team that really is doing the post-training around teaching Claude some of these really specific skills and we paired it with some product people and then together we revamped how this looks in the product today and what Claude can do way better than just like, "Yeah, we just used the model and we prompted a little bit."

(00:23:07):
That's just not enough. We need to be in that fine-tuning process. So much of what, if you look at what we're working on right now, but we've shipped recently between research and all these other things are things that the functional unit of work at Anthropic is no longer take the model and then go work with design and product to go ship a product. It's more like we are in the post-training conversations around how these things should work and then we are in the building process and we're feeding those things back and looping them back.

(00:23:36):
I think it's exciting. It's also a new way of working that not all PMs have, but the PMs that have the most internal positive feedback from both research and engineering are the ones that get it that I was in a product review yesterday, I was like, "Oh, if we want to do this memory feature, we should talk to the researchers because we just shipped a bunch of memory capabilities in Claude 4." They're like, "Yeah, yeah, we've been talking to them for weeks, this is how we're manifesting it." It's like, "Okay, I feel good. I feel like we're doing the right things now."

Lenny Rachitsky (00:24:03):
So let me pull on this thread more and there's something I've been thinking about along these lines. So essentially there's a big part of entropic that's building this super intelligent giga brain that's going to do all these things for us over time. And then, as you said, there's the product team that's building the UX around this super intelligent giga brain and over time this super intelligence is going to be able to build its own stuff. And so I guess just where do you think the most value will come from traditional product teams over time? I know this is different because you guys are a foundational alum company and not most companies don't work this way, but just, I don't know, thoughts on just the where most value will come from product teams over time working on AI.

Mike Krieger (00:24:42):
I think there's still a lot of value in two things. One is making this all comprehensible. I think we've done an okay job. I think we could do a much better job of making this conference. What's still the difference between somebody who's really adept at using these tools in their work and most people is huge. And maybe that's the most literal answer to your earlier question around what skills to learn. That is a skill to learn and use it in the same way that I remember we did computer lock class when I was in middle school. I remember being really good at Google and that was actually a skill back in the day to think in terms of this information is out there, how do I query for it? How do I do it? I think it actually was an advantage at the time.

(00:25:21):
Of course now Google is pretty good at figuring out what you're trying to do if you are only in the neighborhood and there's less of that research kind of need. But I still think that's a necessary part of good product development, which is the capabilities are there and even if Claude can create products from scratch, what are you building and how do you make it Comprehensible? Still hard because I think that gets at this much deeper empathy and understanding of human needs and psychology. I was a human community interaction major, I still been talking in my book here. I still feel like that is a very, very, very, very necessary skill. So that's one. Two is, and this straight to call back to another one of your guests, strategy, how we win, where we'll play, figuring out where exactly you're going to want to, of all the things that you could be spending your time or your tokens or your computation on what you want to actually go and do.

(00:26:15):
You could be wider probably than you could before, but you can't do everything. And even from an external perspective, if you're seen to be doing everything, it's way less clear around how you're positioning yourselves. Like strategy I think is still the second piece. And then the third one is opening people's eyes to what's possible, which is a continuation of making it understandable. But we were in a demo with a financial services company recently and we were working on here's how you can use our analysis tool and MCP together and you could see their light up and you're like, "Ah, okay." We call it overhang. The delta between what the models and the products can do and how they're being used on a daily basis. Huge overhang. So that's where still a very, very strong necessary role for product.

Lenny Rachitsky (00:26:59):
Okay, that's an awesome answer. So essentially areas for product teams to lean into more is strategy, just getting better and better at strategy, figuring out what to build and how to win in the market, making it easier to help people understand how to leverage the power of these tools, the comprehensibility and kind of along those lines is opening people's eyes to the potential of these sorts of things. That's where product can still help.

Mike Krieger (00:27:21):
Exactly.

Lenny Rachitsky (00:27:22):
Awesome. So along those lines actually, do you have any just prompting tricks for people, things that you've learned to get more out of Claude when you chat with it?

Mike Krieger (00:27:30):
Sometimes it's funny because in some ways we have the ultimate prompting job, which is to write the system prompt for Claudia AI and we publish all of these, which I think is another nice area of transparency. And we are always careful when giving prompting advice because at least officially, but I'll give you the unofficial version because you don't want things to become like we think this works, but we're not sure why. But I will do small things like in Claude Code and we actually do react to this very literally, but I always ask it to, if I wanted to use more reasoning, think hard and it'll use a different flow and I usually start with that. Nudging, there's a great essay around make the other mistake like if you tend to be too nice, can you focus on... Even if you're trying to be more critical or more blunt, you're probably not going to be the most critical blunt person in the world.

(00:28:18):
And so with Claude sometimes I'm like, "Be brutal, Claude, roast me. Tell me what's wrong with this strategy." I know we were talking earlier about the Claude as thought partner around critiquing product strategy. I think I previously would say things like, "What could be better on this product strategy?" And I'm just like, "Just roast this product strategy," and Claude's like a pretty nice entity. It's hard to push it to be super brutal, but it forces it to be a little bit more critical as well. The last thing I'll say is, so we have a team called Applied AI that does a lot of work with our customers around optimizing Claude for their use case. And we basically took their insights and their way of working and we put it into a product itself. So if you go to our console, our work bench, we have this thing called the prompt improver where you describe the problem and you give it examples and Claude itself will agentically create and then iterate on a prompt for you.

(00:29:09):
I find what comes out of that ends up being quite different than what my intuitions would've been for a good prompt. And so I'd encourage folks to also check that out even for their own use cases because while that tool is met for an API developer putting a prompt into their product, it's equally applicable for a person doing a prompt for themselves. It'll insert XML tags which no human is going to think to do ahead of time. It actually is very helpful for Claude to understand what it should be thinking versus what it should be saying, et cetera. So that's another one is watch our prompt improver and then note that Claude itself is a very good prompter of Claude.

Lenny Rachitsky (00:29:41):
Awesome. Okay, so we're going to link to that, the prompt improver. The core piece of advice you shared early is just do the opposite of what you would naturally do. So if you're trying to be nice, just be brutal, be very honest and frank with me.

Mike Krieger (00:29:53):
Exactly. I find that works quite well. What are the thought patterns that I've fallen into that you want to break me out of?

Lenny Rachitsky (00:29:59):
I saw you guys just today maybe launched a Rick Rubin collab where it said vibe coding. What's that all about?

Mike Krieger (00:30:06):
What I've heard about that. And again, a lot of the coalesce this week between model launch developer event and The Way of Code. We had one of our co-founders, Jack Clark is our head of policy and he got connected to Rick Rubin because I think he's been thinking a lot about coding, the future of coding and creativity and they've stayed in touch. And Rick got excited about this idea of he was creating art and visualizations with Claude and then he had these ideas around the way of the vibe coder and they put together this, actually I mean I love almost everything Rick Rubin. So the aesthetic of it I think is just so on point too. But yeah, this sort of like med meditation is probably the right word. Meditation on creativity, working alongside AI coupled with this really rich, interesting visualizations. But it's one of those things where internally they're like, "Oh yeah, and we're doing this Rick Rubin collab." We were like, "We're doing what? That's amazing."

Lenny Rachitsky (00:31:03):
I looked at it briefly and there's that meme of him just thinking deeply, sitting on a computer with a mouth.

Mike Krieger (00:31:09):
Yes.

Lenny Rachitsky (00:31:10):
And ASCII art, I think.

Mike Krieger (00:31:11):
It's totally, it's like ASCII art vibe.

Lenny Rachitsky (00:31:14):
I'm excited to have Andrew Luo joining us today. Andrew is CEO of OneSchema, one of our long time podcast sponsors. Welcome, Andrew.

Speaker 3 (00:31:21):
Thanks for having me, Lenny. Great to be here.

Lenny Rachitsky (00:31:23):
So what is new with one schema? I know that you work with some of my favorite companies like Ramp and Vanta and Watershed. I heard you guys launch a new data intake product that automates the hours of manual work that teams spent importing and mapping and integrating CSV and Excel files.

Speaker 3 (00:31:39):
Yes, so we just launched the 2.0 of OneSchema FileFeeds. We've rebuilt it from the ground up with AI. We saw so many customers coming to us with teams of data engineers that struggled with the manual work required to clean messy spreadsheets. FileFeeds 2.0 allows non-technical teams to automate the process of transforming CSV and Excel files with just a simple prompt. We support all of the trickiest file integrations, SFTP, S3, and even email.

Lenny Rachitsky (00:32:05):
I can tell you that if my team had to build integrations like this, how nice would it be to take this off our roadmap and instead use something like OneSchema?

Speaker 3 (00:32:13):
Absolutely, Lenny. We've heard so many horror stories of outages from even just a single bad record in transactions, employee files, purchase orders, you name it. Debugging these issues is often like finding a needle in a haystack. OneSchema stops any bad data from entering your system and automatically validates your files, generating error reports with the exact issues in all bad files.

Lenny Rachitsky (00:32:34):
I know that importing incorrect data can cause all kinds of pain for your customers and quickly lose their trust. Andrew, thank you so much for joining me. If you want to learn more, head on over to oneschema.co. That's oneschema.co.

(00:32:48):
Actually going back to the beginning of your journey at Anthropic, what's the story of you getting recruited at Anthropic? Is there anything fun there?

Mike Krieger (00:32:55):
It all started and I actually sent my friend this text. So Joel Lewenstein, who I've known, he and I built our first iPhone apps together in 2007 when the App Store was just out and you could still make money by selling dollar apps on the App Store back in the day. And we were both at Stanford together and we were friends and we've stayed in touch over years and we've never gotten to work together since then. We've just remained close. And I was coming out of the Artifact experience, I was trying to figure out, do I start another company? I don't think so. I need a break from starting something from zero. Do I go work somewhere? I don't know what company would I want to go work at. And he reached out and he's like, "Look, I don't know if you at all considered joining something rather than starting something, but we're looking for a CPO. Would you be interested in chatting?"

(00:33:37):
And at that time, Claude 3 had just come out and I was like, "Okay, this company's clearly got a good research team. The product is so early still." And it was like, "Great, I'll take the meeting." And I first met with Daniela, was one of the co-founders and the president in Anthropic. And just from the beginning I was like a breath of fresh air, very little grandiosity coming off the founders, I mean they're clear-eyed about what they're building. They know what they don't know. How many times I talk to Dario always like Dario is like, "Look, I don't know anything about product, but here's an intuition." Usually the intuition is really good and leads to some good conversation, but I think that intellectual honesty and shared view of what it means to do AI in a responsible way, it just resonated.

(00:34:22):
I kept having this feeling in these interviews, this is the AI company I would've hoped to have found it if I had founded an AI company. And that's kind of the bar around if I'm going to join something that should be where I'm going to go. But what I realized, I actually hadn't joined a company since my first internship in college basically. And I was like, "Oh, how do I onboard myself? How do I get myself up to speed? How do I balance making sweeping changes versus understanding what's not broken about it overall?" And looking back on a year, I think I made some changes too slowly. I think there was ways reorganizing product that I could have made a change earlier. And I think I didn't appreciate how much a couple of really key senior people can shape so much of product strategy.

(00:35:10):
I'll harken back to Claude Code. Claude Code happened because Boris, who actually was a Boris Cherney, he was an Instagram engineer and one of our senior ICs there, we overlapped a bit, was started that project from scratch internal first and then we got it out and then shipped it. And that's the power of one or two really strong people. And I made this mistake, we need more headcount and we do, I think there's more work that we need to do and there's things that I want to be building. But more than that we need a couple of almost founder type engineers that maybe connect back to our question on what skills are useful and how does product development change. And maybe even more so I'm a huge believer in the founding engineer tech lead with an idea and pair them with the right design and product supports, help them realize that, I'm 10 times more a believer in that than before.

Lenny Rachitsky (00:36:01):
I actually asked people on Twitter what to ask you ahead of this conversation. And the most common question surprisingly was why did you shut down Artifact? And I also wondered that because I loved Artifact. I was a power user. I was just like, "Finally a news app that I love that it's giving me what I want to know." So I guess just what happened there at the end?

Mike Krieger (00:36:20):
I still really miss it too. I didn't find a replacement and I think I substituted it by visiting individual sites and keeping things up that way. And it's not really the same, especially on the log to I think we got right with Artifact and if people didn't play with it before, it was we really tried to not just recommend top stories, they were part of it. But really if you were interested in Japanese architecture, you could pretty reliably get really interesting stories about Japanese architecture every day. Whether that's from a Dwell or from Architectural Digest or from a really specific blog that we found that somebody recommended to us. It captured some of that Google reader joy of content discovery of the deeper web. Our headwinds were a couple. One of them was just mobile websites have really taken a turn. I don't blame any individuals for this.

(00:37:10):
I think it's the market dynamics of it, but we put so much time or designers, sky Gunner Gray who's phenomenal that for Perplexity now, the app experience I was so proud of, but when you click through it was like the pressures on these mobile sites and these mobile publishers would be like, "Sign up for our newsletter. Here's a full screen video ad." It was very jarring and we didn't feel like it ethically made sense for us to do a bunch of ad blocking because then you're like, "Sure, you can deliver a nice experience for people, but that doesn't feel like it's playing fair with the publishers." But at the same time, the actual experience wasn't good. So the mobile web deteriorating, which makes me very sad, but I think was part of it. Two was Instagram spread in the early days because people would take photos and then post them on other networks and tell friends about it.

(00:37:57):
And there was this really natural like, "How did you do that? I want to do it." News was very personal. I can't tell you how many people would be like, "I love Artifact." I'm like, "Did you tell anybody about it?" And they're like, "I told one person," and it didn't have that kind of spread. And any attempt that we had to do it felt kind of contrived, like, "Oh, we'll wrap all the links in artifact.news." But we didn't want interstitial things. In some ways, this sounds very puritanical, I don't mean it to sound this way, but there were lines that we didn't want to cross that just felt ethically not us, that I've seen other news players do more of. And maybe if we had done that, it would've grown more, but I don't think that's the company we wanted to have built other way. I don't think we were the founders to have built it.

(00:38:39):
And the third one, which is an underappreciated one, is we started at mid-COVID, which meant that we were fully distributed and I think there were major shifts that we would've wanted to make both in the strategy and the product and the team. And it's really hard to do that if you are all fully remote. Nothing replaces the Instagram days of we went through some hard times like Ben Horowitz called the we're F'ed, it's over kind of moments. This is definitely type two fun. I wouldn't say that my favorite memories because they weren't happy ones, but memories I really stayed with me with Instagram was like me and Kevin at Taqueria, Cancun on Market Street eating burritos at literally 11:00 PM being like, "How are we going to get out of this? How are we going to work through this?" And Zoom is not a good replica for that.

(00:39:26):
You tend to let things go or things build up over time. So the confluence of those three things, we entered I guess 2024 and said, "Look, there is a company to be built in the space. I'm not sure where the people would've built it. This concurrent incarnation we love, but it's not growing." The way I put it's like 10 units of input in for one unit of output versus the other way around. If we put blood and tears into the product and launch something we were proud of and metrics would barely move, the energy is not present in this product, in this system. And so are we going to expend another year or two and then go off and fundraise only to find that this is the case or do we call it and see that it's run its course and try to find a home for it, et cetera.

(00:40:06):
So that was the confluence on it and they started feeling this opportunity cost of AI is starting to change everything. We have an AI powered news app, but is this the maximal way in which we're going to be able to impact this? And it felt like the answer was increasingly no. But it was hard. I mean in the end I was really at peace of the decision, but it was a conversation that went on for a couple of months.

Lenny Rachitsky (00:40:26):
On that note, just how hard was it because because there's an ego component to it, like, "Oh, I'm starting my new company, it's going to be great," and then you end up having to shut it down. Just how hard is that as a very successful previous founder shutting something down and then not working out?

Mike Krieger (00:40:41):
Yeah, I mean I think when we started it, one of the conversations was like, "Look, what is the bar to success here? And do we want it to be something other than Instagram DAU?" Which is just an impossible bar. Only one company since, maybe two, you could say maybe ChatGPT and TikToK have reached that kind of mass consumer adoption starting a news app. Most people are not daily news readers even, right? And so we knew that we weren't pursuing that size of usage, at least with the first incarnation, but we did have an idea of building out complementary products over time that all use personalization and machine learning. We didn't even call it AI at the time. It was 2021 back-

Lenny Rachitsky (00:41:17):
Yeah, yeah, AI, it was called machine learning back then.

_[141 additional lines trimmed for context budget]_

---

### Inside ChatGPT: The fastest growing product in history | Nick Turley (OpenAI)
**Guest:** Nick Turley | **Date:** 2025-08-09 | [YouTube](https://www.youtube.com/watch?v=ixY2PvQJ0To)  

# Inside ChatGPT: The fastest growing product in history  | Nick Turley (OpenAI)

## Transcript

Lenny Rachitsky (00:00:00):
You were a product leader at Dropbox, then Instacart. Now, you're the PM of the most consequential product in history.

Nick Turley (00:00:05):
I didn't know what I would do here because it was a research lab. My first task was I fix the blinds, or something like that.

Lenny Rachitsky (00:00:11):
When someone offers you a rocket ship, don't ask which seat.

Nick Turley (00:00:13):
We set out to build a super assistant. It was supposed to be a hackathon code base.

Lenny Rachitsky (00:00:16):
What was it called before?

Nick Turley (00:00:17):
It was going to be Chat with GPT-3.5 because we really didn't think it was going to be a successful product.

Lenny Rachitsky (00:00:21):
And then Sam Altman is just like, "Hey, let me tweet about it."

Nick Turley (00:00:23):
This is a pattern with AI, you won't know what to polish until after you ship. My dream is that we ship daily.

Lenny Rachitsky (00:00:28):
By the time people hear this, they're going to have their hands on GPT-5.

Nick Turley (00:00:31):
About 10% of the world population uses every week. With scale comes responsibility. It just feels a little bit more alive, a bit more human. This model has taste.

Lenny Rachitsky (00:00:38):
Kevin Weil, your CPO, said to ask you about this principle of, "Is it maximally accelerated?"

Nick Turley (00:00:43):
I just really want to jump to the punchline, "Why can't we do this now?" I always felt like part of my role here is to just set the pace and the resting heartbeat.

Lenny Rachitsky (00:00:49):
Everyone is always wondering, "Is Chat the future of all of this stuff?"

Nick Turley (00:00:52):
Chat was the simplest way to ship at that time. I'm baffled by how much it took off, even more baffled by how many people have copied.

Lenny Rachitsky (00:00:58):
ChatGPT is now driving more traffic to my newsletter than Twitter.

Nick Turley (00:01:02):
That is the type of capability that has been incredibly retentive. I've been really excited about what we've been doing in search.

Lenny Rachitsky (00:01:06):
Can you give us a peek into where this goes long-term?

Nick Turley (00:01:09):
ChatGPT feels a little bit like MS-DOS. We haven't built Windows yet, and it will be obvious once we do.

Lenny Rachitsky (00:01:15):
Today, my guest is Nick Turley. Nick is Head of ChatGPT at OpenAI. He joined the company three years ago, when it was still primarily a research lab. He helped come up with the idea of ChatGPT and took it from 0 to over 700 million weekly active users, billions in revenue, and arguably the most successful and impactful consumer software product in human history. Nick is incredible. He's been very much under the radar. This is the first major podcast interview that he has ever done, and you are in for a treat. We talk about all the things, including the just launched GPT-5.

(00:01:50):
A huge thank you to Kevin Weil, Claire Vo, George O'Brien, Joanne Jang, and Peter Deng for suggesting topics for this conversation. If you enjoy this podcast, don't forget to subscribe and follow it in your favorite podcasting app, or YouTube. And if you become an annual subscriber of my newsletter, you get a year free of a bunch of incredible products, including Lovable, Replit, Bolt, n8n, Linear, Superhuman, Descript, Wispr Flow, Gamma, Perplexity, Warp, Granola, Magic Patterns, Raycast, ChatPRD, and Mobbin. Check it out lennysnewsletter.com and click, "bundle". With that, I bring you Nick Turley.

(00:02:21):
This episode is brought to you by Orkes, the company behind open source Conductor, the orchestration platform powering modern enterprise apps and agentic workflows. Legacy automation tools can't keep pace. Siloed, low-code platforms, outdated process management, and disconnected API tooling falls short in today's event-driven, AI-powered agentic landscape. Orkes changes this. With Orkes Conductor, you gain an agentic orchestration layer that seamlessly connects humans, AI agents, APIs, microservices, and data pipelines in real time at enterprise scale, visual and code-first development, built-in compliance, observability, and rock-solid reliability, ensure workflows evolve dynamically with your needs. It's not just about automating tasks, it's orchestrating autonomous agents and complex workflows to deliver smarter outcomes faster. Whether modernizing legacy systems or scaling next-gen, AI-driven apps, Orkes accelerates your journey from idea to production. Learn more and start building at orkes.io/lenny, that's orkes.io/lenny.

(00:03:22):
This episode is brought to you by Vanta, and I am very excited to have Christina Cacioppo, CEO and co-founder of Vanta, joining me for this very short conversation.

Christina Cacioppo (00:03:31):
Great to be here. Big fan of the podcast and the newsletter.

Lenny Rachitsky (00:03:34):
Vanta is a longtime sponsor of the show, but for some of our newer listeners, what does Vanta do and who is it for?

Christina Cacioppo (00:03:41):
Sure. So we started Vanta in 2018, focused on founders, helping them start to build out their security programs and get credit for all of that hard security work with compliance certifications, like SOC 2 or ISO 27001. Today, we currently help over 9,000 companies, including some startup household names, like Atlassian, Ramp, and LangChain, start and scale their security programs, and ultimately build trust by automating compliance, centralizing GRC, and accelerating security reviews.

Lenny Rachitsky (00:04:12):
That is awesome. I know from experience that these things take a lot of time and a lot of resources, and nobody wants to spend time doing this.

Christina Cacioppo (00:04:20):
That is very much our experience, but before the company, and some extent, during it, but the idea is, with automation, with AI, with software, we are helping customers build trust with prospects and customers in an efficient way. And our joke, we started this compliance company so you don't have to.

Lenny Rachitsky (00:04:36):
We appreciate you for doing that, and you have a special discount for listeners. They can get $1,000 off Vanta at vanta.com/lenny, that's vanta.com/lenny for $1,000 off Vanta. Thanks for that, Christina.

Christina Cacioppo (00:04:50):
Thank you!

Lenny Rachitsky (00:04:55):
Nick, thank you so much for joining me, and welcome to the podcast.

Nick Turley (00:04:59):
Thanks for having me, Lenny.

Lenny Rachitsky (00:05:00):
I already had a billion questions I wanted to ask you, and then you guys decided to launch GPT-5 the week that we're recording this. So, now, I have at least 2 billion questions for you. I hope you have a lot of time. First of all, just congrats on the launch. It's coming tomorrow, the day after recording this. Just congrats. How are you feeling? I imagine this is an ungodly amount of work and stress. How are you doing?

Nick Turley (00:05:22):
It's a busy week, but we've been working on this for a while, so it also feels really good to get it out.

Lenny Rachitsky (00:05:27):
So, by the time people hear this, they're going to have their hands on GPT-5, the newest ChatGPT. What's the simplest way to just understand what this is, what it unlocks, what people can do with it? Give us the pitch.

Nick Turley (00:05:39):
I'm so excited about GPT-5. I think for most people, it's going to feel like a real step change. If you're the average ChatGPT user, and we have 700 million of them this week, you've probably been on GPT-4o for a while. You probably don't even think about the model that powers the product. And GPT-5, it just feels categorically different. I'll talk about a lot of the specifics, but at the end of the day, the vibes are good, at least we feel that way. We hope that users feel the same. And increasingly, that is the thing that I think most people notice, right? They don't look at the academic benchmarks. They don't look at evaluations. They try the model and see what it feels like. And just on that dimension alone, I'm so excited. I've been using it for a while, but it is also the smartest, most useful, and fastest frontier model that we've ever launched.

(00:06:33):
On pure SMARTs, one way to look at that is academic benchmarks on many of the standard ones, whether or not it's math, or reasoning, or just raw intelligence. This model is state of the art. I'm especially excited about its performance on coding, whether or not that's SWE-bench, which is a common benchmark, or actually front-end coding is really, really good as well, and that's an area where I feel like there's the true step change improvement in GPT-5. But really, no matter how you measure the SMARTs, it's quite remarkable, and I think people are going to feel the upgrade, especially if they weren't using o3 already.

(00:07:13):
And the second thing beyond SMARTs is it's just really useful. Coding is one axis of utility, whether or not you have coding questions or you're vibe coding an app, but it's also a really good writer. I write for a living, internally, externally. I just wrote a big blog post that we published Monday, and this thing is such an incredible editor. And compared to some of the older models, it's got taste, which I think is really exciting. And to me, that's something that is truly useful in my day-to-day. And there's a bunch of other areas, like it's state of the art on health, which is useful when you need it, but again, the thing you can't really express in use cases or data is the vibe of the model. And it just feels a little bit more alive, a bit more human in a way that is hard to articulate until you try it. So, feel good about that.

(00:08:06):
And yeah, as mentioned, it's faster. It thinks, too, just like o3 did, but you don't have to manually tell it to do that. It'll just dynamically decide to think when it needs to. And when it doesn't need to think, it just responds instantly, and that ends up feeling quite a bit faster than using o3 did. And then maybe the thing that's most exciting is that we're making it available for free, and that's one of those things that I feel like we can uniquely do at OpenAI. Because many companies, I think, if they have a subscription model like us, they would gate it behind their paid plan. And for us, if we can scale it, we will, and that just feels awesome. We did that with 4o as well. So, everyone is going to be able to try GPT-5 tomorrow, hopefully.

Lenny Rachitsky (00:08:46):
How long does something like this take? I don't know if there's a simple answer to this, but just how long have you guys been working on GPT-5?

Nick Turley (00:08:51):
We've been working on it for a while. You can view GPT-5 as a culmination of a bunch of different efforts. We had a reasoning tech, we had a more classic post-screening methodologies, and therefore, it's really hard to put a beginning on it, but it really is the end point of a bunch of different techniques that we began for a while.

Lenny Rachitsky (00:09:14):
Can you give us a peek into the vision for where ChatGPT is going, GPT in general is going? If you look at on the surface, it's been the same idea with a much smarter brain for a long time. I'm curious where this goes long-term.

Nick Turley (00:09:28):
So, to maybe back up a bit, now, you think of ChatGPT as, "Is this going to be ubiquitous product?" Again, about 10% of the world population uses every week.

Lenny Rachitsky (00:09:37):
Holy shit.

Nick Turley (00:09:39):
I think we have 5 million business customers now. It's an established category in its own right. But really, when we started, we set out to build a super assistant, that's how we talked about it at the time. In fact, the code base that we use is called SA Server. It was supposed to be a hackathon code base, but things always turn out a little bit differently. So, yeah, in some ways, that is still the vision. The reason I don't talk about it more than I do is because I think assistant is a bit limiting in terms of the mental model we're trying to create. You think of this very personified human thing, maybe utilitarian, maybe a... And frankly, having an assistant is not particularly relatable to most people, unless they're in Silicon Valley and they're a manager, or something like that. So it's imperfect.

(00:10:24):
But really, what we envision is this entity that can help you with any task, whether or not that's at home, or at work, or at school, really any context, and it's an entity that knows what you're trying to achieve. So, unlike ChatGPT today, you don't have to describe your problem in menu to detail because it already stands your overarching goals and has context on your life, et cetera. So, that's one thing that we're really excited about. The inverse of giving it more inputs on your life is giving it more action space. So, we're really excited to allow it to do, over time, what a smart, empathetic human with a computer could do for you. And I think the limit of the types of problems that you can solve for people, once you give it access to tools like that, is very, very different than what you might be able to do in a chatbot today. So, that's more outputs.

(00:11:19):
And I often think, "Okay, I'm a general intelligence. What happened if I became Lenny's intern, or something?" And I wouldn't be particularly effective despite having both of those attributes that I just mentioned, and it's because I think this idea of building a relationship with this technology is also incredibly important. So, that's maybe the third piece that I'm excited about is building a product that can truly get to know you over time. And you saw us launch some of those things with improved memory earlier this year, and that's just the beginning of what we're hoping to do so that it really feels like this is your AI. So, I don't know if supersystem is still the right exact analogy, but I think people just think of it as their AI. And I think we can put one in everyone's pocket and help them solve real problems, whether or not that's becoming healthy, whether or not that's starting a business, whether or not that's just having a second opinion on anything. There's so many different problems that you can help with people in their daily life, and that's what motivates me.

Lenny Rachitsky (00:12:16):
So an interesting between the lines that I'm reading here is the vision is for it to be an assistant for people not to replace people. It feels like a really important piece of the puzzle. Maybe just talk about that.

Nick Turley (00:12:29):
AI is really scary to people, and I understand there's decades of movies on AI that have a certain mental model baked in. And even if you just look at the technology today, everyone, I think, has this moment where the AI does something that was really deeply personal to them and you're thought, "Hey, AI can never do that." For me, it was weird music theory things where I was like, "Wow, this thing actually understands music better than I do," and that's something I'm passionate about. And so it's naturally scary. And I think the thing that's been really important to us for a long time is to build something that feels like it's helpful to you, but you're in the driver's seat, and that's even more important as the stuff becomes agentic, the feeling of being in control, and that can be small things.

(00:13:15):
We built this way of watching what the AI is doing when it's in agent mode. And it's not that you actually are going to watch it the whole time, but it gives you a mental model and makes you feel in control in the same way that, when you're in a Waymo, you get that screen, for those of you who've tried Waymo. You can see the other cars. It's not like you're going to actually watch, but it gives you the sense that you know how this thing works and what's happening, or we always check with you to confirm things. It's a little bit annoying, but it puts you in the driver's seat, which is important. And for that reason, we always view technology and the technology that we build as something that amplifies what you're capable of, rather than replacing it, and that becomes important as the deck gets more powerful.

Lenny Rachitsky (00:13:53):
Okay. So you mentioned the beginnings of ChatGPT. I was reading in a different interview. So you joined OpenAI. ChatGPT was just this internal experimental project that was basically a way to test GPT-3.5, and then Sam Altman is just like, "Hey, let me tweet about it, maybe see if people find this interesting," yada yada, yada. It's the most successful consumer product in history, I think both in growth rate in users and revenue, and just absurd. Can you give us a glimpse into that early period before it became something everyone is obsessed with?

Nick Turley (00:14:24):
Yeah. So we had decided that we wanted to do something consumer-facing, I think, right around the time that GPT-4 finished training, and it was actually mainly for a couple of reasons. We already had a product out there, which was our developer product. That's actually what I came in to help with initially, and that has been amazing for the mission. In fact, it's grown up. And now, it's the OpenAI platform with, I don't know, 4 million developers, I think. But at that time, it was early stage, and we were running into some constraints with it because there was two problems. One, you couldn't iterate very quickly because, every time you would change the model, you'd break everyone's app. So, it was really hard to try things.

(00:15:03):
And then the other thing was that it was really hard to learn because the feedback we would get was the feedback from the end user to the developer to us. So it was very disintermediated, and we were excited to make fast progress towards AGI and it just felt like we needed a more direct relationship with consumers. So we were trying to figure out where to start. And in classic OpenAI fashion, especially back then, we put together a hackathon of enthusiasts of just hacking on GPT-4 to see what awesome stuff we could create and maybe ship to users, and everyone's idea was some flavor of a super assistant. They were more specific ideas, like we had a meeting bot that would call into meetings, and the vision was maybe it would help you run the meeting over time. We had a coding tool, which full circle now, probably ahead of its time. And the challenge was that we tested those things, but every time we tested these more bespoke ideas, people wanted to use it for all this other stuff because it's just a very, very generically powerful technology.

(00:16:04):
So, after a couple of months of prototyping, we took that same crew of volunteers, and it was truly a volunteer group, right? We had someone from the supercomputing team who had built an iOS app before. We had someone on the research team who had written some backend code in their life. They were all part of this initial ChatGPT team, and we decided to ship something open-ended because we just wanted a real use case distribution. And this is a pattern with AI, I think, where you really have to ship to understand what is even possible and what people want, rather than being able to reason about that a priori. So, ChatGPT came together at the end because we just wanted the learnings as soon as we could, and we shipped it right before the holiday thinking we would come back and get the data and then wind it down. And obviously, that part turned out super differently because people really liked the product as is.

(00:16:56):
So I remember going through the motions of like, "Oh, man, dashboard is broken. Oh, wait, people are liking it. I'm sure it's just going viral and stuff is going to die down," to like, "Oh, wow, people are retaining, but I don't understand why." And then eventually, we fell into product development mode, but it was a little bit by accident.

Lenny Rachitsky (00:17:14):
Wow. I did not know that ChatGPT emerged out of a hackathon project. Definitely the most successful hackathon project.

Nick Turley (00:17:21):
I like to tell this story when we do our hackathons because I really do want people to feel like they can ship their idea, and it's certainly been true in the past, and we'll continue to make it true.

Lenny Rachitsky (00:17:32):
If you don't want to share these things, but I wonder who that team was.

Nick Turley (00:17:34):
The team is largely still around. Some of the researchers working on GPT-5, actually, were always part of the ChatGPT team. Engineers are still around. Designers are still around. I'm still here, I guess. So, yeah, you've got the team still running things, but obviously, we've grown up tremendously, and we've had to because with scale comes responsibility. And we're going to hit a billion users soon and you have to begin acting in a way that is appropriate to that scale.

Lenny Rachitsky (00:18:06):
Okay. So let me spend a little time there. So, I don't know if this is 100% true, but I believe it is that ChatGPT is the fastest growing, most successful consumer product in history. Also, the most impactful on people's lives. It feels like it's just part of the ether of society now. It's just my wife talks to it. Every question I have, I go to it, voice mode. My wife is just like, "Let me check with ChatGPT." It's just such a part of our life now, and I think it's still early. So many people don't even know what the hell is going on. Just as someone leading this, do you ever just take a moment to reflect and think about just like, "Holy shit"?

Nick Turley (00:18:45):
I have to. It's quite humbling to get to run a product like that, and I have to pinch myself very frequently, and I also have to sometimes sit back and just think, which is really hard when things are moving so quickly. I love setting a fast pace at the company, but in order to do that with confidence, I need at least one day every week that I'm entirely unplugged and I'm just thinking about what to do and process the week, et cetera.

(00:19:14):
And the other thing is I've never ever worked on a product that is so empirical in its nature where, if you don't stop, and watch, and listen to what people are doing, you're going to miss so much, both on the utility and on the risks, actually. Because normally, by the time you ship a product, you know what it's going to do. You don't know if people are going to like it, that's always empirical, but you know what it can do. And with AI, because I think so much of it is emergent, you actually really need to stop and listen after you launch something and then iterate on the things people are trying to do and on the things that aren't quite working yet. So, for that reason alone, I think it's very important to take a break and just watch what's going on.

Lenny Rachitsky (00:20:03):
Okay. So you take a day off every week... not off. Okay, that's not the right way to put it. You take a day of thinking time, deep work.

Nick Turley (00:20:12):
I need it. Yeah, yeah, yeah. And I need to hard unplug on a Saturday, or something like that. Obviously-

Lenny Rachitsky (00:20:16):
On a Saturday [inaudible 00:20:16].

Nick Turley (00:20:16):
But it's just not possible otherwise. This has been a giant marathon for three years now. Yeah.

Lenny Rachitsky (00:20:25):
Like a sprint marathon.

Nick Turley (00:20:26):
Sprint marathon, that's right, or interval training, or something. I don't know how to exactly describe the OpenAI launch cadence, but you've got to set yourself up in a way that is sustainable. Even if this wasn't AI and it didn't have the interesting attributes that I just mentioned, I think you would need to do that. But especially with AI, it's important to go watch.

Lenny Rachitsky (00:20:45):
So, along those lines, I talked to a bunch of people that work with you, that work at OpenAI. Joanne specifically said that urgency and pace are a big part of how you operate, that that's just something you find really important, to create urgency within the team constantly, even when you are the fastest growing product in history, growing like crazy. Talk about just your philosophy on the importance of pace and urgency on teams.

Nick Turley (00:21:08):
Well, it's nice of her to say that. Two things, with ChatGPT, when we decided to do it, we had been prototyping for so long and I was just like, "In 10 days, we're going to ship this thing," and we did. So, that was maybe a moment in time thing where I just really wanted to make sure that we go learn something. Ever since then, I spent so much time thinking about why ChatGPT became successful in the first place, and I think there was some element of just doing things where there was many other companies that had technology in the LLM space that just never got shipped. And I just felt like, of all the things we could optimize for, learning as fast as possible is incredibly important. So I just started rallying people around that, and that took different forms.

(00:21:55):
For a while, when we were of that size, I just ran this daily release sync and had everyone who was required to make a decision in it, and we would just talk about what to do and to pivot from yesterday, et cetera. Obviously, at some point, that doesn't scale, but I always felt like part of my role here, obviously, was to think about the direction of the product, but also to just set the pace and the resting heartbeat for our teams. And again, this is important anywhere, but it's especially important when the only way to find out what people like and what's valuable is to bring it into the external world. So, for that reason, I think it's become a superpower of OpenAI, and I'm glad that Joanne thinks that I had some part in that, but it really has taken a village.

Lenny Rachitsky (00:22:38):
I love this phrase, "the resting heart rate of your team". That's such a perfect metaphor of just the pace of being equivalent to your resting heart rate.

Nick Turley (00:22:46):
I actually learned that at Instacart, when I showed up there, because we were in the pandemic and it was all hands on deck. For a while, there was this... I think there was a company-wide stand-up because we disbanded all teams. We were just trying to keep the site up. And for me, I had been used to taking my sweet time and just thinking really hard about things, and that's important, but I really learned to hustle over there, and I think that's come in handy at OpenAI.

Lenny Rachitsky (00:23:12):
Okay. So, along these same lines, I asked Kevin Weil, your CPO, what to ask you, and he said to ask you about this principle of, "Is it maximally accelerated?" Talk about that.

Nick Turley (00:23:22):
That's funny, we have a Slack emoji, apparently, for this now because I used to say that. Now, I try to paraphrase. Sometimes, I just really want to jump to the punchline of like, "Okay, why can't we do this now?" or, "Why can't we do it tomorrow?" And I think that it's a good way to cut through a huge number of blockers with the team and just instill... especially if you come from a larger company. At some point, we started hiring people from larger tech companies. I think they're used to, "Let's check in on this in a week," or, "Let's circle back next quarter to see if we can go on the plan." And I just, as a-

Nick Turley (00:24:00):
... on the plan and I just kind of as a thought exercise, always like people asking, "Okay, if this was the most important thing and you wanted to truly maximally accelerate it, what would you do?" That doesn't mean that you go do that, but it's really a good forcing function for understanding what's critical path versus what can happen later. And I've just always felt like execution is incredibly important. These ideas, they're everywhere. Everyone's talking about a personal AI, you might've seen news on that and I really think that execution is one of the most important things in the space and this is a tool. So, it's funny that that became a meme. It's like a little pink Slack emoji that people just put on whatever they're trying to force the question.

Lenny Rachitsky (00:24:45):
I was going to ask, what theme [inaudible 00:24:47]. So, it's a little pink, is there something in there like-

Nick Turley (00:24:48):
It's a Comic Sans emoji that says, is this maximally accelerated?

Lenny Rachitsky (00:24:53):
Okay. And so, the kind of the culture there is when someone is working on something, the push is, is this maximally accelerated? Is there a way we can do this faster? Is there anything we can unblock?

Nick Turley (00:25:02):
Yeah. And we use that sparingly, right? Because it needs to be appropriate to the context. There's some things where you don't want to accelerate as quickly as possible because you kind of want process. And we're very, very deliberate on that where your process is a tool. And one of the areas where we have an immense amount of process is safety. Because A, the stakes are already really high, especially with these models, GPT-5 which is a frontier in so many different ways. But B, if you believe in the exponential, which I do and most people who work on this stuff do, you have to play practice for a time where you really, really need the process for sure, sure, sure. And that's why I think it's been really important to separate out the product development velocity, which has to be super high from, for things like frontier models, there actually needs to be a rigorous process where you red team, you work on the system card, you get external input, and then you put things out with confidence that it's gone through the right safeguards.

(00:26:02):
So, again, it's a nuanced concept, but I found it very, very useful when we needed and for everything product development, you're a dead on arrival, so it's important to get stuff out.

Lenny Rachitsky (00:26:11):
We got to open source those memes so that other teams can build on this approach.

Nick Turley (00:26:16):
Absolutely.

Lenny Rachitsky (00:26:17):
So, interestingly with ChatGPT, and it's not a surprise, but not only is it the fastest-growing, most successful consumer product ever, retention is also incredibly high. People have shared these stats that one month retention is something like 90%, six month retention is something like 80%. First of all, are these numbers accurate? What can you share there?

Nick Turley (00:26:39):
I'm obviously limited on what exactly I can share, but it is true that our retention numbers are really exciting and that is actually the thing we look at. We don't care at all how much time you spend in the product. In fact, our incentive is just to solve your problem and if you really like the product, you'll subscribe, but there's no incentive to keep you in the product for long. But we are obviously really, really happy if over the long run, three month period, et cetera, you're still using this thing. And for me, this was always the elephant in the room early on. It's like, "Hey, this may be a really cool product, but is this really the type of thing that you come back to?" And it's been incredible to not just see strong retention numbers, but just see an improvement in retention over time even as our cohorts become less of an early adopter and more the average person, so.

Lenny Rachitsky (00:27:29):
Yeah. So, that note is something that I don't think people truly understand how rare this is when a product... The cohort of users comes, tries it out and then retention over time goes down and then it comes back up, people come back to it a few months later and use it more. It's called a smiling curve, a smile curve, and that's extremely rare.

Nick Turley (00:27:48):
Yeah, yeah. Yeah. There's some smiling going on that's just on the team and I feel like have technology, some of it is not the product. I think people are actually just getting used to this technology in a really interesting way, where I find, and this is why the product needs to evolve too, that this idea of delegating to an AI, it's not natural to most people. It's not like you're going through life and figuring out what can I delegate? Certain sphere of Silicon Valley does that because they're in a self-optimization mode and they're trying to delegate everything they can. But I think for most people in the world it's actually quite unnatural. And you really have to learn, "Okay, what are my goals actually and what could another intelligence help me with?"

(00:28:26):
And I think that just takes time and people do figure it out once they've had enough time with the product. But then of course there's been tons of things that we've done in the product too, whether or not it's making the core models better, whether or not it's new capabilities like search and personalization and all that kind of stuff, or just standard growth work too, which we're starting to do. That stuff matters too, of course.

Lenny Rachitsky (00:28:49):
So, you might be answering this question already, but let me just ask it directly. People may look at this and be like, "Okay, they're building this kind of layer on top of this God-like intelligence. Of course it will grow incredibly fast and retention will be incredible. What do you guys actually doing that sits on top of the model that makes it grow so fast and retain so much?" Is there something that has worked incredibly well that has moved metrics significantly that you can share?

Nick Turley (00:29:18):
One thing we've learned, I'll answer that question in a minute, but one thing we've learned with ChatGPT is that there really is no distinction between the model and the product. The model is the product and therefore you need to iterate on it like a product. And by that I mean obviously you typically start by shipping something very open-ended, at least if you're OpenAI [inaudible 00:29:38] that's kind of a playbook. But then you really have to look at what are people trying to do? Okay, they're trying to write, they're trying to code, they're trying to get advice, they're trying to get recommendations and you need to systematically improve on those use cases. And that is pretty similar to product development work. Obviously the methodology is a bit different, but discovery is the same. You got to talk to people, you got to do data science and you got to try stuff and get feedback.

(00:30:04):
So, that's one chunk of work that we've been very consciously doing is improving the model on the use cases people care about. And there's also such thing as vibes because I'm sure you know and that's one of the things that I'm excited about in GPT-5 is that the vibes are really good. So, that too is, we have a model behavior team and they really focus on what is the personality of this model and how does it speak and talk. So, there's that kind of work. I would say that's maybe a third of the retention improvements that we see or so just roughly. And then I think another third is what I would call product research capabilities. They're research driven for sure. They have a research component, but they're really new product features or capabilities. And search is one example of that where if you remember in the olden days, maybe 20 months ago or something, you would talk to ChatGPT and it'd be like, "As of my knowledge cut off..." Or, "I can't answer that because that happened to recently," or something like that.

(00:31:00):
And that is the type of capability that has been incredibly retentive and for good reason. It just allows you to do more with the product personalization, like this idea of advanced memory where it can really get to know you over time is another example of a capability like that. I think that's another good chunk. And then the third stuff is the stuff you would do in any product and those things exist too. Not having to log in was a huge hit because it removed a ton of the friction. I think we had this intuition from the beginning, but we never got to it because we didn't have enough GPU or other constraint to really go do that. So, there's the traditional product work too. So, I often think about it as roughly a third, a third, a third, but really we're still learning and we're planning to evolve the product a ton, which is why I'm sure there's going to be new levers.

Lenny Rachitsky (00:31:52):
You mentioned something that I want to come back to real quick. You said that it was something like 10 days from Hackathon to Sam tweeting about ChatGPT being live?

Nick Turley (00:32:01):
The Hackathon happened much earlier and we were prototyping for a long time, but at some point we basically ran out of patience on trying to build something more bespoke. And again, that was mostly because people always wanted to do all this other stuff whenever we tested it. So, it was 10 days from when we decided we were going to ship to when we shipped. And the research we'd been testing for a long time, it was kind of an evolution of what we'd called instruction following, which was the idea that instead of just completing the sentence, these models could actually follow you instructions. So, if you said summarize this, it would actually do so. And the research had evolved from that into a chat format where we could do it multi-turn. So, that research took way longer than 10 days and that kind of baking in the background, but the productization of this thing was very, very fast and lots of things didn't make it in.

(00:32:50):
I remember we didn't have history, which of course was the first user feedback we got. The model had a bunch of shortcomings and it was so cool to be able to iterate on the model. The thing I just talked about, treating the model as a product was not a thing before ChatGPT because we would ship in more hardware where there'd be a release GPT-3 and then we would start working on GPT-4 and these weird giant big spend R&D projects that would take a really long time and the spec was whatever the spec was and then you'd have to wait another year. And ChatGPT really broke that down because we were able to make iterative improvements to it just like software. And really, my dream is that it would be amazing if we could just ship daily or even hourly like in software land because you could just fix stuff, et cetera. But there's of course all kinds of challenges in how you do that while keeping the personality intact while not regressing other capabilities. So, it's an open field to get there.

Lenny Rachitsky (00:33:42):
That's such a good example of is it maximally accelerated? Okay, we're going to ship ChatGPT 10 days.

Nick Turley (00:33:48):
[inaudible 00:33:48]-

Lenny Rachitsky (00:33:48):
Holy moly. We've been talking about ChatGPT. Clearly it's kind of a chat interface. Everyone's always wondering is chat the future of all of this stuff? Interestingly, Kevin Weil made this really profound point that has always stuck with me when he was on the podcast that chat is actually a genius interface for building on a super intelligence because it's how we interact with humans of all variety of intelligence. It scales from someone at the lower end to a super smart person. And so, it's really valuable as a way to scale this spectrum. Maybe just talk about that and is chat the long-term interface for ChatGPT, I guess it's called ChatGPT.

Nick Turley (00:34:27):
I feel like we should either drop the chat or drop the GPT at some point because it is a mouthful. We're stuck with the name, but no matter what we do, the product will evolve. I think that I agree that there's something profound about natural language. It just really is the most natural form of communicating to humans and therefore it feels important that you should be communicating with your software in natural language. I think that's different from chat though. I think chat was the simplest way to ship at the time. I'm baffled by how much it took off as a concept. Even more baffled by how many people have copied the paradigm rather than trying out a different way of interacting with AI. I'm still hoping that will happen. So, I think natural language is here to stay, but this idea that it has to be a turn-by-turn chat interaction I think is really limiting.

(00:35:24):
And this is one of the reasons I don't love the super system analogy, even though we used to always use it is because if you think that way, then you kind of feel like you're talking to a person and GPT-5 it's amazing at making great front-end applications. So, I don't see a reason why you wouldn't have AIs that can render their own UI in some way. And you obviously want to make that predictable and feel good. But it feels limiting to me to think of the end-all-be-all interface as a chatbot. It actually kind of feels dystopian almost where I don't want to use all my software through the proxy of some interface. I love being in Figma, I love being in Google Docs. Those are all great products to me and they're not chatbots.

_[570 additional lines trimmed for context budget]_

---

### How to build a high-performing growth team | Adam Fishman (Patreon, Lyft, Imperfect Foods)
**Guest:** Adam Fishman | **Date:** 2022-10-13 | [YouTube](https://www.youtube.com/watch?v=wP8YyWH524A)  

# How to build a high-performing growth team | Adam Fishman (Patreon, Lyft, Imperfect Foods)

## Transcript

Adam Fishman (00:00:00):
Onboarding is the only part of your product experience that a hundred percent of people are ever going to touch. Good luck getting a hundred percent feature adoption of anything else in your product, right? But onboarding is the thing that you have to go through in order to use the product. It's also the first opportunity that you have as a company to deliver on the promise that you made out in the marketplace. So I like to think of your brand is the promise that you're making and your product experience is your delivery of that promise. And those two things have to be in lockstep with each other, or you're going to have mismatched expectations and some really disappointed customers. So this is the first chance that a customer has to be really excited or really disappointed in what they thought they were getting. So don't mess that up.

Lenny (00:00:55):
Adam Fishman was the first growth and marketing hire at Lyft where he spent two and a half years leading their growth efforts. Then he went on to lead product and growth at Patreon, where he spent over four years building one of the most successful and lasting created platforms out there. And most recently, he was CPO at Imperfect Foods. Today he spends his time advising companies on product and growth, and he's also doing a lot more writing.

Lenny (00:01:19):
And in this episode we cover three things, his growth competency model, which helps you hire and evaluate growth talent and also get a job as a growth person. We go deep into why onboarding is such an underappreciated growth lever and all of the impact that you can have optimizing your onboarding flow. Adam also shares a super cool framework for choosing which company to work at. Adam is hilarious and he's so full of wisdom and I can't wait for you to hear this episode. With that, I bring you Adam Fishman.

Lenny (00:01:50):
This episode is brought to you by Coda. Coda's an all-in one doc that combines the best of documents, spreadsheets, and apps in one place. I actually use Coda every single day. It's my home base for organizing my newsletter writing, it's where I plan my content calendar, capture my research, and write the first drafts of each and every post. It's also where I curate my private knowledge repository for paid newsletter subscribers, and it's also how I manage the workflow for this very podcast.

Lenny (00:02:17):
Over the years, I've seen Coda evolve from being a tool that makes teams more productive to one that also helps bring the best practices across the tech industry to life with an incredibly rich collection of templates and guides in the Coda Doc Gallery, including resources from many guests on this podcast, including Shreyas, GoCool, and Shishir, the CEO of Coda.

Lenny (00:02:38):
Some of the best teams out there like Pinterest, Spotify, Square and Uber, use Coda to run effectively and have published their templates for anyone to use. If you're ping ponging between lots of documents and spreadsheets, make your life better and start using Coda, You can take advantage of a special limited time offer just for startups. Head over to coda.io/lenny to sign up and get a thousand dollar credit on your first statement. That's C-O-D-A.I-O/lenny to sign up and get a thousand dollars in credit on your account.

Lenny (00:03:16):
This episode is brought to you by Linear. Let's be honest, the issue tracker that you're using today isn't very helpful. Why is it that it always seems to be working against you instead of working for you? Why does it feel like such a chore to use? Well, Linear is different. It's incredibly fast, beautifully designed, and it comes with powerful workflows that streamline your entire product development process from issue tracking all the way to managing product roadmaps. Linear is designed for the way modern software teams work. What users love about Linear are the powerful keyboard shortcuts, efficient GitHub integrations, cycles that actually create progress, and built in project updates that keep everyone in sync. In short, it just works. Linear is the default tool of choice among startups, and it powers a wide range of large established companies such as Versal, Retool, and Cash App. See for yourself by product teams describe using Linear as magical. Visit linear.app/lenny to try Linear for free with your team and get 25% off when you upgrade. That's linear.app/lenny.

Lenny (00:04:26):
Adam Fishman, welcome to the podcast.

Adam Fishman (00:04:30):
Thanks for having me, Lenny. Super excited to be here. It's a pleasure to be on chatting with you today.

Lenny (00:04:36):
It's even more my pleasure. Thank you for being here. To set a little context for listeners that don't know too much about you yet, can you spend 45 seconds giving us a little overview of all of the wonderful things that you've done in your career?

Adam Fishman (00:04:50):
Sure. I'm setting my timer for 45 seconds.

Lenny (00:04:53):
And, start.

Adam Fishman (00:04:55):
So I've been the Chief Product Officer and VP of Growth and Product at a bunch of different companies Imperfect Foods, Patreon, Lyft, the precursor to Lyft which was Zimride to name a few. Now, I do three things primarily, I'm an EIR at Reforge and a program partner, which means I create courses. I run an advisory practice on growth and product strategy, which keeps me pretty busy, and then I recently started a newsletter which was pretty much inspired by your work in this area over the last several years. Those are the three pillars of my life these days, my professional life at least.

Lenny (00:05:31):
Nailed it. Wow, that was very contained and clear. Let's plug the newsletter real quick. What's the URL? Is it fishmanafnewsletter.com?

Adam Fishman (00:05:39):
It is fishmanafnewsletter.com. I am blessed with great initials that allow me to have that name for a newsletter.

Lenny (00:05:47):
It's not obvious. Adam Fishman AF. There you go. That's the acronym. And then in terms of the advising, just to set this expectation, are you looking for more clients or are you capped out? How should people think about that as they listen?

Adam Fishman (00:05:58):
Well, I'm pretty busy, but it is a bit of a revolving door, so there's always some pipeline. So even if I can't work with folks right now, sometimes it makes sense for me to work with them a few months down the road. So always open to new interest and learning about new companies.

Lenny (00:06:12):
All right, great. We'll share how to contact you at the end of this and it will be in the show notes.

Adam Fishman (00:06:15):
Awesome.

Lenny (00:06:17):
One question I wanted to ask off the bat is, Lyft, you were there super early. I imagine it was an incredibly wild ride. I'm curious, what's the most tangible memory you have of your time at Lyft?

Adam Fishman (00:06:29):
I have a ton of memories from the almost three years that I was there, but I think the biggest one, and probably the most tactile or memorable thing was when we launched Lyft, when we were bringing it out of private beta, we had this press event at the office, which is in Soma, and it had these doors, these big huge garage style doors. And so we opened them up and we actually drove a car into the office with a pink mustache on the front of it, and then a bunch of drivers piled out of it and met the press members and were high fiving people and stuff like that. And then we also served a giant pink mustache cake at this event. So of all of the things there, that one sticks in my head the most.

Lenny (00:07:14):
I love that. I hope there was no hair in this mustache cake.

Adam Fishman (00:07:18):
No, no, there was not. It was delicious.

Lenny (00:07:21):
Okay, awesome. Another question I always have for folks that worked at Lyft or at Uber is how they feel about Super Pumped and what it is like watching that, if you've seen it, what it's like watching the story of Uber and especially working for Lyft.

Adam Fishman (00:07:35):
Yeah, I have seen it. I actually read the book first. I've been a pretty big fan of Mike Isaac, who's the New York Times reporter who wrote it. So I've been a big fan of his reporting for a long time. I connected with him a bit while he was writing the book, and I would say, there were parts of it that were painful. It was like renavigating, relitigating a history that I had lived through already. And so much of that story was stuff that I had experienced pretty regularly while I was at Lyft battling against Uber.

Adam Fishman (00:08:04):
So one part of me was like, "I remember that. God, that was hard." Another part of me was happy to see that all of the things were out in the open finally, things that were hard to talk about about competitive practices and stuff like that and just shady things. And some part of me was happy that it was out there. And then one other thing separately, so I actually was interviewing with Joseph Gordon-Levitt to be an advisor with his company HitRecord while he was filming Super Bumped.

Adam Fishman (00:08:35):
And we had this conversation over Zoom when he was on break in between scenes of filming the show. And for those who don't know, he's the guy that plays Travis Kalanick, the CEO and founder of Uber. So he did this interview with me and he was in his trailer on set, fully decked out as TK in his look with his hair slicked back. And it was really funny and also cool to see him in this context. And he is probably the polar opposite from Travis Kalanick. Obviously they're both super successful, but JGL is a really down to earth, nice, very family-oriented calm guy. So anyways, this is a funny story of my time meeting and interviewing with him.

Lenny (00:09:19):
That is amazing. Did you slip in a couple Lyft facts to help Lyft look a little happier and better?

Adam Fishman (00:09:26):
I did. We talked a bit about that journey and I asked him some questions about how he prepared for the role and stuff like that. And so yeah, it was pretty fun. We swapped some stories and stuff. So yeah, it was neat.

Lenny (00:09:37):
Fun fact, my mom wanted me to dress like Joseph Gordon-Levitt and I was like, "I should look like this guy. Wear some vests. This guy's looking good. Looks like you a little bit."

Adam Fishman (00:09:48):
I see that. I do see that. That's awesome. Way to go, mom.

Lenny (00:09:51):
Yeah. Then I'll follow her advice. Maybe I should have.

Adam Fishman (00:09:54):
It didn't work out so well.

Lenny (00:09:56):
Yeah. Okay. So transitioning to the meat of our chat, there's three things I wanted to spend our time chatting about. One is your growth competency model, which essentially is this post you wrote recently that tells founders how to hire and how to evaluate growth people. Two, I want to chat about onboarding flows. You have a lot of really interesting experience optimizing onboarding flows and the impact that's had. And then three is how to choose a company to work out. You've chosen a lot of really interesting companies and you have some cool insights around how to think about that. Does that sound good?

Adam Fishman (00:10:27):
Yeah, it sounds great. I've also chosen some bad companies in my time too, so we can talk about the good, the bad and the ugly.

Lenny (00:10:32):
Okay, excited to hear about that. So first we talked about this briefly. You've started to do a lot more writing. You have this newsletter at fishmanafnewsletter.com, and you recently wrote this really epic piece called the Growth Competency Model, which essentially lays out what to look for in a growth leader, how to hire, how to evaluate, what to concentrate skills on? Things like that. First question, just what made you feel like you had to write this? What mistakes have you found founders make when hiring and evaluating growth leaders?

Adam Fishman (00:10:59):
Yeah, so first I'm going to put up my new background here, which is my growth competency model background. For those watching on YouTube, I have a funny graphic in the background for this. So I guess the reason that I wrote this, there were a few reasons that I wrote it. One is it really felt necessary to write it. And at first it's, I don't think I've ever come across this type of canonical reference before around how to hire and plan for growth. So that's number one. I didn't think it existed. It needed to exist.

Adam Fishman (00:11:32):
Two, as an advisor and someone who has held the head of growth role many times, I get asked this question a lot. I get asked all the time about how do you hire growth people? And it's usually something like, "Hey, how do we find you but 10 years earlier in your career?" And it feels like that question is missing the first principles approach to hiring a growth person. And if you're doing that, you're just pattern matching to me. And I may not be the best person, right, or somebody who looks like me may not be the best person. So I think we're missing that.

Adam Fishman (00:12:09):
And then third, one of the programs that I'm currently creating is around growth leadership. And so I've been thinking a lot about what it takes to be an exceptional growth leader, how you hire other people onto your team and create balanced teams. And this is going to be a really important component of that program. So it was just really top of mind. So yeah, that's why I wrote it.

Lenny (00:12:30):
Let me ask one quick question and I think you were about to start answering one mistakes founders make, but I guess it's less of a question. I know this was built on another model for product management, which is really awesome. And I love that there's this growing fountain of career ladders, competencies per function. And so I don't know if there's been one that's really great for design and engineering, but I love that this is happening. And so first of all, I just want to say I appreciate that you put in the time to think this through.

Adam Fishman (00:12:54):
Cool. Thank you. Yeah, and I do think we need more of these. I think we need one for marketers, I think we need one for engineers, designers, research probably, there's probably a bunch of functions that could benefit from some canonicalized resource around this stuff. So yeah, some myths and mistakes that I think founders made and that I've made. I've definitely lived through some mistakes. A few experiences where I was hired into a company where the founder had really messed up expectations of what a growth person should be doing, what I should be doing. And I wanted to set the record straight for founders because they are the folks who hire for this and other leaders.

Adam Fishman (00:13:34):
So one example that comes to mind is, I did a very brief stint at a company called Wyzant, in between Lyft and Patreon. And the founders of that company, so Wyzant was a tutoring marketplace. It was acquired by a company not too long ago, so it's now part of a bigger learning company. And the founders of that company we're looking for what I would call a silver bullet strategy to their growth challenges. And what they really needed, and I was naive and didn't think about asking these questions or about evaluating this properly, what they really needed was to create a strategy to add on new growth loops and a system for how to execute against that strategy.

Adam Fishman (00:14:13):
If we had talked about that as part of evaluating each other, as part of them evaluating me, it would've given me a lot of confidence that it was the right hire to make. But the problem was they didn't approach it that way. They didn't ask me those questions and I was too naive to recognize that, "Hey, they're not actually evaluating this with the competencies that you would expect for a growth leader." Executing a growth strategy takes time and patience, and they didn't really have it. And overall it ended rather unsuccessfully. I helped them build this office in San Francisco, hired a bunch of folks, we actually closed down the office. I had to lay off a bunch of the people that I'd hired, including myself, which wasn't particularly fun. And I think all of it comes back to that they didn't have a really strong set of criteria on what it meant to hire a great growth person. And so if I can help even just a little bit with founders making that decision better, it'll lead to fewer mismatches I think in the hiring process.

Lenny (00:15:11):
Awesome. Let's get into it. What are the components of this model? And you can put it back up again for folks to follow along in the background.

Adam Fishman (00:15:18):
It's coming back.

Lenny (00:15:18):
It's like a TikTok.

Adam Fishman (00:15:19):
The whole thing looks like a big circle and there's four wedges in that circle, four equal parts. And I think that answer of what are the components, what should people look for, really, I'll give the PM answer, which is it depends. And what does it depend on? It depends on the state of the team and where the skill gaps are. So the goal of the competency model is not to find a unicorn human being that is an 11 out of 10 on every one of these things, because frankly, that person doesn't exist. I am not an 11 out of 10 on these skills and I have been a growth executive at many companies. I consider myself to be pretty good at the job. The goal is to create a well-rounded team so that you're hiring and balancing skills across your team and that you don't have any gaps in your portfolio.

Adam Fishman (00:16:06):
And so when you think about it, there are four main components to the growth competency model. And the four big components or buckets are growth execution, customer knowledge, growth strategy, and then the last one is communication and influence. And each of those has three really specific skills. And so to give an example, let's break down growth execution for example, which is one of the very first ones that you should be good at if you're a growth practitioner. Within this competency grouping, there's three individual competencies that are channel fluency, experimentation, and what I'd call productizing learnings. And so you can evaluate and ask questions to understand how good people are at these different things and what their experience has been with them.

Adam Fishman (00:16:54):
So in productizing learnings for example, one of the key things that a growth person has to do is generate hypotheses, learn from those hypotheses and then translate that into changes that they're making in the product. And you want to find somebody who has a track record of at least understanding how to take something they have learned that might be an experiment or something that's very NVP and turn that into something that has hooks into different areas of the product. And so that's a critical skill to evaluate. And one of an example, the overall set of competencies.

Lenny (00:17:26):
To throw in a question real quick, is one way to use this model, coming back to your Wyzant example where there's of course misalignment between you and the founder to sit down and look at this circle here and be like, "Here's where I'm strong, here's where maybe we need to hire someone to take over."

Adam Fishman (00:17:41):
Yeah, absolutely. You can almost use it as a reverse interviewing characteristic and say, "What are the competencies you, founder, CEO, care about? What's really important to you for the first person in this role? Or the expectations that you have of what I will bring to the table." And then you can be really candid about what you're good at and what you're not good at, where you're developing. So yeah, I agree with that for sure.

Lenny (00:18:06):
Awesome. I know this also plays into evaluating your performance and it could be like, "Hey Adam, here's the three things that we want to focus on in the next cycle."

Adam Fishman (00:18:13):
For sure. Yeah. It's really a good foundational framework for being really concrete and specific in the feedback that you provide someone. So rather than saying, "Adam, you really need to be more strategic." Everybody's heard that at some point and it's like, "Well, what do you mean?" You can say, "Hey, we need to work on the area of growth strategy and that means I need to see better modeling of loops out of you, better understanding communication of what those loops are." And you can give really concrete feedback.

Lenny (00:18:44):
Awesome. So we've talked about growth execution, What's next?

Adam Fishman (00:18:47):
The next one is what I would call customer knowledge. And within customer knowledge there's a set of things and really this is about data fluency and instrumentation, understanding user psychology, which is a big one, and then this idea of being able to experiment and learn over time and how you develop your narrative and your creative approach to talking to customers. I love user psychology and it's one of the things that I think attracted me to growth and to product. I studied consumer psychology when I was in college and I'm just probably would've been a psychologist if not for a growth practitioner, which is a weird other approach I could have taken in my career. But-

Lenny (00:19:35):
It's not too late to pivot.

Adam Fishman (00:19:37):
... Second career. Yeah, actually, announcing today I'm pivoting the newsletter to just talk about therapy and psychology.

Lenny (00:19:43):
Yes. This is going to be the title of the podcast.

Adam Fishman (00:19:47):
Awesome. So like user psychology, I think one of the things that people don't easily understand is that most people come to your product with an emotional frame and a lot of people want to appeal to their logical brain right away. And the reality is, don't do that. And so you have to understand the mindset that people are in. How low are their lows? How high are their highs? Why are they seeking out your product? And it often, as I mentioned, stems from some emotional challenge that they're having or something that they're seeking. And it's not because your product is the best chat bot or because it gives the best answers. There's a different thing that they're trying to solve. And only when you address that can you move on to those other things. So that's the second competency is, customer knowledge.

Lenny (00:20:34):
It reminds me of something I mentioned on some podcast recently, this tweet that one of the Collison brothers tweeted about how user research is often misunderstood in that people think you do user research and that informs what you build. And instead the way you think about it is user research informs your model of your customer and that model informs what you build.

Adam Fishman (00:20:56):
Exactly.

Lenny (00:20:56):
And the more user research you do, the more your model evolves to help you understand the customer. And that's been really helpful for me because I never thought of it that way.

Adam Fishman (00:21:04):
Yeah. I think that's a great example. Seeking the solution in a customer interview is never going to work, right? You seek to understand and then you take that away and you can think through what the model should look like and how things should change as a result. Yeah. That's great. I can cover the third and fourth competency-

Lenny (00:21:24):
That's good.

Adam Fishman (00:21:25):
... and I'll try to be a little more brief on that.

Lenny (00:21:27):
No, no. It's good.

Adam Fishman (00:21:28):
The third competency is growth strategy. And now the third and fourth competencies I would say are a bit more advanced topics. And again, remember when I said people index higher or lower on these as they go across their career, I would expect a more junior growth person to not necessarily be a 10 out of 10 on strategy and communication. These are things that you really have to work at a long time. They tend to be softer skills in a lot of cases and you learn best by either getting it very right or getting it very wrong. And so you've got to put in the time to develop these. But in growth strategy, there's three. The first one is growth loop modeling. So really understanding how you grow and where you should be spending your time. What acquires users? What retains them? What monetizes them?

Adam Fishman (00:22:18):
The second one is capital allocation and forecasting. This is basically where are you deploying either your money, your people, how are you managing the portfolio and how are you projecting that out into the future? And that's a really hard one. You have to become best buddies with your finance friends in order to do that. And so anyways, that's capital allocation and forecasting. And then the third one is prioritization and road mapping. So you have to be able to sequence the work much like in building a product, right? Building your growth strategy, you have to sequence it. That sequence has to make sense based on your growth models. You have to have the capital or the people to allocate to that stuff. And you have to be able to build a series of hypotheses in a series of solutions to test against those hypotheses so that you can learn and then productize to those learnings.

Adam Fishman (00:23:09):
And there's a lot of ways to test this stuff. Situational, behavioral, interviewing, I talk a lot about this in my post on hiring growth. There's like very specific questions you can ask to understand what people have done or what they might do in different situations. There's case studies and things like that. So that's the third one. And then the fourth one is communication and influence.

Adam Fishman (00:23:29):
And I would say, I think you probably know this as a PM, influence is one of the biggest and hardest skills to develop. It's no different in a growth practitioner and in some cases it can even be harder because sometimes people come in with a preconceived notion of what growth is and isn't and you have to change their mind. And so within communication and influence, there's strategic communication. So how do a series of experiments and a series of things that you're trying, fit into the overall picture of a bigger bet that you're taking?

Adam Fishman (00:24:06):
How do you lead a team? So team leadership is a big part of communication and influence. And then how do you manage stakeholders? And this is hard in growth because often growth can be viewed as at odds with really thoughtful and quality craftsmanship and product building, but it's not. Those things go hand in hand. And so you really have to win people over on what it means to do growth. And I would say, this is the pinnacle, right? This is the hardest one. It's a lifelong journey. It relies very extensively on understanding people and who you're talking to. And those people can be very different from company to company and the currency that they trade in can be really different. And so this one is one where you have to go back to square one and relearn who the people are that you're working with every time you step into a new role or a new company or get a new leader or something like that. So it can be really challenging and it's never done.

Lenny (00:25:03):
That component of the competency models, we're the PM of the growth PM role feels like if you were to do these Venn diagram of the two roles, feels like that's a big part of the overlap.

Adam Fishman (00:25:13):
Yep, absolutely. There's a lot of overlap between exceptional product managers and exceptional growth practitioners. I think sometimes they just use the skills in different ways. So you need to be great at product strategy to be a product leader, you need to be great at growth strategy to be a growth leader. Just what those things look like can be different between the two. But they're both required skills, required competencies to be exceptional at the role.

Lenny (00:25:38):
So just to rehash, the four categories are communication influence, growth execution, customer knowledge and growth strategy. You mentioned earlier that people often come to you and they're like, "How do we find another Adam?" And your feedback is one, how do you know that's exactly what you need? And then two, find someone that's up and coming more because it's so hard to find folks like you that are actually ready to take on a head of growth role.

Adam Fishman (00:26:03):
Yeah.

Lenny (00:26:03):
So a question for you is, one is, do you recommend founders focus on finding someone that's more up and coming and fast learning with a high trajectory? And if so, which of these components would you suggest they look for most?

Adam Fishman (00:26:19):
One of the challenges of hiring a senior person is that they're all off writing newsletters and making podcast.

Lenny (00:26:25):
Right.

Adam Fishman (00:26:25):
Not that we know anybody who does that or anything, but-

Lenny (00:26:28):
No. That's bullshit. What is that?

Adam Fishman (00:26:29):
Yeah. So I don't want to right now go back and work at a company again. I'm enjoying what I'm doing and so it's hard to get me, right? But also you don't want me, I think, when you're doing this for the first time. So I think the key there is smart and driven people for sure. Age and youth is a bit relative I'd say. So I would think about hiring somebody who you might say is less experienced in growth, but they could be very senior in another aspect of their career. You can also benefit from hiring somebody internally who wants to branch out into growth. And one of the benefits that you get from that is a lot of time they have one of those competencies really nailed already. Could be like customer knowledge. They might understand your customer base super well because they're already inside the company.

Adam Fishman (00:27:15):
So if you're going to hire a junior person, the key is, how do you help them learn? And I think if you don't know what you're doing as the founder or the leader, it's hard. You're not going to be able to help them yourself. So you've got to be willing to invest in advisors like me, outside education, mentorship, coaching, otherwise, what's going to happen is that very driven, hungry and enthusiastic person is going to run through a lot of brick walls, which is great, except they're going to miss the fact that the door was standing right next to the wall that they just ran through and you're going to end up with a lot of bricks on the ground.

Lenny (00:27:54):
Excellent metaphor.

Adam Fishman (00:27:56):
Yeah, thank you. And so I have some examples of both hiring people and moving people over in my career. So I think tangible examples are really helpful here. So when I was at Lyft, I had the privilege of hiring that young, smart and driven person a bunch of times, but one of the people who comes to mind is a guy named Ben Lauzier. You should have him on the podcast at some point. He's French, he's very nice to listen to you. He was most recently a VP of product at Thumbtack, but he was definitely not that when I hired him, he was a jack of all trades marketer working at a corporate catering startup when I hired him at Lyft. And he had nailed most of the growth execution and the customer knowledge competencies.

Adam Fishman (00:28:38):
One was, he was an avid Lyft user himself, so he really understood the product really well. And also, he was just very thoughtful about how he executed, how he experimented. He had done a lot of it and he was very skilled at pulling them off in a very small environment because he had just had to be. And those are the two most important competencies when you're bringing in a younger, earlier stage person because teaching somebody how to execute is not that fun, right? That's a hard skill. The ability to execute is something like executive function skills and things like that. If you haven't developed those by the time you are an adult, I don't know that I'm going to be able to teach you and I'm certainly probably not the right person to teach you. You should definitely get some outside assistance with that. And so I like to get those types of people that have those things if I'm hiring that generally inexperienced growth practitioner.

Lenny (00:29:42):
And the two things you said are customer knowledge and execution. Is that the two you said?

Adam Fishman (00:29:46):
Yes. Customer knowledge and growth execution. And so let me tell you an execution story about Ben really quick. This comes also down to work ethic too. So we interviewed Ben and it was a long day of interviews in a tiny glass conference room and Lyft was, I don't know, 30 people or something at the time. At one of the interview breaks, Ben had come over and he didn't have his computer and anything like that. And so he was ducking out of work to do the interview. And he actually came out of the office and he's like, "Hey, does anybody have a computer that I can borrow?" And so we gave him this old crappy Chromebook and I was like, "Why do you need a computer?" And he's like, "I have to go edit this file and do this thing and check something in at work and deploy this thing or whatever."

Adam Fishman (00:30:32):
And I'm like, "Wow, this is a guy that really knows how to get stuff done and is so much so that he's pausing his interview process so that he could go get a thing done for his company that he's actively trying to leave, but he's got this work ethic and this execution." And that really stuck with me. And he probably would remember this story if we brought it up. So the time he borrowed a crappy Chromebook laptop to do work at the job that he was trying to leave.

Lenny (00:31:00):
What are the chances that was staged?

Adam Fishman (00:31:01):
He seemed very innocent. So I think not staged.

Lenny (00:31:06):
Okay, great.

Adam Fishman (00:31:06):
But I could be wrong. But now everyone that interviews that is going to ask to borrow a laptop and-

Lenny (00:31:10):
That's right.

Adam Fishman (00:31:10):
... check in some work.

Lenny (00:31:11):
The phone call.

Adam Fishman (00:31:13):
Yeah.

Lenny (00:31:13):
"Adam, I need to fix something."

Adam Fishman (00:31:15):
That's right. So that's Ben. And then on the other side, and I'll cover this one really quickly, and the other side of moving somebody over internally, I had equally great success with that. So there was another person named Sean at Patreon who was a marketer, again, a jack of all trades marketer. And I think I have a propensity and a bias towards folks like that because that's one of the ways that I started my career was in marketing. But anyways, I moved him over from marketing to being a growth PM, an entry level growth PM.

Adam Fishman (00:31:47):
And he had a ton of the execution skills, ability to prioritize really well and customer knowledge skills as well, more so on the customer side because he was an internal transfer, so he really knew our customers. He'd spent tons of time talking to people, studying data and things like that. And he had really great fluency with data and a drive to continue improving his ability to access and pull out insights and information. And so he turned out to be really great, but just a very different profile of a person than Ben was. And here's an internal hire versus an external hire.

_[222 additional lines trimmed for context budget]_

---

### When to invest in new acquisition channels | Adam Grenier (Uber, MasterClass)
**Guest:** Adam Grenier | **Date:** 2022-09-15 | [YouTube](https://www.youtube.com/watch?v=-PDsvl2WCZU)  

# When to invest in new acquisition channels | Adam Grenier (Uber, MasterClass)

## Transcript

Adam Grenier (00:00:00):
One of the biggest pieces of advice I'm giving to people that are like, "How should we adjust our marketing with the economic changes and things like that?" I was like, "Start by assuming you no longer have product market fit, because you had product market fit in a different market." It's a different market now, so you have to start over. And hopefully you do, or it's pretty close to it and you just have to adjust a couple things, and you could be right back on track. But if you just assume you need to launch a new channel to fix this problem, you're going to be wrong, because your entire customer base changed, not just the next 10% of customers that you're looking for.

Lenny (00:00:34):
Welcome to Lenny's Podcast. I'm Lenny and my goal here is to help you get better at the craft of building and growing products. I interview world class product leaders and growth experts to learn from their hard won experiences building and scaling today's most successful companies. Today, my guest is Adam Grenier. Adam was head of Growth Marketing and Innovation at Uber where he basically built their growth marketing infrastructure and the team from the ground up. Then he went on to VP of Product and Marketing at Lambda School, and most recently he was VP of Marketing at Masterclass. These days, Adam advises companies, large and small, on growth and marketing strategy.

Lenny (00:01:10):
In our conversation, we cover how to decide when to try new and emerging acquisition channels like TikTok, VR, newsletter ads, and how to go about testing them out. We get into the growth CMO role, which is an emerging role that Adam has helped pioneer, and we get into some real talk about burnout and depression and mental health issues that often come with working in tech. This was a really powerful and insightful conversation and I learned a lot from Adam both as an operator and as a human. I can't wait for you to hear this episode. And so with that, I bring you Adam Grenier.

Lenny (00:01:48):
This episode is brought to you by Whimsical. When I ask product managers and designers on Twitter what software they use most, Whimsical is always one of the most mentioned products, and the users are fanatical. Whimsical is built for collaborative thinking combining visual, text and data canvases into one fluid medium. Distributed teams use Whimsical for workshops, white boarding, wire frames, user flows, and even feature specs, and that includes thousands of built-in icons and a rich library of templates. See why product teams at leading companies call Whimsical a game changer. Visit whimsical.com/lenny to have my own templates added to your account when you sign up. That's whimsical.com/lenny.

Lenny (00:02:34):
This episode is brought to you by Coda. Coda's an all-in-one doc that combines the best of documents, spreadsheets, and apps in one place. I actually use Coda every single day. It's my home base for organizing my newsletter writing, it's where I plan my content calendar, capture my research, and write the first drafts of each and every post. It's also where I curate my private knowledge repository for paid newsletter subscribers, and it's also how I manage the workflow for this very podcast. Over the years I've seen Coda evolve from being a tool that makes teams more productive, to one that also helps bring the best practices across the tech industry to life, with an incredibly rich collection of templates and guides in the Coda Dock Gallery, including resources from many guests on this podcast, including Shreyas, Gokul, and Shishir, the CEO of Coda.

Lenny (00:03:23):
Some of the best teams out there like Pinterest, Spotify, Square, and Uber use Coda to run effectively, and have published their templates for anyone to use. If you're ping ponging between lots of documents and spreadsheets, make your life better and start using Coda. You can take advantage of a special limited time offer just for startups. Head over to coda.io/lenny to sign up and get a $1,000 credit on your first statement. That's C-O-D-A.I-O/lenny to sign up and get a $1,000 in credit on your account.

Lenny (00:04:02):
Adam, welcome to the podcast.

Adam Grenier (00:04:04):
Thank you. Thanks for having me.

Lenny (00:04:06):
It's my pleasure. I'm really excited to chat. So, I'm going to give a very brief overview of your very impressive career, and just let me know if I missed anything.

Adam Grenier (00:04:15):
All right.

Lenny (00:04:15):
Sound good?

Adam Grenier (00:04:15):
Yeah.

Lenny (00:04:17):
Okay, so you were most recently VP of Marketing at Masterclass, which I'm actually a happy subscriber of and I've watched many a video. Before that, you were VP of Product and Marketing at Lambda School. I don't know if that's right before, but that was something you did. Also, you were head of Growth Marketing and Innovation at Uber, which is a really cool title. And I think you spent four years there and you basically built their growth marketing infrastructure and the team. And currently you're doing a bunch of advising and exploring to see what you want to do next. Is that about right?

Adam Grenier (00:04:50):
Yeah, you hit most of the key points. I think pre-Uber, the first chunk of my career was on the advertising side, so worked in agency world. So, I kind of think of this as phase three of my life. Ads world was phase one, startup and growth world phase two, and now really just spending time helping entrepreneurs and founders and built companies and that type of stuff.

Lenny (00:05:16):
What's been your favorite phase so far?

Adam Grenier (00:05:18):
I mean all of them. I just embrace what gets thrown at me and allow it to organically happen. So, each phase has had its pros and cons and ups and downs, and so I think they've all fit pretty well into where I was in my career.

Lenny (00:05:34):
Speaking of moving and adjusting and iterating, I know you're big into improv. How serious are you about improv?

Adam Grenier (00:05:44):
Good question. Serious in the sense that I've done it for a very long time and I still do it and I try to do it regularly. Serious as in am I aiming to make money off of it and have a career out of it? Unfortunately not. There was a point in my life that that is what I wanted to do. I lived in Chicago, did Second City ImprovOlympic, a variety of different places, did quite a bit of performing, but also got into corporate paychecks early as well.

Adam Grenier (00:06:12):
And so kind of built a lifestyle that made doing improv full time probably not the best path for me at the time. And so made a pretty conscious choice early on that it was more of a hobby and if something ever came of it, cool, but if not, that's okay. It's something I keep coming back to, because it's very grounding and fulfilling in ways that work and family life and things like that don't quite hit for me.

Lenny (00:06:34):
Actually at Airbnb we had an improv teacher come and work with the PM team. It was for months. We did improv games once a week.

Adam Grenier (00:06:42):
Nice.

Lenny (00:06:43):
And played all these fun things. And I'm curious what you've taken away from improv that has helped you become better at your work?

Adam Grenier (00:06:51):
So, I think generally the whole suite of skills that you develop in improv are pretty applicable, right? Because you are getting comfortable on your feet with change, with teamwork, building off of each other, experimenting, trying new things, a little bit of everything. I think a couple of the key rules or themes of improv that I really try to hammer home with people are obviously the "Yes, and..." side of improv, which everyone's probably heard, which is in a scene the worst thing you could do is deny somebody, because you're actually just stopping progress and you're not building off of anything.

Adam Grenier (00:07:27):
So, the appropriate approach is to say, "Yes, that is true, and..." and add to it. So, if someone's like, "Hey, you have a chicken on your head," not saying, "No, I don't." Just kind of ruins that scene. Versus saying, "Yes, I do, and it's name is Sally. What's your chicken's name?" Builds on that and gives it more opportunity. And so I think that in growth in business is super important to be able to say, "Yes, I do see your idea," or "Yes, we did accomplish this and this is what we want to do next and this is how it's going to build on it," I think is super important.

Adam Grenier (00:07:59):
The other one that I think is less known or talked about is the gift of details. So, in a scene, if you give somebody really specific details about something, it gives so much more meat to be able to work off of in terms of what's coming next. So, if someone's starting a scene and they're clearly watching television and clicking through the channels and I walk up and just say like, "Oh, you're watching TV, cool." That's a yes statement. I'm not denying what they did. But if I come up and say like, "Oh cool, you're watching TV. Is that an Alf episode? I haven't seen Alf since I was a kid. It reminds me this one time I actually ate my own cat."

Adam Grenier (00:08:39):
Just giving those specific details of Alf and me as a kid and I had a cat, which if people don't know Alf, he ate cats.

Lenny (00:08:48):
I don't remember that. I remember Alf, but I don't remember he ate cats.

Adam Grenier (00:08:51):
He was always trying to get the family cat. So, those kinds of details add a ton of value and you take that into the business world, to use Masterclass for instance, if I say, "Yeah, Masterclass, we've got this way to build content that is both entertainment and education," that's interesting. But if I say, "We create content that is both education and entertainment to solve people's deep curiosities in the way that maybe a biography would." That just opens up the exact problem that you're trying to solve. What are other alternatives to that problem? How are people consuming that? So, I think the gift of details in good improv and learning those skills is something that I really value and look for in every aspect of my business life as well.

Lenny (00:09:41):
It sounds like it's really helpful, one, in marketing, creativity and positioning, and things like that you just described. Have you found it also to be helpful in collaboration like this "Yes, and..." piece? I'm curious, is there an example or story where you like "Yes, and..." someone? Do you actually say "Yes, and..." in a meeting? How do you actually find that you use it?

Adam Grenier (00:09:58):
I hear it every now and then. I don't usually literally say it. I think one of the areas that I've found it to be valuable is when you've got cross-functional work. So, obviously at Uber we dealt with city teams a lot, and so a lot of the times the way that the central team would scope a problem versus a local team would scope a problem, would almost feel at odds with each other. And if you approach it with that "Yes, and...", it's often still true.

Adam Grenier (00:10:24):
It's like, oh, both of these things can be true at once. You could have a different goal than I have, or you have a system problem local to you that is important to you and it's not important to me. That's okay, both things can exist. So, now that we accept both and can work off of each other, we're more likely to build both a better rapport and energy among ourselves because we're not just saying, "No, no, no, no, you're wrong. That's not true, that's not important to the business. Why are you doing that?" That type of energy when cross-functional work, it just kills the scene, it kills that progress. And then you don't build relationships, you don't build the right solutions, all that type of stuff.

Lenny (00:11:06):
It sounds really good. Everyone in theory wants to be really good at this. And I imagine just doing a bunch of improv is a really good way to get better at not getting defensive and being like, "Yes. And how do we make this idea better?" Is there something you can advise folks to work on this skill, or is it just do a bunch of improv classes and it'll kind of help build that skill?

Adam Grenier (00:11:25):
That's one. I would say that I'll say that all the time to people, "Do some improv classes." And I get a lot of people like, "No, I'm not funny," or, "I don't want to do improv." And I think it's still a really great class to take, even if you have zero interest in doing improv, or public speaking, or any of that kind of stuff. Because, again, improv 101 is taught everywhere. Every city has it somewhere, and it's rarely ever people that are trying to do improv professionally. It's all games, like you said. The classes that you all did at Airbnb is what improv 101 is, right?

Adam Grenier (00:12:00):
It's just like, "Hey, let's just have fun. Let's just get out of our skin," and things like that. So, I do think everybody should take improv classes. I think it's also something with a lot of goals or skills that you want to develop, I think being really public and open about you wanting to develop that. So, if you are managing a team and you want to sharpen the skills, make it a team goal, or have accountability and just say, "Hey guys, I know that I've been pushing back on things lately. I want to really try to embrace and grow off of ideas better, hold me accountable, call me out and be like, 'Adam, "Yes, and..." this please."

Adam Grenier (00:12:32):
Or giving people permission to push back on that when it doesn't happen, I think also just opens the door for more productive conversations with people and the ability to hold yourself accountable and keep trying it.

Lenny (00:12:47):
I love that. And such a good team bonding activity. There's like all these reasons to do this as a team. My wife actually, she's a designer, artist, writer, illustrator kind of person, and she's been taking a lot of these sorts of classes to help inspire her creativity. She never wants to be an improv person, she did stand up classes.

Adam Grenier (00:13:03):
That's awesome.

Lenny (00:13:05):
As you said, it just helps you get the juices flowing along these lines. Okay, so we're not going to talk about improv the whole time.

Adam Grenier (00:13:13):
We could if you want to.

Lenny (00:13:15):
We could. Throw me word, let's go. No, we don't want to do that. So, there's basically three things I really wanted to chat about with you. One is how to decide when to invest in an emerging acquisition channel like TikTok, VR, Clubhouse was a big thing. You have some really interesting thoughts on how to decide and approach this thing. To the growth CMR role, which is kind of this, I think emerging role, something you're really good at, and I just want to get your thoughts on what's happening there. And then, three, some real talk on burnout and depression that often comes with working in tech and stuff that we go through. Does that sound good?

Adam Grenier (00:13:51):
Yeah.

Lenny (00:13:51):
Okay, great. So, to start with in the first topic, if you think about it just every company essentially goes through this kind of S-curve of growth. They start slow, they find something that's working, then hopefully it works out and things start to grow, grow, grow, and then eventually it flattens out and you see this S-curve that happens. And every company is always trying to find the next S-curve to add this layer on the cake that keeps overall growth up while this first growth channel slows.

Lenny (00:14:18):
And so people are always looking for what's the next thing? "Oh man, Clubhouse is coming out. We should get on Clubhouse." "Oh, TikTok is so hot, we've got to run some TikTok ads." And there's always something new, like newsletter ads, I don't know, podcast ads, if that's new. And you have a really interesting framework for how to think about this and make decisions and experiment. So, I'd love to hear insights there.

Adam Grenier (00:14:38):
So, the exploring emerging channels framework that I'll take, either my teams or companies that I'm advising through, has three core ingredients that I like to spend time with. So, the first is really understanding if there is an overlap between what the customers need is, what your company's goals are, and what the channel actually does really well. So, the example I've used in the past is Spotify in the moment of things like Clubhouse and Paparazzi and stuff like that becoming really popular.

Adam Grenier (00:15:14):
Well, for Spotify, they're trying to get more people to consume music and be entertained by music and things like that. And it's all audio driven. And so their growth goals are probably around new customers or deeper engagement with audio. The customer's needs are discovery and more ways to maybe have deeper relationships with their music. If you're a jazz fan, can you learn new jazz artists or more about the artists that you love? Things like that. And then take those two channels. If you take something like Clubhouse, it's audio first, it's almost like live podcast radio type feel to it.

Adam Grenier (00:15:57):
You can get into these rooms with just people with really amazing esoteric knowledge about something. And so its strengths have a really clean overlap to me with the goals of Spotify, the needs of the customer and the strengths of that. And so that to me is great. That is probably a green light in terms of is it even worth our time? Versus Paparazzi is very photo driven and nothing really to do with music or anything like that. And so it's like even though Paparazzi might have become the best biggest channel ever, is that the thing you should be putting your time into? It would be a yellow light for me at best.

Lenny (00:16:36):
And how did you describe that again? It's the medium matches?

Adam Grenier (00:16:40):
Yeah, the strengths of the medium. So, let's take influencer right now. Actually two of the channels that a lot of people are talking about right now are streaming TV or OTT and influencer marketing. And so to me, one of the strengths of influencer marketing is hyper targeted contextual marketing. And so I can go find the five influencers that are hardcore Alf fans, and if I'm marketing Alf something, great, I can go find that specific thing. Whereas OTTs a lot harder to get that specific.

Adam Grenier (00:17:16):
OTT strength is broad reach and video storytelling and that type of stuff. So, it's like, okay, well maybe my medium is if I'm Masterclass and I have a ton of video content and storytelling and things like that, that channel actually makes a ton of sense probably. So, it's like what are the strengths of that channel is something that... that is actually probably the piece I see people ignore the most, which is they just want to know if a channel is hot or not. And this gets especially hairy in this world of a lot of B2B doing more consumer-esque marketing. There's so many B2B companies that just don't apply to emerging consumer channels.

Adam Grenier (00:18:00):
And it's just like, please just stop. I don't need a Notion Clubhouse channel this week. And maybe there's a world to do that. But I think that's kind of number one, is making sure that there's even a reason that you should be there to put it on your radar right now.

Lenny (00:18:19):
Awesome. What does the OTT stand for by the way?

Adam Grenier (00:18:22):
Oh my gosh, you're putting it on the spot.

Lenny (00:18:24):
It's all good.

Adam Grenier (00:18:25):
I'm drawing a blank on it.

Lenny (00:18:26):
But essentially it's a streaming platform?

Adam Grenier (00:18:28):
Over the top. Over the top. So, instead of it being cable TV, it's coming from a box. So, it's primarily if you think of ads on Amazon or Hulu or even if you go to cnn.com and you start streaming and you get an ad first. it's basically video ads, but a lot of them now are happening on televisions and on streaming services rather than just on websites.

Lenny (00:18:55):
Got it. Okay, cool. So, the first is the strength of the channel. You should look at that.

Adam Grenier (00:18:59):
Yep. And how that overlaps with your customer and your business needs. The second is the channel DNA. And so this is looking at things like where are they in their trajectory? So, Clubhouse is actually a perfect example, because in a weird way, so Clubhouse got hot before Facebook got cold. And I was pretty amazed how many more people were trying to crack Clubhouse than TikTok, because TikTok hadn't really released their ads solution yet, but neither had Clubhouse. But everybody was talking about Clubhouse, and TikTok is very clearly not going away anytime soon, where Clubhouse hopefully won't. Like this is an amazing product.

Adam Grenier (00:19:41):
I really enjoyed it and loved it, but it was clearly very early, very quickly at that point of hotness where everyone was just kind of, "That's the reason I should be there." And part of this reason is to accept the risks of going into that channel. So, if I go and dedicate two quarters of work to Clubhouse, I need to accept that they are so early in this curve that there's a good chance this is a once in a lifetime opportunity and it'll be over. It's not a repeatable action. It also is important because if you get something to work on a channel that's earlier in their growth curve, the likelihood that they will change is very high.

Adam Grenier (00:20:23):
You're going to need to commit a lot of cycles to keep it going, because it's like, "Okay, well, their product is going to evolve drastically very quickly over the next two years." And so a really great example is Facebook early... I was at Zoosk. And so Zoozk and companies like Zynga got tons of their early growth because of notifications on Facebook, which was one of their early features, which allowed basically anybody that took any action on Zynga, it would post on everybody else's page that you got 10 carrots, and that was a huge growth lever.

Adam Grenier (00:20:59):
But then Facebook just pulled the plug on that. And so it's like, well, if you put all of your energy into that and that's it, it was pretty clear that that was still an area that's like this may not last forever. The last thing on the channel DNA that I like to look at that's a little bit more, I don't know if odd or unusual is the right term, is I like to spend a lot of time thinking about how they monetize. What is the monetization strategy of the channel? And the reason is because if you, as a business, can match or support their monetization strategy, it actually gives you a really interesting leg up with that channel.

Adam Grenier (00:21:37):
Because the likelihood of you being able to call them up and go do custom stuff with them, or partner with them, or that your solutions will actually stick around for a while, it'd go up pretty drastically. And so my key example of this was with when Facebook started exploring mobile ads, Hotel Tonight, we were one of the alpha testers of mobile ads, because I'd been sitting here buying ad inventory on networks for the last five or six years and just waiting for Facebook to work, because it just wasn't really working for mobile installs.

Adam Grenier (00:22:13):
And it's like, I know this is a huge channel because I can use it on my online marketing, my web marketing, but as a mobile acquisition it's nowhere near as efficient as a lot of these other networks. And so as soon as they were doing that, I was able to basically position and say, "Look it, you want to work with us. Let me into your alpha, because I have five years of experience already buying mobile ads. I know the space. I know it'll work. And if you get us to work, we're a killer case study, because we are a non-game and a lot of money is spent on gaming, but there's these whole other major categories that you're going to need other than gaming examples within that group. So, you're going to be able to use me as a case study and a lot of different scenarios than the gaming players."

Adam Grenier (00:22:59):
And so I was spending a lot less than the gaming players, but because of that understanding that your goal at Facebook is to make ads work for all of travel and for all of leisure and those kinds of things, that's the value of working with me. So, that's another piece of the channel DNA I like people to focus on.

Lenny (00:23:18):
Awesome. That's such a good one, because to your point, if your goals are aligned, they're going to be like, "Yes, let's make this happen." And it always feels like it's this behemoth that doesn't want to talk to any new startups, but if you can make the case of this is going to help you and the way you laid out is so clear, it's such a good idea.

Adam Grenier (00:23:35):
And especially with emerging channels, right? Because their whole thing is that make this work for a long time. It's part of the challenge you see with some new channels flipping to the other side of growing an ads business, will gravitate towards like, "I want to get Disney on here," but Disney is very campaign driven, or they have been traditionally, where it's like you may get one big paycheck from them, but that doesn't... The way that UA driven gaming works is you get that to work, that's a gift that keeps on giving forever, right? Because there's not one of those companies, there's thousands of them and they all do the same thing. So, being able to drive that conversation is really helpful.

Lenny (00:24:18):
Cool.

Adam Grenier (00:24:18):
And then the third main ingredient is just your own company DNA. And so I think risk profile is a big one. Do you actually have comfort in being a first mover, a true first mover? Nobody knows anything, tracking's not going to work, it's not going to be programmatic. You're probably going to show up on content that's offensive. You're probably going to ask for refunds that won't happen. It's going to be really painful to be a true first mover. Do you have that appetite? Do you have the staff to actually be able to put someone on that and it not distract from everything else?

Adam Grenier (00:24:53):
And then the other piece on the company side is just your current channel mix. There's very few companies that I recommend saying, "Yes, go put energy on this brand new channel that you don't know how to scale yet before you've figured out some type of volume on Google and Facebook." Every now and then there may be a perfect fit where it's like, absolutely, you should be the person doing this. But if you're not at least getting something out of the basic channels that everybody else is using, it's probably not the thing you should be putting your first energy into. It should be like, "Great, I've got a good foundation." Like you said, now we're at that stage of trying to add things, tends to be a better stage to do more risky exploration into new channels.

Lenny (00:25:40):
What advice do you have or can you give to founders teams that are trying to test one of these in terms of just how to run these tests? How much time should they spend, would you say? What do they look for? I know that this is a hard question and super dependent on the situation, but any advice there?

Adam Grenier (00:25:55):
So, I think going through those three ingredients should help shape that answer, right? Because if you're like, "Okay, well, the first one is super strong, the channel DNA is maybe really early and I've got a small to mid-size team and maybe only one channel working," then it may be like, "Great. Put half of one person into this, because it's maybe interesting. But don't put any more than that into it." Versus if it's like, "Man, this is a killer fit the channel's a little further along and I have a 20 person team, so I'm going to put three dedicated people to this, because we are in prime position to be the leaders in this new channel and really push it."

Adam Grenier (00:26:36):
So, I think it's figuring those pieces out, because it is a very it depends answer, but rarely ever is it like, "Hey, this should be your entire team's focus for the next three sprints or five sprints." I think that if you've got that half person working on it for a while and there starts to be some magic happening, sure, put a sprint or two against it as a whole team. But generally speaking, I think keeping it minimum at first is my typical recommendation.

Lenny (00:27:06):
I had another guest, Yuri, who I think from former Grammarly, and he made a really good point that it's often better not to try something than to do it badly and then take away the wrong lessons. I guess, in your experience, what's a timeframe you think people should put into this stuff? You said two sprints, maybe a couple weeks. I don't know, what's the range of just maybe don't spend more than X months on something new if it's not clearly working, just based on your experience.

Adam Grenier (00:27:33):
Generally I wouldn't let anything bleed past a quarter. You can probably get some good signal in a month or less, what I would call fishing. It wouldn't be like you're just putting bait in the water to figure out where the fish are, not necessarily getting statistically significant repeatable solutions. The big variables that can change that timing, so if I'm exploring a new video channel, the content I need to create is if I'm going to have to create something that takes three weeks to produce and $20,000 to make, I may want to give it a little more time, because I gave it more of an upfront investment.

Adam Grenier (00:28:16):
Versus if it's like I want to put text ads in podcasts listings or something like that. It's like, great, I can do that by myself at midnight and it's not distracting anybody or anything. And if it doesn't work in three weeks, let's move on. But generally ideally what you're working through, and we'll touch a little bit more on this with the Growth CMO, is that this should all be part of a roadmap. It shouldn't just be randomly chosen and thrown at. This should be part of your sprint process and you should have a backlog of other things that you want to try. And so you're actually weighing that decision of how long based on what other opportunities you're missing out on by investing in that.

Adam Grenier (00:28:57):
But I would say most channels, especially new ones, are going to take more than a couple cycles to kind of suss out. Because there's no rules, there's no playbooks yet on how to do them well. So, give it a little bit of time, but if you're going over a quarter and you don't feel like directionally it's getting better or it's interesting, I would put it back on ice for a while.

Lenny (00:29:18):
Cool. And to your point, you're not going to see any statistically significant answers. Is the thing you look for just like you know it when you see it? Oh wow, qualitatively feels like it's working kind of thing? Is that what you kind of look for?

Adam Grenier (00:29:30):
Yeah, and I think define that going into it. What am I looking for, for this? So, something like Clubhouse I'm probably not going to see clicks. It's more about are we able to start a room and increase the size of that room by 10% every time that we run it? Okay great, that means that we're at least getting better at this and there's more reach available to us. But if we're getting 20 people every time we start a room and then it goes down to 15, then we're either not doing this well, the channel's not doing well, or there's just not enough reach for us to actually expand. Versus TikTok you might be able to say, "Great, I can actually track clicks and conversion, so let's look at it the way we would any other channel."

Lenny (00:30:11):
Got it. So, kind of look for momentum and that you're getting better and that it's moving somewhere. Awesome.

Adam Grenier (00:30:16):
Yeah. Yep.

Lenny (00:30:17):
Cool. So, a question that I'm sure is on many people's minds that they would want to ask is, Adam, what are emerging platforms that are interesting right now that we should experiment with? What do you feel?

Adam Grenier (00:30:28):
So, I mean I mentioned OTT, or basically the key thing with OTT is that it's way more trackable than traditional television, but it has similar value that traditional TV does in terms of the ability to do more long form storytelling type content, and a lot of it's not skippable if you buy it. And so those are reasons to be exploring that right now. It's hard for me to call that emerging channel, because it's been around forever. It's there's more of it and the tools and services around it are way better now than they were four years ago. And so I think the sophistication and ability to scale OTT is much higher now than ever before. Influencers probably the one that I'm most intrigued by, because similar to OTT, the scale and services and the ability to go do it is still there.

Adam Grenier (00:31:19):
It's also got that hyper granularity that when I get into influencer tools, it feels to me like early Facebook when I used to go be able to target Lenny, or 10 people that have exactly the same likes as Lenny. And that type of stuff where it's like you can get so specific and find exactly who you need. It's incredibly tedious and manual and it's a lot of relationship management. So, I'm also keeping an eye on the technology being built around influencer, because I think that's a huge area of opportunity for entrepreneurs right now.

Adam Grenier (00:31:57):
But generally speaking, the scope and opportunity there is huge and it's not going away, but it feels very new and different right now. And it supplements the ability to do some hyper level targeting that you've not been able to do, that Facebook and Google are getting less open about at the same time. I think VR is really interesting in the way that mobile was interesting before iPhone Three, where it was if you've got a VR app, it's a really interesting space, but if you don't, it's not that interesting to me yet. Any that have come up for you that you're like... what are your thoughts on this?

Lenny (00:32:35):
No, these are great. All I think of is TikTok.

Adam Grenier (00:32:38):
I feel like TikTok's crossed a chasm, whereas they actually have a formalized ad platform now, people are finding scale. There's still a ton to do there and influencers is also weird, because it crosses all of these other worlds as well. But I think TikTok is actually hyper interesting and everyone should be doing that. But I think of that less as you should be doing it as should I do it, should I not? And it's more I need to figure out how to do Facebook if I'm at least mildly appropriately should be there.

Adam Grenier (00:33:05):
The podcast ads I think are great. I think that I bought podcast ads 15 years ago, so it doesn't feel like an emerging channel to me. I think there's way more volume now than there's ever been before. One of the guys that was on my team at Uber has a company that's doing programmatic buying and that type of stuff. And so I think there's more opportunities on podcast. I think people want to treat it like Facebook ads or direct response ads, immediate response ads. And actually what I keep seeing as the effective strategy with podcast ads is treating them more like radio where it's more about getting on the right program, making it personal and feel like it should be part of that program, and then repeating over and over and over again.

Adam Grenier (00:33:53):
So, I think podcast is super interesting. I think it's just hard to scale. It's likely not going to get people the same volumes as the Googles and Facebooks of the world.

Lenny (00:34:01):
Cue our mid-roll ad. I'm excited to chat with my friend John Cutler from Podcast sponsor, Amplitude. Hey John.

John Cutler (00:34:08):
Hey, Lenny. Excited to be here.

Lenny (00:34:10):
John, give us a behind the scenes at Amplitude. When most people think of Amplitude, they think of product analytics, but now you're getting into experimentation and even just launched a CDP. What's the thought process there?

John Cutler (00:34:21):
Well, we've always thought of Amplitude as being about supporting the full product loop. Think collect data, inform bets, ship experiments and learn. That's the heart of growth to us. So, the big aha was seen how many customers were using Amplitude to analyze experiments, use segments for outreach and send data to other destinations. Experiment and CDP came out of listening to and observing our customers.

Lenny (00:34:42):
And supporting growth and learning has always been Amplitude's core focus, right?

John Cutler (00:34:46):
Yeah. So, Amplitude tries to meet customers where they are. We just launched starter templates and have a great scholarship program for startups. There's never been a more important time for growth.

Lenny (00:34:55):
Absolutely agree. Thanks for joining us, John, and head to amplitude.com to get started.

Adam Grenier (00:35:01):
I also come from a very consumer perspective. I'm actually stronger on B2B companies using podcasting, because it has that exact same value I just described, but each one of their customers is substantially more valuable. So, they don't need the scale that a consumer application or product would need.

Lenny (00:35:20):
Yep. That's exactly who I work with usually. One last question on this topic. What percentage of the time do you find that an emerging channel works? Is it like 20% of the time, 10%, 5%? What should people estimate, it's probably not going to work but when it does it's going to be game changing?

Adam Grenier (00:35:34):
Like 5% of the time. There's new things popping up all the time. I think the area that I think of as emerging that I've found more success in is taking things that exist already and make them... so the two slices of it are either it's existed for a long... like podcasting. So, it existed for a long time and now we're finally getting to a spot where it feels scalable. The other is existing channels that introduce something brand new.

Adam Grenier (00:36:04):
So, at the mobile ads I described on Facebook, pre mobile install ads and post. Those first 18 months of mobile install felt like an emerging channel, right? Because they were changing the product every week and tracking didn't work, and there were all these funky problems with it even though Facebook had been around forever. But brand spanking new channels, I don't know, they rarely work or are worth the effort early that you hope that they will be.

_[249 additional lines trimmed for context budget]_

---

### Humanizing product development | Adriel Frederick (Reddit, Lyft, Facebook)
**Guest:** Adriel Frederick | **Date:** 2022-10-20 | [YouTube](https://www.youtube.com/watch?v=uMhBej6-Ey4)  

# Humanizing product development | Adriel Frederick (Reddit, Lyft, Facebook)

## Transcript

Adriel Frederick:
There are probably, I call them techno utopians who would say, feed all data to the algorithm, give it an objective, and it will do the right thing. And I was like yeah, the reason that falls down is the algorithms don't understand long term effects often, nor do they understand how people might respond to it, nor do they understand your intent for the product, and I think it's really important for product managers to play that role. That is our job. When you are working on algorithmic heavy products, your job is figuring out what the algorithm should be responsible for, what people are responsible for, and the framework for making decisions.

Lenny:
Welcome to Lenny's Podcast. I'm Lenny, and my goal here is to help you get better at the craft of building and growing products. Today my guest is Adriel Fredrick. Adriel is a VP product at Reddit where he focuses on incubating and scaling new products within Reddit. Before that, he was director of product at Lyft where he led the marketplace teams and the pricing teams over the course of five years, and before that, he was an early PM at Facebook where he spent four years leading the user acquisition team. Adriel is one of these incredible product leaders who's way too under the radar because he doesn't spend all day on Twitter and instead is executing and building great products. One of the goals of this podcast is to highlight incredible product leaders who you may not be aware of. And Adriel is a great example.

Lenny:
In our chat we talk about the origins of growth hacking, how to get better as a product leader, ways to increase diversity at your company, what it was like to work on Facebook's growth team early on, the future of AI and a lot more. It was such a joy chatting with Adriel and I am really excited to share this episode with you. With that, I bring you Adriel Frederick.

Lenny:
This episode is brought to you by Linear. Let's be honest, the issue tracker that you're using today isn't very helpful. Why is it that it always seems to be working against you instead of working for you? Why does it feel like such a chore to use? Well, Linear is different. It's incredibly fast, beautifully designed, and it comes with powerful workflows that streamline your entire product development process from issue tracking all the way to managing product roadmaps. Linear is designed for the way modern software teams work. What users love about Linear are the powerful keyboard shortcuts, efficient GitHub integrations, cycles that actually create progress and built in project updates that keep everyone in sync. In short, it just works. Linear is the default tool of choice among startups and it powers a wide range of large established companies such as Versal, Retool and CashApp. See for yourself why product teams describe using Linear as magical. Visit linear.app/lenny to try Linear for free with your team and get 25% off when you upgrade. That's linear.app/lenny.

Lenny:
Hey Ashley, head of marketing at Flatfile. How many B2B SaaS companies would you estimate need to import CSV files from their customers?

Ashley:
At least 40%.

Lenny:
And how many of them screw that up and what happens when they do?

Ashley:
Well based on our data, about a third of people will consider switching to another company after just one bad experience during onboarding. So if your CSV importer doesn't work right, which is super common considering customer files are chop full of unexpected data and formatting, they'll leave.

Lenny:
I am 0% surprised to hear that. I've consistently seen that improving onboarding is one of the highest leverage opportunities for both signup conversion and increasing long term retention. Getting people to your aha moment more quickly and reliably is so incredibly important.

Ashley:
Totally. It's incredible to see how our customers like Square, Spotify and Zoro are able to grow their businesses on top of Flatfile. It's because flawless data onboarding acts like a catalyst to get them and their customers where they need to go faster.

Lenny:
If you'd like to learn more, get started, check out Flatfile at flat file.com/lenny.

Lenny:
Adriel, welcome to the podcast.

Adriel Frederick:
It's good to be here, Lenny. Thanks for having me, man.

Lenny:
It's absolutely my pleasure. I actually found out about you through a guy named Jules Walter who we both know.

Adriel Frederick:
Yeah.

Lenny:
He's a PM at YouTube and I actually asked him, who should I have on this podcast that is maybe a little bit under the radar that is just amazing and immediately he suggested you. And so I'm really excited to be chatting.

Adriel Frederick:
Man, that is hype praise coming from Jules. Jules is my boy. I love him. He's such a great guy. Awesome product manager and dedicated to the craft that I just like being in his presence.

Lenny:
Yeah, and we're going to get him on this podcast at some point. He's busy with some kind of secretive project that we can't talk about. He's scheduled, I could talk about Jules all day, but he actually has the 10th most popular guest post on my newsletter still. How about that?

Adriel Frederick:
Wow. Yep, he's awesome.

Lenny:
Anyway, enough about Jules. So to give listeners a little bit of context on yourself, can you just give us a 55 second overview of all of the wonderful things that you've done in your career?

Adriel Frederick:
Ooh, we'll do real fast. So big highlight about me is I'm originally from Trinidad and Tobago, an island in the Caribbean. Came to the US for college, double E, got seduced by consulting and did that for a couple years, worked in oil and gas, electric power, heavy industries, loved that stuff, but also liked writing code on the weekends for fun. So I thought I should move into tech and I did. Worked at Intuit and helped develop their first iPhone app, which was a thing back in the day. Worked at a startup growth team at Facebook for four years working on user acquisition, which was really fun and a good kind of strong formative experience. I had. Quick stint in biotech and then worked on marketplace at Lyft. So rider pricing, realtime driver incentives, matching rider with drivers, and then a lot of the operational tools that we use to manage our marketplace. And so that's a bit of my journey in maybe 45 seconds.

Lenny:
That was great. I don't have a timer for these, but that sounded right. So we're going to talk about a lot of the things that you've learned along the way at all those places. Can you also share what you do now?

Adriel Frederick:
Awesome. Yes. So I'm the Vice President of Product Management for Reddit X, which sounds like we're out launching balloons into space, but that's not exactly what we're doing. We're more of a team at Reddit that's thinking about the evolving the modes of interaction with Reddit. So content, temporality, the audience that you're talking to, if you think about it, Reddit is primarily about asynchronous conversations between anonymous strangers about shared interests. Sometimes other people find answers to their questions on Reddit, but we're looking into and on the X team, evolving that to look at problems like helping people communicate fast during easier about shared interests, perhaps changing who they're having conversations with. Maybe it's about something other than a shared interest or maybe they have something else in common that brings them together. Maybe bringing video, audio, and other media into being a part of the product and playing with permanence and things like that.

Lenny:
Whoa, I felt like you were going to go into a metaverse direction. Is there metaverse angles to this?

Adriel Frederick:
Not really. I think we look at that as a potential technology, but our primary focus is a lot more on, I see modes of interaction and platforms that are a lot more at scale today.

Lenny:
Got it. Is there anything coming out in the near future we should be looking forward to? I imagine you can't talk about too much of what you're actually working on more concretely.

Adriel Frederick:
I think there's a few things that we've done recently that have been fun. We have an avatar marketplace that we've been working on recently where creators have been able to make art, put it up for sale on Reddit and make that available for other folks to buy and use. And that's been performing amazingly well. The underlying technology behind it is NFTs and we thought that technology was really important to use because it gives a creator a public way of acknowledging their rights to a piece of content. And so they have some form of IP protection, especially in a marketplace where you're doing something like selling digital art, we felt like that was incredibly important. I think the technology behind NFTs has been used for some really nefarious things, but I think we're still in the infancy of using these technologies appropriately. There's a lot of terrible use and a lot of uses that are wastes, but I think there's some gems in there and we're hoping to find some of those.

Lenny:
Sweet. I will avoid getting pulled into a web3 rabbit hole here, but that is very cool. Something I wasn't planning to ask about, but I'm curious because I was just talking to some other guest about this topic is the idea of these kind of R and D ish teams at larger companies and companies that have been around for a while. I know you're relatively new there and that's kind of maybe a new thing, but I'm curious, is there anything you've learned about how to set up teams like this and investments like this, these kind of long term horizon bets R and D teams?

Adriel Frederick:
Yeah, I think it's really good coming to this from being on the other side of it. If you think about where I've been, I've been on growth and on marketplace, which is as far as you get from seeing where on the new stuff kind of team and what I've seen happen a lot is organ rejection, that this thing looks so different to the rest of the body and the rest of the organization that you get some form of rejection of the ideas entirely. So I think what I've learned is a few things.

Adriel Frederick:
First is the rest of the company needs to see what you're doing as being core and critical to the mission. It can't seem like these guys are just playing off in a corner on something that isn't related to what we are doing every day. Because I think that leads to some of the resentment because you can imagine any team internally is fighting for resources and they look at this group as having resources that they can't get. They're like, Oh we got to get rid of that because they're not helping us do what we are here to do. So you have to be part of the core mission, otherwise you're going to have problems culturally with that. So I think that's one thing.

Adriel Frederick:
The second is it has to be everyone's success. So if you end up doing something on one of these R and D teams, it shouldn't just be the R and D team that wins. Everyone should feel like they win. And that is kind of relates to that first goal I was talking about. I think the third is you have to set up the work that these teams are doing such that people don't believe all innovation is going to happen on that team. It can like, okay, we're just stuck with the operational stuff and they're getting to have all the fun. Other teams are still going to innovate, but maybe we're taking on something that other teams don't have capacity for, that the organization needs and it's part of the core mission. And so I think that's been a lot of what I think about when working on and setting up these teams is to make sure that we are part of the organization and everyone wants to hug us as being, yes, you are one of us not, you kind of need to go off in your little corner and behave.

Lenny:
Amazing. That is really helpful. So just to kind of recap, you want it to feel like it's core and critical to the company. You want it to feel like it's everyone's success. It's not just, oh, Adriel over there is doing great, but we're stuck with these terrible hard problems. And then this idea of not all innovation's going to be just coming from that. We can all innovate but they're just working on this one specific innovation.

Adriel Frederick:
Yeah.

Lenny:
Awesome. Okay, great. Another question I definitely wanted to ask you. So you said you were born in Trinidad and Tobago, not something that you hear very often in tech. I'm curious your background and your journey to what you do now, how does that impact the way you lead, the way you build product, the way you just think about your career broadly?

Adriel Frederick:
Yeah, it's not something that I really think about consciously, but it affects me every day and it's tough not to see it in retrospect. I was the first Black product manager at Facebook.

Lenny:
Oh wow.

Adriel Frederick:
And so it's tough for me to not see that having some effect on what was built or how things were built or on me. So it's pretty meaningful. But I think one of the ways to see how it effects things is actually just to understand a little bit about Trinidad. It's kind of its own little unique animal. So Trinidad is an island in the southern Caribbean all the way up bottom next to Venezuela. It's a really diverse place. So ethnically it's 35% Indian, like from East India, 35% African, 25% mixed, and that last 5% is everything under the sun. European, Chinese, Arab, et cetera.

Adriel Frederick:
And then for religions it's about 60% Christian, but then that's a lot of different forms of Christianity in that 60%, 20% Hindu, 7% Islam. The media diet is a mix of British and American TV. You have a really broad range of incomes, but then schools are a melting pot, so you don't have as much of the kind of class and income segregation with schools that you get in most of the west. And so when you have that kind of a melting pot of ethnicities, religions, media consumption and kind of socioeconomic status in one place, you learn a lot of them because in school you're mixing up with everyone. One of the jokes we have is Trinidad probably has the most public holidays of any country because you have to celebrate everyone's holidays from Diwali for Hindus, Eid al-Fitr, to Christmas, I have friends who were fasting for Ramadan. I know a lot of the names of Hindu gods and I always love shocking my coworkers with my knowledge of this stuff. So that gives me this really different perspective that shows up at work.

Adriel Frederick:
So I'll give you an example. Something I've noticed in almost everybody I worked with in tech, as we work on mobile devices, people make an assumption that one phone number plus one device tied to one person. And growing up in Trinidad, I just knew that wasn't true. Someone who is using a prepaid phone could have their number change all the time, so that one person could have multiple phone numbers just because they were using prepaid. You have phones for two sim cards, that was pretty common. And a phone is and definitely was a really expensive digital device. It's a computer. So it was often shared and people couldn't just have one for themselves.

Adriel Frederick:
So when I was working on user acquisition and designing registration for Facebook, that knowledge was incorporated into the design of the product in ways that I think other companies not caught onto yet. And I know for a fact that a lot of that thinking that went into designing how you think about a phone number and a device and it's use among one, it's how to say pairing with an individual, has been helpful for Facebook's growth back then and even after I left, I know that's still been providing benefit. So that's a simple example of how just being in that environment and soaking up information could help product design in a way that I think wouldn't have happened if I and others like me weren't there.

Lenny:
You said you were the first Black PM at Facebook. I didn't realize that. How many PMs were there at that point when you joined?

Adriel Frederick:
Oh man. I remember we all fit into this conference room called Canada and that was probably my second week. It was probably about maybe 30 of us in there.

Lenny:
Wow. Yeah. Is there anything that you learned from that experience about just how to help with diversity at a company? Did Facebook do this well? Have you seen other companies do this better? Is there something you could share there for folks that are trying to work on this?

Adriel Frederick:
Man, that's a tricky one. So there's two parts of that, what was that like for me? I think it went quickly from being a little bit of imposter syndrome, like that day when I was sitting in that group was like, Dude, I'm one of 30 people working on Facebook. What am I doing? I'll belong in this group. This is crazy. And then I recognized after talking to a lot of the other PMs and the engineers, it's like no, no, no, they want me for what I know from my perspective because they're really trying to grow this product globally. And being this guy from Trinidad working on growth with the perspectives I just mentioned was appreciated. I think I was lucky enough to be on the growth team and having leaders on that team really valued diversity. I think about some of the teams I was on and they were awesome. I joke about them sometimes.

Adriel Frederick:
I remember being on a team where I was a Black Trinidadian product manager with a female Israeli engineering manager, a female Brazilian tech lead, then the rest of the engineering and design team was from all over the world. We had Russians, Chinese, some folks from Slavic countries and it made designing products fun because a lot of times when you're building a product and you want to think and get into your head of your customer, you have to go out and talk because you don't necessarily get them really well. Man, we didn't need to on that team. We would just argue with each other. We would think about how our friends would use it, how our cousins would use it, and we are covering a broad swath of the world when we were arguing about how to design a product. But I think the original leadership of the growth team, I think starting with Chima but then followed up with Javi.

Adriel Frederick:
[inaudible 00:16:51] that and kept bringing in that diversity of, again, ethnicities, religions, cultures from all over the world so that you could actually build a product that way. And it just makes you so efficient because an argument that might take two weeks to resolve because you have to go recruit a panel of users and talk to them and figure out what's going on. We kind of knock out in 15 minutes just throwing it back and forth with each other. And I can't stress how much that's important for building products that you want people across the world to use. You got to have your teams look like the world, it just makes you so much faster. It's not perfect. You still have to go out and talk to folks because we still have our own kind of monocultures that form that we need to get out of. But it helps a lot.

Adriel Frederick:
To your second point about diversity and how to foster it, man, from the beginning of my career at McKinsey to today at Reddit, I've been in rooms where everyone's asking the same questions about how to fix it and here's what I've seen work. When you recognize that you get business value from it, then it all of a sudden becomes something that you look out for and you take care of. That's it. And there's definitely a lot more to it, but I think when it goes from frankly something people feel they need to do to be PC or for cultural reasons or because they're getting social pressure to do it to something that you really recognize concretely, no, I get value from this and you are willing to take the other steps to have a culture at your company that utilizes it, then it becomes easy because when you bring folks in from diverse backgrounds, they retain and that's always the number one step to growth as you will know.

Lenny:
Retention.

Adriel Frederick:
You have to retain them, you have to retain diverse talent. And so you have to have an environment that values it, cares about it and uses it and rewards it because it's part of the core system of the company. Then once you have that working, it becomes a lot easier to recruit because people see you valuing it and bringing it in and wanting it. And it's not just lip service that you're paying. That's been what I've seen to be true in all the conversations I've had on the topic.

Lenny:
That first piece is interesting that it answers the second piece, which is the point you made about how having a large diverse global group of employees early on, especially for a company that's trying to go global and international is so powerful, you just save all this time. You don't have to necessarily interview people that you don't already have.

Adriel Frederick:
Yeah, there's something that feels like the approach to not doing it that way feels colonial. It almost feels like we're a group of people sitting down in this tower in this country, in this relatively sterile environment and don't worry, we know exactly what you need and these are the parts of the world. It just doesn't work well. So doesn't feel right to me also.

Lenny:
Yeah. Awesome. Thanks for sharing all that, that was really helpful. There's another topic I definitely wanted to spend a little time on, which is this interesting trend that I noticed when I was looking at your LinkedIn and your background. You worked at Facebook, Lyft, Reddit, and interestingly, they're all very in the news full of controversy type places. People like to tear them down and show all the reasons that they're doing bad things to the world. And I imagine as a PM that's just a challenging place to be and the fact that you've been at three different places. I imagine you've learned some stuff about how to operate as a product leader at companies full of chaos and fires and bad PR and things like that. So is there anything to share about what you've learned there?

Adriel Frederick:
I think the biggest thing is that as a PM you are a leader. You have to provide a buffering or damping effect on the team, and that goes two weeks. Sometimes we're doing stuff that everybody thought was amazing, this is the best thing we've ever seen, and you kind of got to bring people back down to earth and go, Look, that was cool, but we got a lot more stuff to do. We are really not there on providing the value that we want to provide to people in the real world, so slow your role and recognize that there's a lot more to do. And then when it's terrible and the press is telling you that you're the worst thing to ever happen in the world, you kind of have to also go back and say, Guys, slow down. We're not anywhere near as bad as what they think you see and know what we're doing and they're going to misunderstand us sometimes. And so pull your team up at this point in time and keep charging forward with the mission.

Adriel Frederick:
I think some controversy is necessary and so I may be at a different point on that one. I don't think you're going to have any meaningful influence on the world without changing some pattern of behavior. And if you're changing a pattern of behavior, there's somebody who's invested in that pattern of behavior and that's going to create some conflict. The most fun news stories to read involve conflict, so that's always going to make for a great story and put you in the press. For Facebook, it was traditional media and other social networks were one side of the fight and then Facebook was the other side of the fight and then it became other tech companies now and that always makes for a great story. With Lyft it was taxis and unions, and so you have to recognize that you're always going to have some bit of a challenge.

Adriel Frederick:
Now, the really hard part about dealing with this is understanding what criticism is valid and how much of it is just because the source of power is being changed. So I'll give you an example. Let's say with Lyft, rich medallion owners in New York, I had no sympathy for them when they were complaining about trying to ban Lyft because when I was in New York City putting my hand out to get a cab, they were tried right by my Black ass. So I'm sorry, I'm like, I do not feel that much empathy for them, but I think there were really legitimate complaints about the structure of driver pay that were coming up and that were behind I think some of the complaints and some of the big stories in the press and some of the big kind of legal action that was taken. Paying for pickup time when a driver's on their way to pick you up or when they drive somebody far out of town and they have a dead head to come back into a place where they can work, that's real.

Adriel Frederick:
That's a real problem that I think we got called out for that we weren't paying enough attention to and it got us off our ass to go fix it. I don't think we've, and I say we, but I'm not there, I don't think the problem has been fully solved, but I think as a PM listening to this, you kind of have to find the truth behind it and try to find a way to work on that and not get too lost in responding to the specific criticism. And so to walk this line between kind of going, yeah, some of this controversy is just part of the game, versus like, nah, this is really valid. Dude, to figure out where that is, you got to do what is so cliche, but you got to stay close to your users. And so to give you an example of how I did that, when a lot of the complaints were happening about driving on Lyft, I drove, I would just pick up the car and I would get out and I'd go drive and I'm like, let me me go feel this for myself. Let me go see what these guys are talking about.

Adriel Frederick:
But I can give you a story about Rick. I still remember this drive I did with Rick in Berkeley. So I'm at home, I just get in the car, I turn on the app, it's time to go driving. I get up ping 15 minutes away and I'm thinking do this, I go do this right now, this guy might cancel on me, I'm not really getting paid for this, but maybe the ride is worth it. So I drive on over, I'm dodging traffic, pedestrians, drunk college kids, stop signs, I make my way over to Rick, he's coming out Chez Panisse, and he's about 80 years old, jumps in the car and then I pushed the button to figure out the destination and it says the ETA to the destination is two minutes. So I was like, "Hey Rick, you get this right? What's going on?"He's like, "Hey, I had a little bit too much to drink. I'm worried about breaking my hip, so that's why I called a ride." And so I went from wanting to curse Rick out for making me drive 15 minutes to come pick him up to feeling like, all right, no, no, no, there's real value I'm providing you in driving him just two minutes. But I recognized that wasn't embedded in the structure of paying. Rick would've been happy to pee for my 15 minutes to come pick him up, but we weren't one, giving drivers compensation for that, nor were we finding a way to pass that through into pricing for Rick. It's a much more difficult problem than it seems from that simple example. But it clued me into why drivers were complaining. So then I went, got it. I understand what we need to do. So when there were all the PR was going on about AB5 and prop 22, I was out driving and I was out sitting with the team trying to figure out how we're going to design our product that helps pay a driver for this.

Adriel Frederick:
It still keeps prices reasonable for users, doesn't create bad incentives where you end up with riders not getting picked up when they really need a ride because I didn't want Rick to break his hip. He still needs a price that makes him feel like it's okay for him to take that ride, and finding a way to balance this out is actually more complex than you might think. And that's what I stayed focused on whether prop 22 passed or not, I was ready for either side with a solution that was going to work for rider and drivers. That was the job. And so I think for PMs it was so easy to get sucked into the press and it's like, Yo, plan the work, work the plan, go back to your job. That's what you're supposed to do. Solve for customers in the middle of this. And then you figure out how to communicate it well.

Lenny:
What I love about that strategy is it also helps you see that it's not everybody that is worried about something. I think of Airbnb, like all hosts are pissed off about this one feature, there's going to be a revolt. And then to your point, you talk to some, like nobody even knows about it. Nobody cares. Everyone's fine.

Adriel Frederick:
Yeah.

Lenny:
And so this, there's so many benefits to what you're talking about doing, which is talking to customers, not just paying attention to the loud voices.

Adriel Frederick:
Absolutely. I also have empathy for reporters too. The story that with the headline, some Airbnb hosts are annoyed by the-

Lenny:
Right.

Adriel Frederick:
I mean, come on, this is not a great headline. I recognize that they have a job to do and sometimes they hold people accountable and sometimes they're getting people to read a story that maybe has a bit of hyperbole in it. And so they have to do their job and I have to do mine too.

Lenny:
Yep. This episode is brought to you by Eppo. Eppo is a next generation AB testing platform built by Airbnb alums for modern growth teams. Companies like Netlify, Tenfold and Cameo rely on Eppo to power their experiments. Wherever you work, running experiments is increasingly essential, but there are no commercial tools that integrate with a modern growth team stack. This leads to waste of time building internal tools or trying to run your experiments through a clunky marketing tool. When I was at Airbnb, one of the things that I loved about our experimentation platform was being able to easily slice results by device, by country, and by user stage. Eppo does all that and more, delivering results quickly, avoiding annoying prolonged analytics cycles, and helping you easily get to the root cause of any issue you discover. Eppo lets go beyond basic click through metrics, and instead use your north star metrics like activation, retention, subscriptions, and payments. And Eppo supports test on the front end, the back end, email marketing, and even machine learning clients. Check out Eppo at geteppo.com, get E-P-P-O.com and 10x your experiment velocity.

Lenny:
Well you shared this really heartfelt story about Rick, what's your most stressful memory of working at Lyft?

Adriel Frederick:
I think the most stressful time was when I had to unwind a bad product I did and actually make a better version of it. It was really a pricing algorithm change, it was something behind the scenes that nobody would really see, but this was a fairly big initiative that we worked on. We had experts in revenue management to work like PhDs and the people who wrote the textbook on the subject helping advise us on this. We build this model, you launch it and you're expecting this big change. And it goes poof, just does a little bit. And then we work at it and we work at it and we work at it and eventually we get it to be good and it works really well in three cities. We start rolling it out to more cities and it's a pain in the butt to roll it out to more cities because it's super complex.

Adriel Frederick:
And eventually we get it rolled out to maybe a hundred cities and then someone says, "All right, cool, I want to change prices." And oh, we struggled for months to implement price changes and man, the sentiment around this product was just rough for a while. And I remember being on our walk after a particularly bad week of this and trying to figure out what I was going to do about this thing. Do we stay the course? After a while, the answer was kind of simple even though it was emotionally difficult. And the answer was like, Yo, we got to rebuild it. There was no answer where we couldn't have a product like this. We needed some ability to be able to influence prices so that we could actually run an effective marketplace. The current solution didn't work. It wasn't as operationally flexible as we needed it to be because we didn't consider that requirement when we were building it and we got caught up in the kind of algorithmic complexity and sweet sauce of it.

Adriel Frederick:
And so I recognized that we just needed to own up to it, tell everyone we didn't get it right and we needed to come at it in a different approach that was actually more flexible operationally. And we did. I think the big learning, at least in that business was you have to think about operational requirements and operational control as a first order requirement. And I think when a lot of us were building product at a lot of the other consumer internet companies, you didn't have to think about operational control. You gave the algorithms an objective, you feed them some data, you let it run, you observe it and make sure it's doing nothing crazy and you tweak it, but you didn't need to have day to day operational and strategic control over the product and we just needed to snap our brains into being able to put people in the loop with the algorithm.

Lenny:
For folks that haven't worked at a company with this kind of on the ground ops team, can you just unpack what that is? Like operational control, what does that actually mean in practical terms?

Adriel Frederick:
Okay, so I'll give you an example. So back in the days Lyft is in 300 cities, probably roughly across the US, and in every single one of those cities you don't have exactly the same pricing. It's a little bit different. And so sometimes you might need to make a change seasonally because traffic gets worse or because fuel prices were different or because there's a new tax or because your competitor did something that you need to respond to and your algorithm cannot see this. It has zero visibility into this. And so you need a person in the loop to not only give that visibility but also to make a decision about how you respond. Because I think also, let's say you're in Chicago and there is a snowstorm and you need to change the way, let's say you need to update pricing so that it handles the increases in driver pay that you need to create to get people out during a snowstorm. You don't know exactly how you want to respond, every snow storm's different and a person has to make that judgment call and provide the right information to the product to be able to get to utilize it. Now algorithms were handing a lot of that and they could generally respond, but to be a lot more precise and needed a person to help handle that, to make that call.

Lenny:
Got it. Cool. Thanks for sharing that. So you're making this point about when you're at a company that has a big operations component and obviously the core central product team, you were sharing some learnings about what you've learned to work in that environment. So yeah, I just wanted to come back to that.

Adriel Frederick:
For sure. For sure.

Lenny:
So the main thing you said is just ops as a first order component when you're designing the software. Is that the big learning?

Adriel Frederick:
I think it's not just treating ops as the kind of first order requirement. The bigger picture for me was when I look across my career is the algorithms need people to help make judgment calls. And so I saw it really, I got a heavy lesson in it at Lyft, but when I look back I recognized it was there at Facebook too. It just wasn't in my domain. There is always a judgment call that has to be made between how often are there going to be ads versus how often are we going to show organic stories from your friends and family? How often are we going to show content that you might be interested in that's not quite in that group? How often might be want to show you things that help you find your friends or help other people find their friends? And that is a judgment call that varies for different markets and different situations and there may be algorithms behind the scene that are making that call for every single person in real time, but there still have to be people applying some strategic judgment to that.

Adriel Frederick:
And I wasn't in the position of needing to do that at Facebook, but once I saw how much I needed to do it at Lyft and I kind of looked back at history, I saw that it was there too. But I think there are too many people who don't see this and believe that there's an algorithmic solution to everything. I think as a product manager and especially product managers working on systems that are heavy on machine learning or operations research and optimization, to think about where you want a person to make a decision and where you want the machine to be off to the races and to think about that as a product design problem because there actually is actually a computer interface that you have to think about there. You need information about what's going on, let's say at Lyft, what's going on with my market? How long does it take for somebody to get picked up? How expensive am I versus the competition? What are my goals in this market and how am I performing today with that? Give somebody information, but also give them the tools to execute the right decisions without creating trouble. And that's like a product design problem, that's a first order product design problem like anything else that you have to think about. And I'm not privy to it, but I would guarantee that there are people thinking about those same kinds of problems at other companies.

Lenny:
That reminds me, I was just listening to Zuck on Joe Rogan and he made this point that when you look at a post, you can add a little emoji reaction and you can have a little angry emoji reaction. And he made the call that we're not going to use the angry emoji reaction in our algorithm in any way, we're just going to ignore that because naturally you'd be like, okay, people are angry, that's interesting. Let's show that because it's interesting to people. But he specifically wants to avoid anger and facilitating anger probably because a lot of the feedback that they've gotten.

Adriel Frederick:
Exactly. And I think they're probably, I'd call them techno utopians who would say feed all data to the algorithm, give it an objective and it will do the right thing. And I was like, yeah, the reason that falls down is the algorithms don't understand long term effects often, nor do they understand how people might respond to it, nor do they understand your intent for the product. And I think it's really important for product managers to play that role. That is our job. When you are working on algorithmic heavy products, your job is figuring out what the algorithm should be responsible for, what people are responsible for and the framework for making decisions.

Lenny:
Is there an example that comes to mind where you did that or didn't do that well or someone on your team should have? Just something to make it a little more concrete even.

Adriel Frederick:
Let's assume that you are a person working on pricing and you say like, great, I have an objective that is I would like to win market share in a region. And you left that to an algorithm to say I need you to optimize prices such that you maximize market share, but what would the algorithm do? Drop your prices to the floor. All the way to the floor, and then you don't make any money. Okay great. So then you say, okay, what's the next step of that? Let's give it a constraint. Let's set some target that we might want to have for how little profit that you might be willing to take. Okay, go do it now.

Adriel Frederick:
What if the guy on the other side is doing the exact same thing? Both of you will hit your constraints and then the game will stop. Okay great. So now it then flips to, oh we have to choose where we want to win. And so I think one of the things we did that I'm particularly proud of is building products that help people see and understand that game a little bit more and decide where they want to. I think the first few pieces of that are not shockers, but that conclusion at the end where you get to, oh wait, I need to create a tool that gives people information to then decide how to play this game is actually what's critical.

Lenny:
Interesting. So kind of what I'm hearing is a lot of the work is giving humans more information versus giving machine learning algorithms more information and there's a lot more leverage potentially there, giving humans more ways to tweak and dial.

Adriel Frederick:
Let me refine that a little bit more. It's more about giving people the information that they can use for decisions that they alone are good at and giving machines the power to amplify a person's intent. So one of the ways I like to think about it is all software in any form including ML, is just a tool like a screwdriver and you could try to put a flat head into a Phillips and maybe it'll work a little bit but it's better to use a Phillips screwdriver. And we're tool designers generally and especially in part development function, you figure out how much do I put into the tool and how much do I leave it up to the person and I give the person the ability to choose what they want to do. I give them a screwdriver, a flathead, a Phillips, a torques and you let them decide how they want to use the tool for the application at hand.

Adriel Frederick:
And so going from that analogy to concretely with ML you say look, machine learning's going to be amazing at optimizing for a given objective, but it's not going to understand the constraints or strategic choices I need to make. The constraints and strategic choices that we need in the external world are always going to have to be decided by a person. You make that incredibly easy for people to do and intuitive for them to do and then you go that algorithm can then amplify their effect by making decisions on hundreds of thousands, potentially millions of individual decisions to take that person's intent and amplify it given all the information that they can learn in that single context. So I think about it as designing an interface and make it an extension of yourself rather than a black box on its own that you just need more information to. Is that helpful?

Lenny:
Yeah, it makes me think about a neural link and what Elon's trying to do, I don't know if this is how he thinks about it, but the [inaudible 00:39:56] guy described it as Elon's worried that AI will take over at some point and so he wants to build a tool that connects straights our brain that can access the power of AI to kind of have a chance against just a rogue computer in the future.

Adriel Frederick:
Even then you've got to make sure the person is still in control. I hear that thought and I go, okay, you build the interface but then who's in control? Is the person still in control or did they become a slave to the machine and you just made a better interface to make them a slave?

Lenny:
Oh shit, we're in trouble.

Adriel Frederick:
I am not yet as worried about these visions of them taking over. Thus far and maybe I haven't fathomed what they can do, they still seem like tools that need our guidance to be useful. Even the most amazing, we've been seeing the image generation and I've seen the cutting edge like text generation stuff. They can fool you into believing that they're like near human capability, but there is a lack of decision making and judgment that I see coming out of them. I see them as being again, extensions and useful like text generation algorithms. A lot of them can't write a paper for you and that's what I think people are scared of because it still requires your judgment to decide. Now when you decide what the salient topics are in something you've read, let's say you're doing a book report, you've decided what the topics are, it can help you write the paper faster for sure, but it can't write the paper for you. It can't choose the topics that your background and history and interest find useful or compelling to tease out.

Lenny:
This isn't where I was expecting our conversation to go, but I'll add another thought here because it's interesting, the way I think about it is there's nothing magical about our brain and so if that's true, why isn't there a world where we could just completely simulate it? Sam Harris talks about this a lot that it feels like once you get close then it could just accelerate so quickly beyond human potential. It'll start from 20% as good as a human to 40, 50, 60 and then it goes to a million times better. It can move so fast beyond us very quickly. So I think that's where a lot of the, not that I'm afraid of this, but I feel like that's where a lot of fear comes from. It could just dolly coming out and co-pilot just like holy shit.

Adriel Frederick:
Yeah, our brains are good with linear thinking, not exponential. So I've heard that argument that like, yes, this is increasing exponentially and you can't fathom it. I'm like, yes, that is definitely potentially true. Completely see that possibility and recognize that I have that cognitive defect in being able to understand it and even if it's a remote possibility, we should be paying attention to it. So I'm all for paying attention to it given the, let's just say the high cost of a low probability outcome is still a high cost and so it's still worth paying attention to.

Lenny:
Yep. Okay, good tangent, I wanted to chat about your learnings at Facebook. We've been chatting about all these other places and especially about growth, just stuff you've learned about growth and growth hacking and I was thinking about this interesting world that Facebook is in slash Meta where on the one hand when they started, and I'm talking about growth hacking, like Facebook did a lot of growth hacks, emailed all of Harvard, he had all these interesting dating thing happening and got a lot of controversy and it was all these interesting tactics to start Facebook, but now people use Facebook to growth hack and grow like Zenga famously, a few other places. So all that to say, I'm curious, what have you learned about growth slash growth hacking from your time at Facebook and other places?

Adriel Frederick:
I think growth hacking as traditionally assigned, finding those small changes you can make to a product to give you outsized impact, that is absolutely valuable. Where I've seen people get lost is they assume that if you do that alone, it will work. You can grow your way into something successful if you just find those few hacks and patch them together. And there's something about that that I find disrespectful to the people using the product. It's like you assume that they have no intelligence and they won't catch on to what you're doing eventually. The old saying fool me once, fool me twice, it kind of applies. So if you don't have a product that's providing real fundamental value to people, you can be a one hit wonder and have a flash in the pan and growth hack your way into something that might last for a few months but people will catch on it and then it'll disappear.

Adriel Frederick:
So I think that stuff is helpful, especially early on to get your initial traction. But you got to have something people and want to continue using. And when I think back over the products we did that really moved the needle, they were all things that just focus on the marginal user and figured out to make the product easier. It's easy to get seduced to thinking that there is a fast secret way to do it. And I'm like, no. The vast majority of it was just hard work and finding ways to solve the real problems. And what are those real problems? They were pretty damn simple but we just grinded on them for a long time it just stayed on it. One, make it easy to find the product. Number two, make it easy to get into the product. Three, stupid easy to find your friends. And then once you did that you were off the races and those were the things we were doing over and over again.

Adriel Frederick:
I think another big piece of it is reminding people that there's something interesting here and building the habit of coming back to the product and it was also part of it, but we just grinded on those few things over and over again. And some of the really big wins weren't hacks, they were just paying attention to little details. I'll give you an example. I remember sitting one day thinking about how to help people find their first few friends and we would do this thing where we'd have recommendations, if you could get one or two friends, you'd be off to the races and we could find you more people that were in that same friend group. So I thought about the way the people you may know algorithm worked, I get one or two friends, they would find your mutual friends and then would help find you more of those kinds of folks.

Adriel Frederick:
And I was like, you know what that does is it spirals you down one friend group but it doesn't get you all your other friends. I remember just looking at somebody using the product and recognizing that we were only taking them down this one path. So I was like, man, how do I see all your friend groups? And so we had this idea that we came up with that would do it. I'm not going to let that one out. And it was like game changer, like absolute game changer. Especially for users helping them find those first few friends in a few different friend groups, which then meant we could get you down one group and another and just continue building out that graph just by using recommendations. Because we had a great tool for seating it and that was not easy. That was not a hack. That was hard work.

Adriel Frederick:
I also remember one of my favorites is something Tom Allison did, Tom Allison I think now is responsible for the Facebook App, and when he was working on the engineering manager for one of those teams, there was a change we wanted to do to one of these algorithms and it was a bitch, it wasn't a hack and it was going to take a few months to pull off and Tom just hit it in a corner. He just let everybody know that we were really going to change the way this product worked. He had a really smart guy working on it, [inaudible 00:47:03], and they just hid off in a corner, rebuilt the product in the way it needed to be built to make it easier for us to operate it and scale it and then put it up there. I know of course they crushed it and they were incredibly modest about it, but it was not a hack. And it came from them looking at this deep problem of finding that thing that mattered and then saying, we need to make a fundamental change to make it easier to recommend friends to folks and just grinding on it.And so one of the things I recommend for people when they're thinking about growth for their product is figure out what the core actions are and then grind on them. Think about removing them, removing friction and some of them. But just keep staying at it and as you grind on it, you'll do little hacks. You got to figure out how to put the right text in the button and get it above the fold, create the right copy. All the things that we traditionally associate with growth marketing, you got to do those things. But to me that's stable stakes and just doing good product communication with your user, but then you got to think about this person who can't yet figure out your product and it's trying to take this action and making it stupid easy for them. I got a million more examples of that one, but that's the game. It's not just finding some trick to [inaudible 00:48:18] a site.

Lenny:
I love that. The way I think about this that I've heard well described is just there's no silver bullets, just many lead bullets.

Adriel Frederick:
Yes. And a few massive cannonballs every now and then. Every now and then there's some cannonballs.

Lenny:
What's an example of a cannonball as you think about that?

_[168 additional lines trimmed for context budget]_

---

### Why most AI products fail: Lessons from 50+ AI deployments at OpenAI, Google & Amazon
**Guest:** Aishwarya Naresh Reganti + Kiriti Badam | **Date:** 2026-01-11 | [YouTube](https://www.youtube.com/watch?v=z7T1pCxgvlA)  

# Aishwarya Naresh Reganti + Kiriti Badam

## Transcript

Lenny Rachitsky (00:00:00):
We worked on a guest post together. They had this really key insight that building AI products is very different from building non-AI products.

Aishwarya Naresh Reganti (00:00:08):
Most people tend to ignore the non-determinism. You don't know how the user might behave with your product, and you also don't know how the LLM might respond to that. The second difference is the agency control trade-off. Every time you hand over decision-making capabilities to agentic systems, you're kind of relinquishing some amount of control on your end.

Lenny Rachitsky (00:00:25):
This significantly changes the way you should be building product.

Kiriti Badam (00:00:28):
So we recommend building step-by-step. When you start small, it forces you to think about what is the problem that I'm going to solve. In all this advancements of the AI, one easy, slippery slope is to keep thinking about complexities of the solution and forget the problem that you're trying to solve.

Aishwarya Naresh Reganti (00:00:42):
It's not about being the first company to have an agent among your competitors. It's about have you built the right flywheels in place so that you can improve over time.

Lenny Rachitsky (00:00:50):
What kind of ways of working do you see in companies that build AI products successfully?

Aishwarya Naresh Reganti (00:00:55):
I used to work with the CEO of now Rackspace. He would have this block every day in the morning, which would say catching up with AI 4:00 to 6:00 AM. Leaders have to get back to being hands-on. You must be comfortable with the fact that your intuitions might not be right. And you probably are the dumbest person in the room and you want to learn from everyone.

Lenny Rachitsky (00:01:13):
What do you think the next year of AI is going to look like?

Kiriti Badam (00:01:16):
Persistence is extremely valuable. Successful companies right now building in any new area, they are going through the pain of learning this, implementing this and understanding what works and what doesn't work. Pain is the new moat.

Lenny Rachitsky (00:01:29):
Today, my guests are Aishwarya Reganti and Kiriti Badam. Kiriti works on Kodex at OpenAI and has spent the last decade building AI and ML infrastructure at Google and at Kumo. Ash was an early AI researcher at Alexa and Microsoft and has published over 35 research papers. Together, they've led and supported over 50 AI product deployments across companies like Amazon, Databricks, OpenAI, Google, and both startups and large enterprises. Together, they also teach the number one rated AI course on Maven, where they teach product leaders all of the key lessons they've learned about building successful AI products. The goal of this episode is to save you and your team a lot of pain and suffering and wasted time trying to build your AI product. Whether you are already struggling to make your product work or want to avoid that struggle, this episode is for you. If you enjoy this podcast, don't forget to subscribe and follow to your favorite podcasting app or YouTube.

(00:02:22):
It helps tremendously. And if you become an annual subscriber of my newsletter, you get a year free of a ton of incredible products, including a year free of Lovable, Replit, Bold, Gamma, NA, and Linear Dev and Posttalk, Superhuman, Descript, Whisper Flow, Perplexity, Warp, Granola, Magic [inaudible 00:02:38] Mobbin, and Stripe Atlas. Head on over to lennysnewsletter.com and click product pass. With that, I bring you Aishwarya, Reganti, and Kiriti Badam after a short word from our sponsors.

(00:02:49):
This episode is brought to you by Merge. Product leaders hate building integrations. They're messy. They're slow to build. They're a huge drain on your roadmap, and they're definitely not why you got into product in the first place. Lucky for you, Merge is obsessed with integrations. With a single API, B2B SaaS companies embed Merge into their product and ship 220 plus customer-facing integrations in weeks, not quarters.

(00:03:14):
Think of merge like Plaid, but for everything B2B SaaS. Companies like Mistral AI, Ramp, and Drata use Merge to connect their customers as accounting, HR, ticketing, CRM, and file storage systems to power everything from automatic onboarding to AI-ready data pipelines. Even better, Merge now supports the secure deployment of connectors to AI agents with a new product so that you can safely power AI workflows with real customer data. If your product needs customer data from dozens of systems, Merge is the fastest, safest way to get it. Book and attend a meeting at merge.dev/lenny, and they'll send you a $50 Amazon gift card. That's merge.dev/lenny. This episode is brought to you by Strella, the customer research platform built for the AI era. Here's the truth about user research. It's never been more important or more painful. Teams want to understand why customers do what they do, but recruiting users running interviews and analyzing insights takes weeks.

(00:04:14):
By the time the results are in, the moment to act has passed. Strella changes that. It's the first platform that uses AI to run and analyze in depth interviews automatically, bringing fast and continuous user research to every team. Strella's AI moderator asks real follow-up questions, probing deeper when answers are vague, and services patterns across hundreds of conversations all in a few hours, not weeks. Product, design, and research teams at companies like Amazon and Duolingo are already using Strella for Figma prototype testing, concept validation, and customer journey research, getting insights overnight instead of waiting for the next sprint. If your team wants to understand customers at the speed you ship products, try Strella. Run your next study at strella.io/lenny. That's S-T-R-E-L-L-A.io/lenny. Ash and Kiriti, thank you so much for being here and welcome to the podcast.

Aishwarya Naresh Reganti (00:05:13):
Thank you, Lenny.

Kiriti Badam (00:05:14):
Thank you for having us. Super excited for this.

Lenny Rachitsky (00:05:16):
Let me set the stage for the conversation that we're going to have today. So you two have built a bunch of AI products yourself. You've gone deep with a lot of companies who have built AI products, have struggled to build AI products, build AI agents. You also teach a course on building AI products successfully and you're kind of on this mission to just reduce pain and suffering and failure that you constantly see people go through when they're building AI products. So to set a little just foundation for the conversation we're going to have, what are you seeing on the ground within companies trying to build AI products? What's going well? What's not going well?

Aishwarya Naresh Reganti (00:05:54):
I think 2025 has been significantly different than 2024. One, the skepticism has significantly reduced. There were tons of leaders last year who probably thought this would be yet another crypto wave and kind of skeptical to get started. And a lot of the use cases that I saw last year were more of slap chat on your data. And that was calling themselves an AI product. And this year, a ton of companies are really rethinking their user experiences and their workflows and all of that and really understanding that you need to deconstruct and reconstruct your processes in order to build successful AI products. And that's the good stuff. The bad stuff is the execution is still all over the place. Think of it. This is a three-year-old field. There are no playbooks, there are no textbooks. So you really need to figure out as you go. And the AI lifecycle, both pre-deployment and post-deployment is very different as compared to a traditional software lifecycle.

(00:06:57):
And so a lot of old contracts and handoffs between traditional roles, like say PMs and engineers and data folks has now been broken and people are really getting adapted to this new way of working together and kind of owning the same feedback loop in a way. Because previously, I feel like PMs and engineers and all of these folks had their own feedback loops to optimize. And now you need to be probably sitting in the same room. You're probably looking at agent traces together and deciding how your product should behave. So it's a tighter form of collaboration. So companies are still kind of figuring that out. That's kind of what I see in my consulting practice this year.

Lenny Rachitsky (00:07:37):
So let me follow that thread. We worked on a guest post together that came out a few months ago. And the thing that stood out to me most that stuck with me most after working on that post is this really key insight that building AI products is very different from building non-AI products. And the thing that you're big on getting across is there's two very big differences. Talk about those two differences.

Aishwarya Naresh Reganti (00:08:01):
Yes. And again, I want to make sure that we drive home the right point. There are tons of similarities of building AI systems and software systems as well, but then there are some things that kind of fundamentally change the way you build software systems versus AI systems. And one of them that most people tend to ignore is the non-determinism. You're pretty much working with a non-deterministic API as compared to traditional software. What does that mean and why does that have to affect us is in traditional software, you pretty much have a very well-mapped decision engine or workflow. Think of something like Booking.com. You have an intention that you want to make a booking in San Francisco for two nights, et cetera. The product has kind of been built so that your intention can be converted into a particular action and you kind of are clicking through a bunch of buttons, options, forms, and all of that, and you finally achieve your intention.

(00:08:59):
But now that layer in AI products has completely been replaced by a very fluid interface, which is mostly natural language, which means the user can literally come up with a ton of ways of saying or communicating their intentions. And that kind of changes a lot of things because now you don't know how your user's going to be here. That's on the input side. And the output is also that you're working with a non-deterministic probabilistic API, which is your LLM. And LLMs are pretty sensitive to prompt phrasings and they're pretty much black boxes. So you don't even know how the output surface will look like. So you don't know how the user might behave with your product, and you also don't know how the LLM might respond to that. So you're now working with an input, output, and a process. You don't understand all the three very well. You're trying to anticipate behavior and build for it.

(00:09:53):
And with agentic systems, this kind of gets even harder. And that's where we talk about the second difference, which is the agency control trade-off. What we mean by that, and I'm kind of shocked so many people don't talk about this. They're extremely obsessed with building autonomous systems, agents that can do work for you. But every time you hand over decision-making capabilities or autonomy to agentic systems, you're kind of relinquishing some amount of control on your end. And when you do that, you want to make sure that your agent has gained your trust or it is reliable enough that you can allow it to make decisions. And that's where we talk about this agency controlled trade-off, which is if you give your AI agent or your AI system, whatever it is, more agency, which is the ability to make decisions, you're also losing some control and you want to make sure that the agent or the AI system has earned that ability or has built up trust over time.

Lenny Rachitsky (00:10:49):
So just to summarize what you're sharing here, essentially, people have been building product, software products for a long time. We're now in a world where the software you're building is one, non-deterministic, can just do things differently. As you said, you go to booking.com, you find a hotel, it's going to be the same experience every time. You'll see different hotels, but it's a predictable experience. With AI, you can't predict that it's going to be the exact same thing, the thing that you plan it to be every time. And then the other is there's this trade-off between agency and control. How much will the AI do for you versus how much should the person still be in charge? And what I'm hearing is the big point here is this significantly changes the way you should be building product. And we're going to talk about the impact on how the product development lifecycle should change as a result.

(00:11:35):
Is there anything else you want to add there before we get into that?

Kiriti Badam (00:11:39):
Yeah, it's definitely one of the key points that this kind of distinction needs to exist in your mind when you're starting to build. For example, think about if your objective is to hike Half Dome in Yosemite. You don't start hiking it every day, but you start training yourself in minor parts and then you slowly improve and then you go to the end goal. I feel like that's extremely similar to what you want to build AI products in the sense that when you don't start with agents with all the tools and all the context that you have in the company in day one and expect it to work or even tinker at that level. You need to be deliberately starting in places where there is minimal impact and more human control so that you have a good grip of what are the current capabilities and what can I do with them and then slowly lean into the more agency and lesser control.

(00:12:29):
So this gives you that confidence that, okay, I can know that, okay, this is the particular problem that I'm facing and the AI can solve this extent of it. And then let me next think through what context I need to bring in, what kind of tools I need to add to this to improve the experience. So I feel like also it's a good and a bad thing in the sense that it's good that you don't have to see the complexity of the outside world of all of this fancy AI agents force and feel like I cannot do that. Everyone is starting from very minimalistic structures and then evolving. And the second part is the bad thing is that as you are trying to build this one click agents into your company, you don't have to be overwhelmed with this complexity. You can slowly graduate.

(00:13:16):
So that's extremely important. And we see this as a repeating pattern over and over.

Lenny Rachitsky (00:13:20):
Okay. So let's actually follow that because that's a really important component of how you recommend people build AI stuff, AI products, AI agents, all the AI things. So give us an example of what you're talking about here, this idea of starting slow with agency and control and then moving up this rung.

Kiriti Badam (00:13:38):
Yeah. For example, a very important or very prevalent application of AI agents is customer support. Imagine you are a company who has a lot of customer support tickets and why even imagine OpenAI is the exact same thing when we were launching products and there was a huge spike of support volume as we launched successful products like Image or GPT-5 and things like that. The kind of questions you get is different. The kind of problems that the customers bring to you is different. So it's not about just dumping all the list of help center articles that you have into the AI agent. You kind of understand what are the things that you can build. And so initially the first step of it would be something like you have your support agents, the human support agents, but you will be suggesting in terms of, okay, this is what the AI thinks that is the right thing to do.

(00:14:33):
And then you get that feedback loop from the humans that, okay, this is actually a good suggestion for me in this particular case and this is a bad suggestion. And then you can go back and understand, okay, this is what the drawbacks are or this is where the blind spots are, and then how do I fix that? And once you get that, you can increase the autonomy to say that, okay, I don't need to suggest to the human. I'll actually show the answer directly to the customer. And then we can actually add more complexity in terms of, okay, I was only answering questions based on health center articles, but now let me add new functionality. I can actually issue refunds to the customers. I can actually raise feature requests with the engineering team and all of these things. So if you start with all of this on day one, it's incredibly hard to control the complexity.

(00:15:19):
So we recommend building step by step and then increasing it.

Lenny Rachitsky (00:15:23):
Awesome. And you have a visual actually that we'll share of what this looks like. But just to kind of mirror back what you're describing, this idea of start with high control, low agency, the example you gave is the support agents just kind of giving suggestions, is not able to do anything, the user is in charge. And then as that becomes useful and you are confident it's doing the right sort of work, you give it a little more agency and you kind of pull back on the control the user has. And then if that's starting to go well, then you give it more agency and the user needs less control to control it. Awesome.

Aishwarya Naresh Reganti (00:16:02):
I think the higher level idea here is with AI systems, it's all about behavior calibration. It's incredibly impossible to predict upfront how your system behaves. Now, what do you do about it? You make sure that you don't ruin your customer experience or your end user experience. You keep that as is, but then remove the amount of control that the human has. And there is no single right way of doing it. You can decide how to constrain that autonomy. I mean, a different example of how you could constrain autonomy is pre-authorization use cases. Insurance pre-authorization is a very ripe use case for AI because clinicians spend a lot of time pre-authorizing things like blood tests, MRIs and things like that. And there are some cases which are more of low hanging fruits. For instance, MRIs and block tests, because as soon as you know patient's information, it's easier to approve that and AI could do that versus something like an invasive surgery, et cetera, is more high risk. You don't want to be doing that autonomously.

(00:17:11):
So you can kind of determine which of these use cases should go through that human and the loop layer versus which of the use cases AI can conveniently handle. And then all through this process, you're also logging what the human is doing because you want to build a flywheel that you could use in order to improve your system. So you're essentially not showing the user experience, not eroding trust, at the same time logging what humans would otherwise do so that you can continuously improve your system.

Lenny Rachitsky (00:17:41):
So let me give you a few more examples of this kind of progression that you recommend. And the reason I'm spending so much time here is this is a really key part of your recommendation to help people build more successful AI products. This idea of start slow with high control and low agency and then build up over time once you've built confidence that it's doing the right sort of work. So a few more examples that you shared in your post that I'll just read. So say you're building a coding assistant, V1 would be just suggest inline completion and boilerplate snippets. V2 would be generate larger blocks like tests or refactors for humans to review. And then V3 is just apply the changes and open PRs autonomously. And then another example is a marketing assistant. So V1 would be draft emails or social copy, just like here's what I would do.

(00:18:26):
V2 is build a multi-step campaign and run the campaign. And V3 is just launch it A/B tested auto-optimize campaigns across channels. Awesome. Yeah. And again, just to summarize where we're at, just to give people the advice we've shared so far. One is just important to understand AI products are different. They're non-deterministic. And you pointed out, and I forgot to actually mirror back this point, both on the input and the output. The user experience is non-deterministic.People will see different things, different outputs, different chat conversations, different maybe UI if it's designing the UI for you. And also the output obviously is going to be non-terministic. So that's a problem and a challenge. And then-

Aishwarya Naresh Reganti (00:19:08):
I mean, if you think of it's also the most beautiful part of AI, which is, I mean, we are all much more comfortable talking than following a bunch of buttons and all of that. So the bar to using AI products is much lower because you can be as natural as you would be with humans, but that's also the problem, which is there are tons of ways we communicate and you want to make sure that that intent is rightly communicated and the right actions are taken because most of your systems are deterministic and you want to achieve a deterministic outcome, but with non-deterministic technology and that's where it gets a little messy.

Lenny Rachitsky (00:19:44):
Awesome. Okay. I love the optimistic version of why this is good. Okay. And then the other piece is this idea of this trade-off of autonomy versus control when you're designing a thing. And I imagine what you're seeing is people try to jump to the ideal, like the V3 immediately and that's when they get into trouble both. It's probably a lot harder to build that and it just doesn't work. And then they're just like, "Okay, this is a failure. What are we even doing?"

Kiriti Badam (00:20:08):
Exactly. I feel there's a bunch of things that you actually have to get confidence in before you get to V3. And it's easy to get overwhelmed that, oh, my AI agent is doing these things wrong in a hundred different ways and you're not going to actually tabulate all of them and fix it. Even though you've learned how do you deal with the evaluation practices and stuff like that, if you're starting on the wrong spot, you are actually going to have a hard time correcting things from there. And when you start small and when you start with building a very minimalistic version with high human control and low agency, it also forces you to think about what is the problem that I'm going to solve. We use this term called problem first. And to me, it was obvious in the sense that that I do need to think about the problem, but it's incredible how well it resonates with the people that in all this advancements of the AI that we are seeing, one easy, slippery slope is to just keep thinking about complexities of the solution and forget the problem that you're trying to solve.

(00:21:10):
So when you're trying to start at a smaller scale of autonomy, you start to really think about what is the problem that I'm trying to solve and how do I break it down into levels of autonomy that I can build later? So that is incredibly useful and we keep repeating this part and over and over with everyone we talk to.

Lenny Rachitsky (00:21:31):
And there's so many other benefits to limiting autonomy because there's just danger also of the thing doing too much for you and just messing up your, I don't know, your database, sending out all these emails you never expected. And there's like so many reasons this is a good idea.

Aishwarya Naresh Reganti (00:21:45):
Yep. I recently read this paper from a bunch of folks at UC Berkeley. Basically Matei Zaharia, [inaudible 00:21:54] and the folks at Databricks and it said about 74% or 75% of the enterprises that they had spoken to, their biggest problem was reliability. And that's also why they weren't comfortable deploying products to their end users or building customer facing products because they just weren't sure or they just weren't comfortable doing that and exposing their users to a bunch of these risks. And that's also why they think a lot of AI products today have to do with productivity because it's much low autonomy versus end-to-end agents that would replace workflows. And yeah, I love their work otherwise as well, but I think that's very in line with what at least we are seeing at my startup as well.

Lenny Rachitsky (00:22:38):
Okay. Very interesting. There's an episode that'll come out before this conversation where we go deep into another problem that this avoids, which is around prompt injection and jailbreaking and just how big of a risk that is for AI products where it's essentially an unsolved and unsolvable problem potentially. I'm not going to go down that track, but that's a pretty scary conversation we had that'll be out before this conversation.

Aishwarya Naresh Reganti (00:23:02):
I think that will be a huge problem once systems go mainstream. We're still so busy building AI products that we're not worried about security, but it will be such a huge problem to kind of, especially with this non-deterministic API again. So you're kind of stuck because there are tons of instructions that you could inject within your prompt and then it's going really bad.

Lenny Rachitsky (00:23:28):
Okay. Let's actually spend a little time here because it's actually really interesting to me and no one's talking about this stuff, which is like the conversation we had is just it's pretty easy to get AI to do stuff it shouldn't do. And there's all these guardrail systems people put in place, but turns out these guardrails aren't actually very good and you can always get around them. And to your point, as agents become more autonomous and robots, it gets pretty scary that you could get AI to do things you shouldn't do.

Kiriti Badam (00:23:54):
I think this is definitely a problem, but I feel in the current spectrum of customers adopting AI, the extent to which companies can actually get advantage of AI or improve their processes or streamline the existing processes that they have, I feel it's still in the very early stage. 2025 has been an extremely busy year for AI agents and customers trying to adopt AI, but I feel the penetration is still not as much as you would actually get advantage out of it. So with the right sort of human in the loop points in here, I feel we can actually avoid a bunch of these things and focus more towards streamlining the processes. And I am more on the optimist side in the sense that you need to try and adopt this before actually trying to be only for highlighting the negative aspects of what could go wrong.

(00:24:47):
So I feel like strongly that companies has this adopt this, they definitely ... No company at OpenAI we talk to has never had been the case that, oh, AI cannot help me in this case. It has always been that, oh, there is this set of things that it can optimize for me and then let me see how I can adopt it.

Lenny Rachitsky (00:25:06):
Sweet. I always like the optimistic perspective. I'm excited for you to listen to this and see what you think because it's really interesting. And to your point, there's a lot of things to focus on. It's one of many things to worry about and think about. Okay, let's get back on track here. So we've shared a bunch of pro-tips and important piece of advice. Let me ask, what other patterns and kind of ways of working do you see in companies that do this well and teams that build AI products successfully? And then just what are the most common pitfalls people fall into? So we could just maybe start with, what are other ways that companies do this well, build AI products successfully?

Aishwarya Naresh Reganti (00:25:43):
I almost think of it as like a success triangle with three dimensions that's never always technical. Every technology problem is a people problem first. And with companies that we have worked with, it's these three dimensions, like great leaders, good culture and technical prowess. With leaders itself, we work with a lot of companies for their AI transformation, training, strategy and stuff like that. And I feel like a lot of companies, the leaders have built intuitions over 10 or 15 years and they're kind of highly regarded for those intuitions. But now with AI in the picture, those intuitions will have to be relearned and leaders have to be vulnerable to do that. I used to work with the CEO of now Rackspace, Gagan. So he would have this block every day in the morning, which would say catching up with AI 4:00 to 6:00 AM, and he would not have any meetings or anything like that.

(00:26:42):
And that was just his time to pick up on the latest AI podcast or information and all of that. And he would have weekend vibe coding sessions and stuff like that. So I think leaders have to get back to being hands-on. And that's not because they have to be implementing these things, but more of rebuilding their intuitions because you must be comfortable with the fact that your intuitions might not be right and you probably are the dumbest person in the room and you want to learn from everyone. And that I've seen that being a very distinguishing factor of companies that build products which are successful because you're kind of bringing in that top-down approach. It's almost always impossible for it to be bottom-up. You can't have a bunch of engineers go and get buy-in from the leader if they just don't trust in the technology or if they have misaligned expectations about the technology.

(00:27:34):
I've heard from so many folks who are building that our leaders just don't understand the extent to which AI can solve a particular problem or they just vibe code something and assume it's easy to take it to production and you really need to understand the range of what AI can solve today so that you can guide decisions within the company. The second one is the culture itself. And again, I work with enterprises where AI is not their main thing and they need to bring in AI into their processes just because a competitor is doing it. And just because it does make sense because there are use cases that are very ripe. Then along the way, I feel a lot of companies have this culture of FOMO and you will be replaced and those kind of things and people get really afraid. Subject matter experts are such a huge part of building AI products that work because you really need to consult them to understand how your AI is behaving or what the ideal behavior should be.

(00:28:27):
But then I've spoken to a bunch of companies where the subject matter experts just don't want to talk to you because they think their job is being replaced. So I mean, again, this comes from the leader itself. You want to build a culture of empowerment, of augmenting AI into your own workflows so that you can 10X at what you're doing instead of saying that probably you'll be replaced if you don't adopt AI and stuff like that. So that kind of an empowering culture always helps. You want to make your entire organization be in it together and make AI work for you instead of trying to guard their own jobs, et cetera. And with AI, it's also true that it opens up a lot more opportunities than before. So you could have your employees doing a lot more things than before and 10x their productivity. And the third one is the technical part which we talk about.

(00:29:18):
I think folks that are successful are incredibly obsessed about understanding their workflows very well and augmenting parts that could be ripe for AI versus the ones that might need human in the loop somewhere, et cetera. Whenever you're trying to automate some part of a workflow, it's never the case that you could use an AI agent and that will solve your problems. It's always, you probably have a machine learning model that's going to do some part of the job. You have deterministic code doing some part of the job. So you really need to be obsessed with understanding that workflow so you can choose the right tool for the problem instead of being obsessed with the technology itself. And another pattern I see is also folks really understand this idea of working with a non-deterministic API, which is your LLM. And what that means is they also understand the AI development lifecycle looks very different and they iterate pretty quickly, which is can I build something iterate quickly in a way that it doesn't ruin my customer experience at the same time gives me enough amount of data so that I can estimate behavior.

(00:30:31):
So they build that flywheel very quickly. As of today, it's not about being the first company to have an agent among your competitors. It's about, have you built the right flywheels in place so that you can improve over time? When someone comes up to me and says, "We have this one click agent, it's going to be deployed in your system." And then in two or three days, it'll start showing you significant gains. I would almost be skeptical because it's just not possible. And that's not because the models aren't there, but because enterprise data and infrastructure is very messy and you need a bit to ... Even the agent needs a bit to understand how these systems work. There are very messy taxonomies everywhere. People tend to do things like get customer data, we want, get customer data, we do, and these kind of things. And all those functions exist and they're being called and basically there's a lot of tech debt that you need to deal with.

(00:31:23):
So most of the times, if you're obsessed with the problem itself and you understand your workflows very well, you will know how to improve your agents over time instead of just slapping an agent and assuming that it'll work from day one. I probably will go as far to say that if someone's selling you one click-agents, it's pure marketing. You don't want to buy into that. I would rather go with a company that says, "We're going to build this pipeline for you," and that will learn over time and build a flywheel to improve than something that's going to work out of the box. To replace any critical workflow or to build something that can give you significant ROI, it easily takes four to six months of work, even if you have the best data layer and infrastructure layer.

Lenny Rachitsky (00:32:05):
Amazing. There's a lot there that resonates so deeply with other conversations I've been having on this podcast. One is just for a company to be successful at seeing a lot of impact from AI, the founder-CEO has to be deep into it. I had Dan Shipper on the podcast and they work with a bunch of companies helping them adopt AI. And he said that's the number one predictor of success. Is the CEO chatting with ChatGPT, Claude, whatever, many times a day. I love this example you gave with the Rackspace CEO has catch up on AI news in the morning every day. I was imagining he'd be chatting with the chatbot versus reading news.

Aishwarya Naresh Reganti (00:32:42):
With the kind of information you have as of today, you could just ... I mean, you want to choose the right channels as well because everybody has an opinion. So whose opinion do you want to bank on? I feel like having that good quality set of people that you're listening to really makes sense. So he just has a list of two or three sources that he always looks at. And then he comes back with a bunch of questions and bounces it around with a bunch of AI experts to see what they think about it. And I was part of that group, so I kind of know-

Lenny Rachitsky (00:33:11):
I love that.

Aishwarya Naresh Reganti (00:33:13):
... about the questions that he comes up with.

Lenny Rachitsky (00:33:13):
That's cool.

Aishwarya Naresh Reganti (00:33:15):
It's pretty cool. I was like, "Why are you doing so much?" And then he says, "It trickles down into a bunch of decisions that we take."

Lenny Rachitsky (00:33:21):
Okay. Let me talk about another topic that's very ... It's been a hot topic on this podcast. It was a hot topic on Twitter for a while, evals. A lot of people are obsessed with evals, think they're the solution to a lot of problems in AI. A lot of people think they're overrated that you don't need evals. You can just feel the vibes and you'll be all right. What's your take on evals? How far does that take people in solving a lot of the problems that you talk about?

Kiriti Badam (00:33:47):
In terms of what is going on in the community, I feel there's just this false dichotomy of this either evals is going to solve everything or online monitoring or production monitoring is going to solve everything. And I find no reason to trust one of the extremes in the sense that I will entirely bank my application on this or like that to solve the thing. So if you take a step back, think of what are evals. Evals are basically your trusted product thinking or your knowledge about the product that is going into this set of data sets that you're going to build in the sense that this is what matters to me. This is the kind of problems that my agent should not do and let me build a list of datasets so that I'm going to do well on those. And in terms of production monitoring, what you're doing there is you're deploying your application and then you're having some sort of key metrics that actually communicate back to you on how customers are using your product.

(00:34:47):
You could be deploying any agent and if the customer is giving a thumbs up for your interaction, you better want to know that. So that is what production monitoring is going to do. And this production monitoring has existed for products for a long time, just that now with the AI agents, you need to be monitoring a lot more granularity. It's not just the customer always giving you explicit feedback, but there is many implicit feedback that you can get. For example, in ChatGPT, if you are liking the answer, you can actually give a thumbs up. Or if you don't like the answer, sometimes customers don't give you thumbs down, but actually regenerate the answer. So that is a clear indication that the initial answer that regenerator is not meeting the customer's expectation. So these are the kind of implicit signals you always need to think about.

(00:35:35):
And that spectrum has been increasing in terms of production monitoring. Now let's come back to the initial topic of like, okay, is it evals or is it production monitoring? What does it matter? So I feel, again, we go back to this problem first approach of what is it that you're trying to build. You're trying to build a reliable application for your customers that's not going to do a bad thing. It's always going to do the right thing. Or if it is doing a wrong thing, you're basically alerted very quickly. So I break this down into two parts. One is nobody goes into deploying an application without actually just testing that. This testing could be wipes or this testing could be, "Okay, I have this 10 questions that it should not go wrong no matter what changes I make, and let me build this and let's call this an evaluation dataset." Now, let's say you build this, you deployed this, and then you figured, "Okay, now I need to understand whether it's doing the right thing or not."

(00:36:32):
So if you're a high throughput or a high transaction customer, you cannot practically sit and evaluate all the traces. You need some indication to understand what are the things that I should look at. And this is where production monitoring comes into the picture that you cannot predict the base in which your agent could be doing wrong, but all of these other implicit signals and explicit signals, those are going to communicate back to you what are the traces that you need to look at. And that is where production monitoring helps. And once you get this kind of traces, you need to examine what are the failure patterns that you're seeing in these different types of interactions. And is there something that I really care about that should not happen? And if that kind of failure modes are happening, then I need to think about building an evaluation dataset for it.

(00:37:20):
And okay, let's say I built an evaluation dataset for my agent trying to offer refunds where explicitly I have configured it not to. So I built this evaluation dataset and then I made my changes in tools or prompts or whatever, and then I deployed the second version of the product. Now there is no guarantee that this is the only problem that you're going to see. You still need production monitoring to actually catch different kinds of problems that you might encounter. So I feel evals are important, production monitoring is important, but this notion of only one of them is going to solve things for you that is completely dismissible in my opinion.

Lenny Rachitsky (00:37:58):
All right. A very reasonable answer. And the point here isn't, it's not just as simple as do both. It's more that there are different things to catch and one approach won't catch all the things you need to be paying attention to.

Aishwarya Naresh Reganti (00:38:11):
Exactly.

Lenny Rachitsky (00:38:12):
Awesome.

Aishwarya Naresh Reganti (00:38:13):
I want to take two steps back and kind of talk about how much weight the term evals has had to take in the second half of 2025 because you go meet a data labeling company and they tell you our experts are writing evals and then you have all of these folks saying that PMs should be writing evals, they're the new PRDs. And then you have folks saying that evals is pretty much everything, which is the feedback loop you're supposed to be building to improve your products. Now, step back as a beginner and kind of think what are evals? Why is everyone saying evals? And these are actually different parts of the process and nobody is wrong in the sense that yes, these are evals, but when a data labeling company is telling you that our experts are writing evals, they're actually referring to error analysis or experts just leading notes on what should be right.

(00:39:02):
Lawyers and doctors write evals, that doesn't mean they're building LLM judges or they're building this entire feedback loop. And when you say that a PM should be writing evals, doesn't mean they have to write an LLM judge that's good enough for production. I think there are also very prescriptive ways of doing this and plus one to KD, which is you cannot predict upfront if you need to be building an LLM judge versus you need to be using implicit signals from production monitoring, et cetera. I think Martin Fowler at some point had this term called semantic diffusion back in the 2000s, which kind of means that someone comes up with a term, everybody starts butchering it with their own definitions and then you kind of lose the actual definition of it. That is what is happening to evals or agents or any word in AI as of today, everybody kind of sees a different side to it, I guess.

(00:39:54):
But if you make a bunch of practitioners sit together and ask them, "Is it important to build an actionable feedback loop for AI products?" I think all of them will agree. Now, how you do that really depends on your application itself. When you go to complex use cases, it's incredibly hard to build LLM judges because you see a lot of emerging patterns. If you built a judge that would test for verbosity or something like that, it turns out that you're seeing newer patterns that your LLM judge is not able to catch, and then you just end up building too many evals. And at that point, it just makes sense to look at your user signals, fix them, check if you have regressed and move on instead of actually building these judges. So it all depends. I think one statement that every ML practitioner will tell you is it really depends on the context. Don't be obsessed with prescriptions they're going to change.

Lenny Rachitsky (00:40:45):
That's such an important point, this idea that, especially that evals just means many things to different people now. It's just a term for so many things. And it's complicated to just talk about evals when you see it as the stuff data labeling companies are giving you and things PMR, right? And there's also benchmarks. People call benchmarks a little bit evals. It's like-

Aishwarya Naresh Reganti (00:41:03):
I recently spoke to a client who told me, "We do evals." And I was like, "Okay, can you show me your dataset?" And said, "No, we just checked LM Arena and Artificial Analysis. These are independent benchmarks and we know that this model is the right one for our use case." And I'm like, "You're not doing evals. That's not evals. Those are model evals."

Lenny Rachitsky (00:41:20):
But it makes sense. The word, it could be used in that context. I get why people think that, but yeah, now it's just confusing it even more.

Aishwarya Naresh Reganti (00:41:26):
Yep.

Lenny Rachitsky (00:41:27):
Just one more line of questioning here that I think that's on my mind is the reason this became kind of a big debate is Cloud Code. The head of Cloud Code, Boris, was like, "Nah, we don't do evals on Cloud Code. It's all vibes." What can you share, Kiriti, on Kodex and the Kodex team, how you approach evals?

Kiriti Badam (00:41:44):
So Kodex, we have this balanced approach of you need to have evals and you need to definitely listen to your customers. And I think Alex has been on your podcast recently and he's been talking about how you're extremely focused on building the right product. And a big part of it is basically listening to your customers. And coding agents are extremely unique compared to agents for other domains in the sense that these are actually built for customizability and these are built for engineers. So coding agent is not a product which is going to solve these top five workflows or top six workflows or whatever. It's meant to be customizable in multi different ways. And the implication of that is that your product is going to be used in different integrations and different kinds of tools and different kinds of things. So it gets really hard to build an evaluation dataset for all kinds of interactions that your customers are going to use your product for.

(00:42:38):
With that said, you also need to understand that, okay, if I'm going to make a change, it's at least not going to damage something that is really core to the product. So we have evaluations for doing that, butt the same time we take extreme care on understanding how the customers are using it. For example, we built this code review product recently and it has been gaining extreme amount of traction. And I feel like many, many bugs in OpenAI as well as even our external customers are getting caught with this. And now let's say if I'm making a model change to the code review or a different kinds of RL mechanism that I trained with it, and now if I'm going to deploy it, I definitely do want A/B test and identify whether it's actually finding the right mistakes and how are users reacting to it? And sometimes if users do get annoyed by your incorrect code regis, they go to the extent of just switching off the product.

(00:43:36):
So those are the signals that you want to look at and make sure that your new changes are doing the right thing. And it's extremely hard for us to think of these kind of scenarios beforehand and develop evaluation data sets for it. So I feel like there's a bit of both. There's a lot of vibes and there's a lot of customer feedback and we are super active on the social media to understand if anybody's having certain types of problems and quickly fix that. So I feel it's a ... How do I put this? It's like a domain of things that you do here.

Lenny Rachitsky (00:44:08):
That makes so much sense. Okay. What I'm hearing, Codex, pro evals, but it's not enough.

Kiriti Badam (00:44:13):
Yes.

Lenny Rachitsky (00:44:13):
But also just watch customer behavior and feedback. And also there's some vibes just like, is this feeling good? As I'm using it, generating great code that I'm excited about that I think is great.

Kiriti Badam (00:44:24):
I don't think if anybody's coming and seeing that I have this concrete set of evals that I can bet my life on and then I don't need to think about anything else, it's not going to work. And every new model that you're going to launch, we get together as a team and test different things. Each person is concentrating on something else. And we have this list of hard problems that we have and we throw that to the model and see how well they're progressing. So it's like custom evals for each engineer, you would say, and just understand what the product is doing in its new model.

Lenny Rachitsky (00:44:58):
If you're a founder, the hardest part of starting a company isn't having the idea, it's scaling the business without getting buried in back office work. That's where Brex comes in. Brex is the intelligent finance platform for founders. With Brex, you get high limit corporate cards, easy banking, high yield treasury, plus a team of AI agents that handle manual finance tasks for you. They'll do all the stuff that you don't want to do, like file your expenses, scour transactions for waste, and run reports all according to your rules. With Brex's AI agents, you can move faster while staying in full control. One in three startups in the United States already runs on Brex. You can too at brex.com.

(00:45:43):
We've been talking for almost an hour already, and we haven't even covered your extremely powerful software development workflow for building AI products that you two developed that you teach in your course, that you basically combined all the stuff we've been talking about into a step-by-step approach to building AI products. You call it the continuous calibration, continuous development framework. Let's pull up a visual to show people what the heck we're talking about, and then just walk us through what this is, how this works, how teams can shift the way they build their AI products to this approach to help them avoid a lot of pain and suffering.

Aishwarya Naresh Reganti (00:46:18):
Before we go about explaining the life cycle, a quick story on why Kiriti and I came up with this is because there are tons of companies that we keep talking to that have the pressure from their competitors because they're all building agents. We should be building agents that are entirely autonomous. And I did end up working with a few customers where we built these end-to-end agents. And turns out that because you start off at a place where you don't know how the user might interact with your system and what kind of responses or actions the AI might come up with, it's really hard to fix problems when you have this really huge workflow, which is taking four or five steps, making tons of decisions. You just end up debugging so much and then kind of hot fixing to the point where at a time we were building for a customer support use case, which is the example that we give in the newsletter as well.

(00:47:13):
And we had to shut down the product because we were doing so many hot fixes and there was no way we could count all the emerging problems that were coming up. And there's also quite some news online. Recently, I think Air Canada had this thing where one of their agents predicted or hallucinated a policy for a refund, which was not part of their original playbook, and they had to go by it because legal stuff. And there have been a ton of really scary incidents. And that's where the idea comes from. How can you build so that you don't lose customer trust and you don't end up, or your agent or AI system doesn't end up making decisions that are super dangerous to the company itself. At the same time, build a flywheel so that you can improve your product as you go. And that's where we came up with this idea of continuous calibration, continuous development.

(00:48:08):
The idea is pretty simple, which is we have this right side of the loop, which is continuous development, where you scope capability and curate data, essentially get a data set of what your expected inputs are and what your expected outputs should be looking at. This is a very good exercise before you start building any AI product because many times you figure out that a lot of the folks within the team are just not aligned on how the product should behave. And that's where your PMs can really give in a lot more information and your subject matter experts as well. So you have this data set that you know your AI product should be doing really well on. It's not comprehensive, but it lets you get started. And then you set up the application and then design the right kind of evaluation metrics. And I intentionally use the term evaluation metrics, although we say evals because I just want to be very specific in what it is because evaluation is a process, evaluation metrics are dimensions that you want to focus on during the process.

(00:49:07):
And then you go about deploying, run your evaluation metrics. And the second part is the continuous calibration, which is the part where you understand what behavior you hadn't expected in the beginning, right? Because when you start the development process, you have this data set that you're optimizing for, but more often than not, you realize that that data set is not comprehensive enough because users start behaving with your systems in ways that you did not predict. And that's where you want to do the calibration piece. I've deployed my system. Now I see that there are patterns that I did not really expect and your evaluation metrics should give you some insight into those patterns, but sometimes you figure out that those metrics were also not enough and you probably have new error patterns that you have not thought about. And that's where you analyze your behavior, spot error patterns.

(00:49:59):
You apply fixes for issues that you see, but you also design newer evaluation metrics to figure out that they are emerging patterns. And that doesn't mean you should always design evaluation metrics. There are some errors that you can just fix and not really come back to because they're very spot errors. For instance, there's a tool calling error just because your tool wasn't defined well and stuff like that. You can just fix it and move on. And this is pretty much how an AI product lifecycle would look like. But what we specifically also mention is while you're going through these iterations, try to think of lower agency iterations in the beginning and higher control iterations. What that means is constrain the number of decisions your AI systems can make and make sure that they're humans in the loop and then increase that over time because you're kind of building a flywheel of behavior and you're understanding what kind of use cases are coming in or how your users are using the system.

(00:50:59):
And one example I think we give in the newsletter itself is the customer support. This is a nice image that kind of shows how you can think of agency and control as two dimensions. And each of your versions keep on increasing the agency or the ability of your AI system to make decisions and lower the control as you go. And one example that we give is that of the customer support agent, where you can break it down into three versions. The first version is just routing, which is your agent able to classify and route a particular ticket to the right department? And sometimes when you read this, you probably think, is it so hard to just do routing? Why can't an agent easily do that? And when you go to enterprises, routing itself can be a super complex problem. Any retail company, any popular retail company that you can think of has hierarchical taxonomies.

(00:51:52):
Most of the times the taxonomies are incredibly messy. I have worked in use cases where you probably have taxonomy that says some kind of hierarchy and then that says shoes and then women's shoes and men's shoes all at the same layer where idea you should be having shoes and then women's shoes and men's shoes should be subclasses. And then you're like, okay, fine. I could just merge that. And you go further and you see that there's also another section on the shoes that says for women and for men, and it's just not aggregated. It's not fixed for some reason. So if an agent kind of sees this kind of a taxonomy, what is it supposed to do? Where is it supposed to route? And a lot of the times we are not aware of these problems until you actually go about building something and understanding it.

(00:52:39):
And when these kind of problems, real human agents see these kind of problems, they know what to check next. Maybe they realize that the node that says for women and for men that's under shoes was last updated in 2019, which means that it's just a dead node that's lying there and not being used. So they kind of know that, okay, we're supposed to be looking at a different node and stuff like that. And I'm not saying agents cannot understand this or models are not capable enough to understand this, but there are really weird rules within enterprises that are not documented anywhere. And you want to make sure that the agents have all of that context instead of just throwing the problem at that.

(00:53:17):
Yeah. Coming back to the versions we had, routing was one where you have really high control because even if your agent routes to the wrong department, humans can take control and undo those actions. And along the way, you also figure out that you probably are dealing with a ton of data issues that you need to fix and make sure that your data layer is good enough for the agent to function. We do is what we said of a Copilot, which is now that you've figured out routing works fine after a few iterations and you've fixed all of your data issues, you could go to the next step, which is, can my agent provide suggestions based on some standard operating procedures that we have for the customer support agent? And it could just generate a draft that the human can make changes to. And when you do this, you're also logging human behavior, which means that how much of this draft was used by the customer support agent or what was omitted. So you're actually getting error analysis for free when you do this because you're literally logging everything that the user is doing that you could then build back into your flywheel.

(00:54:22):
And then we say, post that, once you've figured out that those drafts look good and most of the times maybe humans are not making too many changes, they're using these drafts as is. That's when you want to go to your end-to-end resolution assistant that could draft a resolution that could solve the ticket as well. And those are the stages of agency where you start with low agency and then you go up high. We also have this really nice table that we put together, which is what do you do at each version and what you learn that can enable you to go to the next step and what information do you get that you can feed into the loop, right? When you're just doing your routing, you have better quality routing data, you also know what kind of prompts you need to be building to improve the routing system.

(00:55:09):
Essentially, you're figuring out your structure for context engineering and building that flywheel that you want. And while I go through this, I want to also be very clear that two things. One is when you build with CCCD in mind, it doesn't mean that you've fixed the problem all for one. It's possible that you've probably gone through V3 and you see a new distribution of data that you never previously imagined, but this is just one way to lower your risk, which is you get enough information about how users behave with your system before going to a point of complete autonomy. And the second thing is you're also kind of building this implicit logging system. A lot of people come and tell us that, "Oh, wait, there are evals. Why do you need something like this? " The issue with just building a bunch of evaluation metrics and then having them in production is evaluation metrics catch only the errors that you're already aware of, but there can be a lot of emerging patterns that you understand only after you put things in production.

_[258 additional lines trimmed for context budget]_

---

### Finding hidden growth opportunities in your product | Albert Cheng (Duolingo, Grammarly, Chess.com)
**Guest:** Albert Cheng | **Date:** 2025-10-05 | [YouTube](https://www.youtube.com/watch?v=2BKmNmnEj9w)  

# Finding hidden growth opportunities in your product | Albert Cheng (Duolingo, Grammarly, Chess.com)

## Transcript

Albert Cheng (00:00:00):
Growth as the job is to connect users to the value of your product. Growth sometimes gets this reputation that it's just pure metrics hacking.

Lenny Rachitsky (00:00:08):
You've worked at three of the most successful consumer subscription products in the world. What do you think is the biggest missing piece that people don't get about building a successful consumer subscription product?

Albert Cheng (00:00:18):
User retention is gold for consumer subscription companies. If you don't retain your users, then a lot of the onus is on getting them to pay on day one.

Lenny Rachitsky (00:00:26):
Noam Levinsky, he said that I need to ask you about the biggest monetization win that you found at Grammarly.

Albert Cheng (00:00:31):
The lived product experience for most of the free users was that Grammarly was just a product to fix your spelling and grammar because those were the free suggestions. What if we actually sampled a number of different paid suggestions and interspersed them to free users across their writing? All of a sudden, people were seeing Grammarly as a much more powerful tool than they were before.

Lenny Rachitsky (00:00:50):
What's the most counterintuitive lesson you've learned about building teams?

Albert Cheng (00:00:54):
I saw some of the highest performers just being people that had very high agency, had that clock speed, had that energy, but they didn't necessarily need to have deep experience on that matter. Sometimes experience could be a crutch, especially in this world where the grounds are shifting so fast with AI. A lot of your learned habits actually need to be intentionally discarded.

Lenny Rachitsky (00:01:13):
Today my guest is Albert Cheng. Albert is known as one of the top consumer growth minds in the world. He led growth and monetization at three of the most successful and beloved consumer products in the world, Duolingo, Grammarly, and now Chess.com. Earlier in his career at YouTube, he worked on streaming and gaming features used by over 20 million people.

(00:01:32):
His unique approach to growth blends marketing, data, strategy, and product management, and in our conversation, we cover a lot of ground, including his explore and exploit framework to find growth opportunities. His biggest and most interesting growth wins at Duolingo, Grammarly and Chess.com, how he uses AI to accelerate his growth work, what he's come to realize about the power of brand and community in your growth work, his top experimentation, best practices, why his goal at every company is to run 1,000 experiments a year and so much more.

(00:02:02):
A huge thank you to Erik Allebest, Noam Levinsky, and Jorge Mazal for suggesting topics for this conversation. If you enjoy this podcast, don't forget to subscribe and follow it in your favorite podcasting app or YouTube, it helps tremendously. Also, if you become an annual subscriber of my newsletter, you get 15 incredible products for free for an entire year, including Lovable, Replit, Bolt, n8n, Linear, Superhuman, Descript, Wispr Flow, Gamma, Perplexity, Warp, Granola, Magic Patterns, Raycast, ChatPRD and Mobbin.

(00:02:33):
Head on over to lennysnewsletter.com and click Product Pass. With that, I bring you Albert Chain, my podcast guest tonight love talking about craft and taste and agency and product market fit. You know what we don't love talking about? SOC 2. That's where Vanta comes in. Vanta helps companies of all sizes get complying fast and stay that way with industry-leading AI automation and continuous monitoring. Whether you're a startup, your first SOC 2 or ISO 27001 or an enterprise managing vendor risk, Vanta's trust management platform makes it quicker, easier, and more scalable.

(00:03:08):
Vanta also helps you complete security questionnaires up to five times faster so that you could win bigger deals sooner. The result, according to a recent IDC study, Vanta customers slashed over $500,000 a year and are three times more productive. Establishing trust isn't optional. Vanta makes it automatic. Get $1,000 off at Vanta.com/lenny. This episode is brought to you by Jira Product Discovery. The hardest part of building products isn't actually building products, it's everything else.

(00:03:41):
It's proving that the work matters, managing stakeholders trying to plan ahead. Most teams spend more time reacting than learning, chasing updates, justifying roadmaps, and constantly unblocking work to keep things moving. Jira Product Discovery puts you back in control. With Jira Product Discovery, you can capture insights and prioritize high impact ideas.

(00:04:01):
It's flexible so it adapts to the way your team works and helps you build a roadmap that drives alignment, not questions. And because it's built on Jira, you can track ideas from strategy to delivery all in one place, less chasing, more time to think, learn and build the right thing. Get Jira Product Discovery for free at Atlassian.com/lenny, that's Atlassian.com/lenny. Albert, thank you so much for being here and welcome to the podcast.

Albert Cheng (00:04:33):
Thanks for having me, Lenny. Excited to be here.

Lenny Rachitsky (00:04:35):
I'm even more excited to have you here. So as I do for every podcast conversation, I reached out to a bunch of people that you've worked with that know you well to find out what to ask you about and what topics to spend time on. Jorge Mazal, who is famous in my world for writing, what was for the longest time, the most popular newsletter post on my newsletter, it's actually people have usurped it now, but it was stuck there for a long time. So here's what he wrote. "It is a mystery to me how Albert is able to do what he does. I am actually eager to listen to this episode and learn from him."

Albert Cheng (00:05:10):
That is super nice. Thank you, Jorge. I've learned so much from him. I'm the type of weird person that likes to wake up before their kids and pull up a bunch of browser tabs and look at experiments. So it was perfect that Jorge brought me into the growth world at Duolingo, learned a ton of best practices, and he's just a great guy. Thanks, Jorge.

Lenny Rachitsky (00:05:27):
We're already getting into these tactics. I love it. Let me just give a little framing on what I want to do with this conversation. What I want to try to do is to help people learn tools and mental models for finding growth opportunities for their own products and essentially learn the growth mentality that you bring into the companies and products that you work on.

(00:05:48):
What I want to start with is to give us a little insight into how you became what you became. There's an interesting pattern I found across a bunch of recent guests, which is many people were very good at piano when they were younger and were very serious piano players. For example, Head of ChatGPT, Nick Turley was almost going to become professional jazz pianist. You were very serious as a piano player earlier in your career. How did you go from pianist to one of the top growth minds in the world briefly?

Albert Cheng (00:06:18):
Well, that's very flattering, but I appreciate it. Yeah, I grew up playing a lot of piano. My parents were immigrants from Taiwan and I was the oldest kid that they had, and so I definitely felt that strong encouragement, if you will, to learn a bunch of things, take them seriously, study hard, and so I did. And my parents, even though they weren't musically proficient, they had a deep love for classical music.

(00:06:45):
So I was the stereotypical baby that would listen to Mozart, I guess when I was sleeping type of thing. And I still vividly remember we had this upright Yamaha piano, and at the very top of the piano we had this countdown clock from 90 minutes. Literally every single day of my childhood, just practice really, really consistently.

(00:07:04):
At first, I really was irritated by that thing, but as I grew older, I started to appreciate music quite a bit more. But anyway, I think what really accelerated my interest and abilities in piano was I feel like I hit the lottery. I had perfect pitch, and so I was able to quickly understand whether I was playing the right stuff or the wrong stuff and just pick up music pretty rapidly.

Lenny Rachitsky (00:07:29):
What does perfect pitch even mean? Does that mean which note is playing?

Albert Cheng (00:07:29):
Exactly.

Lenny Rachitsky (00:07:29):
Okay.

Albert Cheng (00:07:34):
Exactly.

Lenny Rachitsky (00:07:34):
Wow.

Albert Cheng (00:07:35):
So I could listen to a song and then just a very, very clear understanding of which note I'm supposed to start with and if I'm playing something wrong. So it's very helpful.

Lenny Rachitsky (00:07:35):
Unfair.

Albert Cheng (00:07:44):
It's unfair. Definitely. So anyway, yeah, I got quite good as a teenager in high school and even considered studying at a music conservatory. My intrinsic motivation for music wasn't necessarily as strong at that point, and so I decided to go to engineering school instead, but that would've been an incredibly different career. And to your original point around the relationship between music and growth, I didn't really reflect on this until recently.

(00:08:12):
I have a four-year-old and I'm starting to teach him how to bang on the keys a little bit, but a couple things stand out. One is that I think music and growth, they both rely on this just consistent repetition. You're constantly making mistakes. You have this super tight feedback loop. You have to get really resilient to just making mistakes all the time. And you know that the way of learning is through those mistakes. So that's a thing that I learned very early, and the second thing that occurred to me is that they both have this structural underpinning to them.

(00:08:45):
With growth, you have a growth model, you have metrics, you have experiments, you have channels, things like that. But you also need on a day-to-day basis to have creativity, you got to come up with interesting solutions or hypotheses to test. And the same is true on the music side. You have music theory of scales and stuff, but to create beautiful music, you need that passion, that emotion, that flow. So I think that's the beautiful combination between the two.

Lenny Rachitsky (00:09:09):
Fun fact, my wife bought me piano/singing lessons for Father's Day recently, and I've gotten really into this stuff. So I'm learning how to play very basic piano now and learning to identify notes and hit notes with my voice.

Albert Cheng (00:09:24):
Nice.

Lenny Rachitsky (00:09:25):
What a weird new thing.

Albert Cheng (00:09:25):
Could be your next act.

Lenny Rachitsky (00:09:26):
It could be. I could go the reverse, I could become a professional piano player. Oh man. No, it's so fun, so hard though. I'm just like, my fingers are like, how do you do four freaking keys at once? I'm just like "What is going on here?" Okay, so let's get into the meat of it. I want to talk about growth.

(00:09:42):
There's a very specific framework that as we were chatting that I think would be really helpful for people to hear and learn from you. You call it explore and exploit. I think there's a bunch of different ways to think about this. Talk about this framework and how that informs the way you think about growth.

Albert Cheng (00:09:56):
Yeah, I initially came up or heard with, heard about explore and exploit through my engineering partner at Grammarly, Nermal, and I think he actually had taken some reforged classes. So maybe the original inventor of it might be Brian Balfour, who I know has been on your pod. But anyway, it's a great concept.

(00:10:13):
The gist of it is that when you're in exploratory mode, think of it as finding the right mountain to climb. And then when you're in exploitation mode, it's like focusing your resources on climbing that mountain effectively. And certain companies, I think the warning is to basically spend too much of your time on one end of the spectrum. If you do too much exploration, you can have your team feel a little bit too scattershot, just trying a hundred different random ideas.

(00:10:40):
What's the through line? What's the strategy? How do you pattern match successes across them? And if you do too much in exploitation, which is often the MO of growth teams, it can lead to this saturation and stagnation where you're just locally maximizing a thing. And even though this principle of explore and exploit, it's typically thought of as a macro thing. I like to work with my teams more on the insight level. So I'll give you a concrete example.

(00:11:07):
So I work at chess.com and one of our priorities is to encourage chess players to improve, to learn and improve. So one of the PMs that we have, Dylan, he works on all the learning features. The most used learning feature in our product is called game review. So you play a game of chess, after the game is over, we have this virtual coach that teaches you about your worst moves, best moves, et cetera. And his job is to improve user engagement and retention.

(00:11:34):
And so he's in this exploratory phase trying to figure out how do I drive more of that type of activity? And what he observes is that 80% of people that review their games actually do so after a win. And that's really counterintuitive to when we initially built the feature. We thought that people would want to use it after losses or to see their mistakes such they could work on their mistakes. That turned out not to be the truth when it came to the human psychology and the actual data of it. And so we made some changes in the product experience.

(00:12:04):
When you lose a game now as opposed to surfacing your blunders and your horrible stuff that you did, we flip it on its head and so we show you your brilliant moves, your best moves, and we have coach say something encouraging, "Losing, just part of learning, keep it up." That type of thing. That change alone was pretty dramatic for us.

(00:12:22):
It grew game reviews by 25%, subscriptions by 20%, user retention by a lot as well. So that was fantastic, but the point is that it doesn't just stop there. You have to take that insight, share it broadly across the company. Now, adjacent product managers like the PM working on puzzles can now think about, "Okay, how do I audit these cold patterns in my product and think about making them more positive?"

(00:12:48):
I can change the success rating, I could tweak some copy, change the color of some buttons, and so you now can take this experiment win and expand it out 10X across your organization and that's the kind of exploitation phase of it. So when done right, you can oscillate between the two until you saturate out of exploitation mode and then you encourage the teams to brainstorm and get more creative again.

Lenny Rachitsky (00:13:11):
Amazing. Okay, so there's a lot here to follow up on. One is the core piece of advice when you find something that works really well, find ways to build on that learning. One is here's an insight, it can apply to other parts of the product. "Hey teams, here's something we learned unexpected. Maybe this can help you. Also, just keep find more, run more experiments in the same zone." I imagine is a part of that.

Albert Cheng (00:13:34):
Yeah, exactly right. In my experience, the typical win rate, and I hate to use that term for experiments, is often something like 30 to 50%. Usually you're trying a bunch of things, a lot of hypotheses turn out not to be true, consumer products are very unpredictable like that, but when you do find a thing that breaks through the noise, and it could actually be a hugely losing experiment too, those are also super valuable.

(00:13:58):
Surfacing those across the company, the original PM running that experiment doesn't necessarily need to be the person that figures out what you should do for all the other parts of your product experience, but the onus is on them to clearly articulate what their hypothesis is, what they found such that then as a growth leader, I can encourage people to swarm around that and try a bunch of different ideas such that the success rate is up and the impact is up. So it's just oscillating back and forth between the two. That is the magic bullet.

Lenny Rachitsky (00:14:30):
I think another takeaway here/something that I think about when I hear what you're saying is there's often a lot more wins in an area than people expect that you can continue to find wins and growth in something for a long time.

Albert Cheng (00:14:46):
Exactly right. Yes. At the end of the day, users, I think within a company sometimes you can have this siloed approach where you break apart the product experience in 50 different ways and distribute them across different teams, and you assume that users interact with each of the different features with a different mentality, but oftentimes that's actually not necessarily the case. And so sometimes, you can surface an insight that's more human psychology based that can resonate across the entire product experience. And so I think when you can find that, you can double down.

Lenny Rachitsky (00:15:19):
People hearing this might feel like, "Okay, yes, find big wins and then find more." Is there something you find that helps you figure out when to explore versus when to exploit when you've exploited too far? Just like any heuristics or I don't know, ways of helping people guide them along this process of exploring and exploiting?

Albert Cheng (00:15:41):
One thing that I try to focus on at a company of our scale of a chess.com, right? We're running roughly 250 experiments a year. So we're not the highest in the industry, but we run a decent volume. And so when that happens, I invest in these experiment explorer tools and we can talk about AI as well as another way to uncover and pick out these nuggets of wisdom, but basically, these explorer tools can allow me to look across the spectrum of experiments that are going on.

(00:16:08):
Try to figure out if there are patterns between the hypotheses and the learnings that are happening. And if I'm starting to see more and more experiments that are not statistically significant, that may be a signal to me to say, "Okay, we might've tried to exploit a little bit too far. There might not be as much juice to squeeze. Hey guys, let's get back to the table and brainstorm and be a little bit more divergent with our thinking."

Lenny Rachitsky (00:16:34):
Well, let me follow this thread on AI and how you're using AI to help you figure this out. That is very cool. Talk about that.

Albert Cheng (00:16:40):
I think one of the latest things that we've been tinkering around with is this text to SQL capability. It's actually pretty powerful. We have this data request Slack channel where for the longest time, and this is still true today, people will toss in all sorts of just one-off questions. How many subscribers do we have in South Africa? Or how long did somebody play puzzles last month or something?

(00:17:07):
And these ad hoc questions, they often take a lot of human time to just go in and a data analyst needs to prioritize it and find time to go run the query. And yes, you can invest in self-serve tooling to improve at this, but also I found that AI is quite good at doing that first pass answer as well. And so we're working on training some of these Slack bots to essentially be the first party provider of a lot of these answers, which makes the company as a whole lot more data informed, I guess.

(00:17:39):
And I think what's also kind of interesting is that just human nature is that if you have a question that you feel like you might be a bit embarrassed to ask or you don't want to bother someone, you just don't ask the question. And so by the nature of having these tools, you get actually a pretty large explosion of questions being asked. And I think you see this in ChatGPT too, right? It's like just having a thing that you can converse with that you feel comfortable in makes a huge difference.

Lenny Rachitsky (00:18:03):
Okay, this is extremely cool. So is this something you build basically it's a Slack bot that gives you the SQL query or does it actually do the analysis for you?

Albert Cheng (00:18:12):
No, it does the analysis. Yeah.

Lenny Rachitsky (00:18:13):
Whoa, so cool. Okay. Is this something you guys are going to release or is this just like somebody, you guys should just build this at every company?

Albert Cheng (00:18:20):
We should. It's a good idea.

Lenny Rachitsky (00:18:21):
Okay. Okay. Well, there's an episode where everyone in the comments is like, "Open source this." So we'll see if that happens again. That is very cool. Are there other examples of that kind of stuff that you've done or seen?

Albert Cheng (00:18:32):
An adjacent example is a lot of the product managers, we're tinkering around with all sorts of different prototyping tools right now. It's just like go from an idea to a representative solution. Today, there's a lot of humans involved in taking an idea, writing up a spec, doing a review, doing design, et cetera. I'm sure you've interviewed plenty of people that have talked about this specific problem.

(00:18:52):
And so for us, we've invested a bit in at least carving out the main screens of our product experience, things like our onboarding flow, our home screen, our chessboard as an example, and building essentially AI prototypes of those using tools like a V0 or a Lovable. And when you have those foundational pieces, you can then share them with the rest of the company and they can use that as a starting point and then they can try to put their ideas on top of that and then they become a lot more discussable and hopefully testable relatively soon.

Lenny Rachitsky (00:19:25):
What's in your AI stack along those lines?

Albert Cheng (00:19:27):
The PMs are mostly using V0. The designers love Figmas, they're using Figma Make. The engineers are using a combination of tools right now. So Cursor, Cloud Code, GitHub, Copilot. Marketing teams use all sorts of tools for translation, subtitles, content adaptations, et cetera. Customers support uses Intercom then. So there's quite a lot of tools that are used across the company.

(00:19:50):
I would say though that something that is annoying to me is that we haven't yet figured out the bridging from the tinkering to the workflow quite as seamlessly as I would like. And so each sub-function, even though the common I guess wisdom now is that AI is going to strip away these functional titles. It is true that based on your experience, you may gravitate to using a type of tool more. And if that tool isn't as interoperable with some of the other tools that you need to pass down the chain to actually ship it into production, at least at our scale.

(00:20:24):
I think for smaller startups, sure, PMs should just go ship it, but for us, we are still doing some handoffs between functions. I expect that to change over time and we are investing in some of design system components and MCPs and stuff to make it a little bit easier. But yeah, it's an investment and it takes time to smooth things out.

Lenny Rachitsky (00:20:42):
I want to come back to this topic of how things have changed and how you work as a product person, as a growth person across the companies you've been at. But first of all, I want to talk about another example of finding growth wins and monetization wins. Noam Levinsky, who is Chief Product Officer at Grammarly, you worked with him for a while while you were at Grammarly. He said that I need to ask you about the biggest monetization win that you found at Grammarly and how you discovered the opportunity.

Albert Cheng (00:21:10):
I had the pleasure of working with Noam and his product team at Grammarly. Some context first for those that don't use Grammarly. So Grammarly is an AI-powered writing assistant. And so typically, people will use it as a Chrome extension or a downloadable desktop client. And basically what it does is it overlays your writing with a bunch of different-

Lenny Rachitsky (00:21:28):
I use it. I'm a big fan. I use it-

Albert Cheng (00:21:30):
Correction, so you're a big fan.

Lenny Rachitsky (00:21:30):
... And it saves my life.

Albert Cheng (00:21:32):
Fantastic. Glad to hear that. Grammarly is a freemium business model, which means that over 90% of our users are on the free service and the rest of it pay for subscriptions essentially, right? And so one of the teams, they work on subscriber conversion, PM there is Kayla, that team is great and their job is to figure out the free to paid subscription path.

(00:21:54):
And so one of the realizations, one, is that we weren't actually tracking the events that well for the types of essentially suggestions that people were getting and how often were users seeing paywalls and stuff like that. That's kind of step number one. We have to put that instrumentation in. Step number two is that, "Hey, we noticed, actually first let me explain some of the logic."

(00:22:18):
So as a free user, you basically get these underlines across your writing and if you accept all of them, then you see the paywall and that encourages you to subscribe for more nuanced features. As a free user, the main things you get are spelling, grammar, they're basically correctness things. And as a paid user you get that, how do you improve your tone to be more empathetic? How do you improve your writing to be more clear?

(00:22:40):
How can you rewrite entire sentences, that type of thing. And so the observed behavior from all that tracking and data was that actually a very small percentage of our free users was deciding to accept all of their suggestions. They were more picking and choosing as they go, and I wonder if your experience is kind of similar too.

Lenny Rachitsky (00:22:59):
Definitely, yeah. I'm always like, "Wait, stop rewriting everything." Just like this part is wrong. I will fix it. Yeah, I'm very much a pick and choose.

Albert Cheng (00:23:07):
That's right.

Lenny Rachitsky (00:23:07):
Correction person.

Albert Cheng (00:23:08):
And then the second thing, which is I think equally if not more interesting is that I was at this company during this generative AI transformation, which is obviously still going on. And quite frankly, both the company brand as well as the lived product experience for most of the free users was that Grammarly was just a product to fix your spelling and grammar because those were the free suggestions we were showing people.

(00:23:34):
And so we decided to flip that on its head entirely and we said, "Okay, what if we actually sampled a number of different paid suggestions and interspersed them to free users across their writing?" Such that they were intermingled and we would provide a limited taste of what the paid offering had to provide. And on the surface, even though it's rational, the concern is that if we give too much of this away, then will people want to subscribe?

(00:23:58):
And we found completely that was not the case all of a sudden, people were seeing Grammarly as a much more powerful tool than they were before and our upgrade rates nearly doubled just through this change. And so I think this is interesting, just modernization learning that especially if you work on a freemium product, try to have your free product be a reflection of everything that your product can offer you. Obviously to an extent there's some costs involved with some of the paid features and things like that, but it generally will pay for itself if you're able to put your best foot forward and go do that. So that really worked well for us there.

Lenny Rachitsky (00:24:36):
I think this is what converted me to being a paid Grammarly subscriber. Wow, what a genius move. So essentially, it's here's a bunch of improvements, but you get three, I think max, and then it's like, "Okay, now you get upgrade."

Albert Cheng (00:24:51):
It's basically a reverse free trial but in real time while you're writing as opposed to a time-based one. So we adopted some patterns that are in the industry, but molded it to Grammarly's specific use case.

Lenny Rachitsky (00:25:04):
Right. I was going to ask, so it's not like a full trial, it's like a capped trial where you get a certain number of things and then you run out and then they get refreshed. I think once a day or something like that is what I found.

Albert Cheng (00:25:16):
Yeah, you got it.

Lenny Rachitsky (00:25:18):
Yeah. Grammarly is the best/most devious at their upsells. I'm always just like, "God damn it, I'm so close to seeing an improvement, I just have to upgrade." And it's right there, it's right there where my mouse is.

Albert Cheng (00:25:32):
Yeah, well, I'm not proud of being devious, but.

Lenny Rachitsky (00:25:33):
In really getting me to buy the thing. Good job. What was it? Kayla? Okay, nice job Kayla. It's very effective. I love that. Okay, so in terms of the free trial, I don't know, is there anything there of just, there's always this question of freemium, give things away and then there's pro account, there's like trial versus time. Some features are limited. I don't know, do you have for consumer subscription products like here's the way to go?

Albert Cheng (00:26:00):
Yeah, I think first of all, why do freemium subscription in the first place is a common question that I've joined all these companies that are freemium subscription. What do I like about it I guess? Well one, I think it ties really nicely to mission orientation of a lot of these companies. It's often like you want to spread the product as wide as possible because that's why the founders built the thing, right?

(00:26:22):
You're trying to improve education with Duolingo or Grammarly or Chess.com, these are meant to be widespread products with a really wide value proposition that fits globally. And so obviously, the lowest friction to that is going to be a free product. So that alone is part of it. Another part of it is that a lot of these products primarily grow through word of mouth and especially if you can build network effects in the product, like Duolingo has a bunch of social features or with Grammarly, they have a bit of a B2C2B play as well.

(00:26:54):
So you see Grammarly being used by teams and by companies and whatnot, and even if users are on the free plan, they still provide quite a lot of value in making sure that Grammarly can be purchased by a coworker or by a team member or whatever. So I think these things are usually why I lean toward make sure that the core value proposition that you're providing users is free and is permanently free and then you layer on a sampling or a taste of some of the premium features that are on top of it. That's usually the sweet spot that I've seen.

(00:27:26):
As to the trials, reverse trials type of thing, I think it largely depends. I think if you have especially a B2B feature where you may have some lock-in, reverse trials can be super powerful. You just want to get people in there. You don't need to ask for their credit card because they're using your CRM or they're investing quite a lot of time in building out material and content. And so by the time that window drops, you actually feel, "Oh man, I probably should keep this and start paying." I think for a lot of consumer products it's a little bit harder for that to work. And so I've typically seen more just normal free trials be the norm.

Lenny Rachitsky (00:28:03):
Let me follow this thread of just consumer subscription products. I feel like this is the category that every indie developer dreams of building a product in because it's easy to build. Cool, I'll build an app, I add a paywall, and then they realize this is a lot harder than I thought. From a perspective of distribution and CAx and growth like that, is that the biggest missing piece that people don't get about building a successful consumer subscription product?

Albert Cheng (00:28:31):
Yeah, user retention is gold for consumer subscription companies. If you don't retain your users, then a lot of the onus is on getting them to pay on day one, that's super hard. Then you're dealing with totally different business models where you're paying for users, you're trying to aggressively upsell them before they hit any habitual usage patterns with your product.

(00:28:53):
A lot of apps naturally do that because that's how they break the mold and get their first users to do it, but I don't know, I've been fortunate to join companies after that initial phase, but especially take Duolingo and Chess.com, these are organic word of mouth driven businesses and in both ways, they grew the market from a much smaller market and as opposed to it being a very competitive space where you're competing and taking market share from others and bidding for higher terms and stuff like that. So I don't know, there's something to that.

Lenny Rachitsky (00:29:26):
So what I'm hearing here is you need to find a way to grow through word of mouth for this to have any chance of success and also retention needs to be very high. Do you have a heuristic of what retention needs to be for you to have a chance building a successful consumer subscription business?

_[723 additional lines trimmed for context budget]_

---

### How to drive word of mouth | Nilan Peiris (CPO of Wise)
**Guest:** Alexander Embiricos | **Date:** 2023-09-24 | [YouTube](https://www.youtube.com/watch?v=xZifSLGOrrw)  

# How to drive word of mouth | Nilan Peiris (CPO of Wise)

## Transcript

Lenny Rachitsky (00:00:00):
You lead work on Codex.

Alexander Embiricos (00:00:01):
Codex is OpenAI's coding agent. We think of Codex as just the beginning of a software engineering teammate. It's a bit like this really smart intern that refuses to read Slack, doesn't check Datadog unless you ask it to.

Lenny Rachitsky (00:00:12):
I remember Karpathy tweeted the gnarliest bugs that he runs into that he just spends hours trying to figure out nothing else has solved, he gives it to Codex, lets it run for an hour and it solves it.

Alexander Embiricos (00:00:21):
Starting to see glimpses of the future where we're actually starting to have Codex be on call for its own training. Codex writes a lot of the code that helps manage its training run, the key infrastructure. So we have a Codex code review that's catching a lot of mistakes. It's actually caught some pretty interesting configuration mistakes. One of the most mind-blowing examples of acceleration, the Sora Android app, like a fully new app, we built it in 18 days and then 10 days later, so 28 days total, we went to the public.

Lenny Rachitsky (00:00:45):
How do you think you win in this space?

Alexander Embiricos (00:00:47):
One of our major goals with Codex is to get to proactivity. If we're going to build a super system, has to be able to do things. One of the learnings over the past year is that for models to do stuff, they're much more effective when they can use a computer. It turns out the best way for models to use computers is simply to write code. And so we're kind of getting to this idea where if you want to build any agent, maybe you should be building a coding agent.

Lenny Rachitsky (00:01:04):
When you think about progress on Codex, I imagine you have a bunch of evals and there's all these public benchmarks.

Alexander Embiricos (00:01:10):
A few of us are constantly on Reddit. There's praise up there and there's a lot of complaints. What we can do is as a product team just try to always think about how are we building a tool so that it feels like we're maximally accelerating people rather than building a tool that makes it more unclear what you should do as the human?

Lenny Rachitsky (00:01:24):
Being at OpenAI, I can't not ask about how far you think we are from AGI.

Alexander Embiricos (00:01:28):
The current underappreciated limiting factor is literally human typing speed or human multitasking speed.

Lenny Rachitsky (00:01:35):
Today, my guest is Alexander Embiricos, product lead for Codex, OpenAI's incredibly popular and powerful coding agent. In the words of Nick Turley, head of ChatGPT and former podcast guest, "Alex is one of my all time favorite humans I've ever worked with, and bringing him and his company into OpenAI ended up being one of the best decisions we've ever made." Similarly, Kevin Weil, OpenAI's CPO, said, "Alex is simply the best."

(00:01:59):
In our conversation, we chat about what it's truly like to build product at OpenAI, how Codex allowed the Sora team to ship the Sora app, which became the number one app in the app store in under one month. Also, the 20x growth Codex is seeing right now and what they did to make it so good at coding, why his team is now focused on making it easier to review code, not just write code, his AGI timelines, his thoughts on when AI agents will actually be really useful, and so much more. A huge thank you to Ed Bayes, Nick Turley, and Dennis Yang for suggesting topics for this conversation. If you enjoy this podcast, don't forget to subscribe and follow it in your favorite podcasting app or YouTube. And if you become an annual subscriber of my newsletter, you get a year free of 19 incredible products, including a year free of Devin, Lovable, Replit, Bolt, n8n, Linear, Superhuman, Descript, Wispr Flow, Gamma, Perplexity, Warp, Granola, Magic Patterns, Raycast, ChatPRD, Mobbin, PostHog, and Stripe Atlas. Head on over to lennysnewsletter.com and click Product Pass.

(00:02:55):
With that, I bring you Alexander Embiricos, after a short word from our sponsors.

(00:03:00):
Here's a puzzle for you. What do OpenAI, Cursor, Perplexity, Vercel, Plaid, and hundreds of other winning companies have in common? The answer is they're all powered by today's sponsor, WorkOS. If you're building software for enterprises, you've probably felt the pain of integrating single sign-on, SCIM, RBAC, audit logs, and other features required by big customers. WorkOS turns those deal blockers into drop-in APIs with a modern developer platform built specifically for B2B SaaS. Whether you're a seed stage startup trying to land your first enterprise customer or a unicorn expanding globally, WorkOS is the fastest path to becoming enterprise ready and unlocking growth. They're essentially Stripe for enterprise features. Visit workos.com to get started or just hit up their Slack support where they have real engineers in there who answer your questions super fast. WorkOS allows you to build like the best, with delightful APIs, comprehensive docs and a smooth developer experience. Go to workos.com to make your app enterprise ready today.

(00:04:01):
This episode is brought to you by Fin, the number one AI agent for customer service. If your customer support tickets are piling up, then you need Fin. Fin is the highest performing AI agent on the market with a 65% average resolution rate. Fin resolves even the most complex customer queries. No other AI agent performs better. In head to head bake offs with competitors, Fin wins every time. Yes, switching to a new tool can be scary, but Fin works on any help desk with no migration needed, which means you don't have to overhaul your current system or deal with delays in service for your customers. And Fin is trusted by over 6,000 customer service leaders and top companies like Anthropic, Shutterstock, Synthesia, Clay, Vanta, Lovable, Monday.com and more. And because Fin is powered by the Fin AI Engine, which is a continuously improving system that allows you to analyze, train, test, and deploy with ease, Fin can continuously improve your results too. So if you're ready to transform your customer service and scale your support, give Fin a try for only 99 cents per resolution. Plus Fin comes with a 90-day money back guarantee. Find out how Fin can work for your team at fin.ai/Lenny. That's fin.ai/lenny.

(00:05:14):
Alexander, thank you so much for being here and welcome to the podcast.

Alexander Embiricos (00:05:18):
Thank you so much. I've been following for ages and I'm excited to be here.

Lenny Rachitsky (00:05:21):
I'm even more excited. I really appreciate that. I want to start with your time at OpenAI. So you joined OpenAI about a year ago. Before that, you had your own startup for about five years. Before that, you were a product manager at Dropbox. I imagine OpenAI is very different from every other place you've worked. Let me just ask you this, what is most different about how OpenAI operates and what's something that you've learned there that you think you're going to take with you wherever you go, assuming you ever leave?

Alexander Embiricos (00:05:49):
By far, I would say the speed and ambition of working at OpenAI are just dramatically more than what I can imagine. And I guess it's kind of an embarrassing thing to say because everyone who's a startup founder thinks like, "Oh yeah, my startup moves super fast and the talent bar is super high and we're super ambitious." But I have to say, working in OpenAI just made me reimagine what that even means.

Lenny Rachitsky (00:06:11):
We hear this a lot about feels like every AI company is just like, "Oh my God, I can't believe how fast they're moving." Is there an example of just like, "Wow, that wouldn't have happened this quickly anywhere else"?

Alexander Embiricos (00:06:20):
The most obvious thing that comes to mind is just the explosive growth of Codex itself. I think it's a while since we bumped our external number, but it's like the 10x-ing of Codex's scale was just super fast in a matter of months and it's well more since then. And once you've lived through that, or at least speaking for myself, having lived through that now, I feel like anytime I'm going to spend my time on building tech product, there's that speed and scale that I now need to meet.

(00:06:52):
If I think of what I was doing in my startup, it moved way slower and there's always this balance with startups of how much do you commit to an idea that you have versus find out that it's not working and then pivot. But I think one thing I've realized at OpenAI is the amount of impact that we can have and, in fact, need to have to do a good job is so high that I have to be way more ruthless with how I spend my time now.

Lenny Rachitsky (00:07:15):
Before we get to Codex, is there a way that they've structured the org or, I don't know, the way that OpenAI operates that allows the team to move this quickly? Because everyone wants to move super fast. I imagine there's a structural approach to allowing this to happen.

Alexander Embiricos (00:07:29):
I mean, so one thing is just the technology that we're building with has just transformed so many things from both how we build, but also what kinds of things we can enable for users. And we spend most of our time talking about the sort of improvements within the foundation models, but I believe that even if we had no more progress today with models, which is absolutely not the case, but even if we had no more progress, we are way behind on product. There's so much more product to build. So I think just the moment is ripe, if that makes sense.

(00:08:01):
But I think there's a lot of counterintuitive things that surprised me when I arrived as far as how things are structured. One example that comes to mind is when I was working on my startup and before that, when I was at Dropbox, it was very important, especially as a PM to always rally the ship and it was like make sure you're pointed in the right direction and then you can accelerate in that direction. But here, I think because we don't exactly know what capabilities will even come up soon and we don't know what's going to work technically, and then we also don't know what's going to land even if it works technically, it's much more important for us to be very humble and learn a lot more empirically and just try things quickly. And the org is set up in that way to be incredibly bottoms up.

(00:08:45):
This is, again, one of those things that, as you were saying, everyone wants to move fast. I think everyone likes to say that they're bottoms up, or at least a lot of people do, but OpenAI is truly, truly bottoms up. And that's been a learning experience for me that now it'll be interesting if I ever work at... I don't think it'll even make sense to work at a non-AI company in the future. I don't even know what that means. But if I were to imagine it or go back in time, I think I would run things totally new.

Lenny Rachitsky (00:09:10):
What I'm hearing is this ready, fire, aim is the approach more than ready, aim, fire. And there's something, and as you process that, because that may not come across well, but I actually have heard this a lot at AI companies is because you don't know, and Nick Turley shared I think the same sentiment, because you don't know how people will use it it doesn't make sense to spend a lot of time making it perfect. It's better to just get it out there in a primordial way, see how people use it, and then go big on that use case.

Alexander Embiricos (00:09:39):
Yeah. Okay, to use this analogy a little bit, I feel like there is an aim component, but the aim component is much fuzzier. It's kind of like, roughly what do we think can happen? Someone I've learned a ton from working here is a research lead, and he likes to say that at OpenAI, we can have really good conversations about something that's a year plus from now, and there's a lot of ambiguity in what will happen, but that's a right sort of timeline. And then we can have really good conversations about what's happening in low months or weeks. But there's this awkward middle ground, which was as you start approaching a year, but you're not at a year where it's very difficult to reason about, right?

(00:10:18):
And so as far as aiming, I think we want to know, "Okay, what are some of the futures that we're trying to build towards?" And a lot of the problems we're dealing with in AI, such as alignment are problems you need to be thinking out really far out into the future. So we're kind of aiming fuzzily there, but when it comes down to the more tactically like, "Oh yeah, what product will we build and therefore how will people use that product?" That's the place where we're much more like, "Let's find out empirically."

Lenny Rachitsky (00:10:41):
That's a good way of putting it. Something else that when people hear this, people sometimes hear companies like yours saying, "Okay, we're going to be bottoms up. We're going to try a bunch of stuff. We're not going to have exactly a plan of where it's going in the next few months." The key is you all hire the best people in the world. And so that feels like a really key ingredient in order to be this successful at bottoms up work.

Alexander Embiricos (00:11:02):
It just super resonates with me. I was just, again, surprised or even shocked when I arrived at the level of individual drive and autonomy that everyone here has. So I think the way that OpenAI runs, you can't read this or listen to a podcast and be like, "I'm just going to deploy this to my company." Maybe this is a harsh thing to say, but I think very few companies have the talent caliber to be able to do that. So it might need to be adjusted if you were going to implement this.

Lenny Rachitsky (00:11:34):
Okay. So let's talk Codex. You lead work on Codex. How's Codex going? What numbers can you share? Is there anything you can share there? Also, just not everyone knows exactly what Codex is, explain what Codex is.

Alexander Embiricos (00:11:45):
Totally, yeah. So I had the very lucky job of living in the future and leading products on Codex. And Codex is OpenAI's coding agent. So super concretely, that means it's an IDE extension, a VS code extension that you can install or a terminal tool that you can install. And when you do so, you can then basically pair with Codex to answer questions about code, write code, run tests, execute code, and do a bunch of the work in that thick middle section of the software development lifecycle, which is all about writing code that you're going to get into production.

(00:12:21):
More broadly, we think of Codex as what it currently is just the beginning of a software engineering teammate. So when we use a big word like teammate, some of the things we're imagining are that it's not only able to write code, but actually it participates early on in the ideation and planning phases of writing software and then further downstream in terms of validation, deploying and maintaining code.

(00:12:46):
To make that a little more fun, one thing I like to imagine is if you think of what Codex is today, it's a bit like this really smart intern that refuses to read Slack and doesn't check Datadog or Century unless you ask it to. And so no matter how smart it is, how much are you going to trust it to write code without you also working with it? So that's how people use it mostly today is they pair with it. But we want to get to the point where it can work just like a new intern that you hire, you don't only ask them to write code, but you ask them to participate across the cycle. So you know that even if they don't get something right the first try, they're eventually going to be able to iterate their rate there.

Lenny Rachitsky (00:13:21):
I thought the point about not reading Slack and Datadog was it's just not distracted, it's just constantly focused and is always in flow. But I get what you're saying there is it doesn't have all the context on everything that's going on.

Alexander Embiricos (00:13:31):
Yeah. And that's not only true when it's performing a task, but again, if you think of the best team and teammates, you don't tell them what to do. Maybe when you first hire them, you have a couple meetings and you're like, "Hey," you learn, "Okay, these prompts work for this teammate, these prompts don't. This is how to communicate with this person." Then eventually you give them some starter tasks, you delegate a few tasks. But then eventually you just say like, "Hey, great. Okay, you're working with this set of people in this area of the code base. Feel free to work with other people on other parts of the code base too, even. And yeah, you tell me what you think makes sense to be done." And so we think of this as proactivity and one of our major goals with Codex is to get to proactivity.

(00:14:09):
I think this is critically important to achieve the mission of OpenAI, which is to deliver the benefits of AGI to all humanity. I like to joke today that AI products, and it's a half joke, they're actually really hard to use because you have to be very thoughtful about when it could help you. And if you're not prompting a model to help you, it's probably not helping you at that time. And if you think of how many times the average user is prompting AI today, it's probably tens of times. But if you think of how many times people could actually get benefit from a really intelligent entity, it's thousands of times per day. And so a large part of our goal with Codex is to figure out what is the shape of an actual teammate agent that is helpful by default.

Lenny Rachitsky (00:14:54):
When people think about Cursor and even Cloud Code, it's like a IDE that helps you code and auto completes code and maybe does some agentic work. What I'm hearing here is the vision is different, which is it's a teammate. It's like a remote teammate, a building code for you that you talk to and ask to do things. And that also does IDE, auto complete and things like that. Is that a kind of a differentiator in the way you think about Codex?

Alexander Embiricos (00:15:18):
It's basically this idea that if you're a developer and you're trying to get something done, we want you to just feel like you have superpowers and you're able to move much, much faster. But we don't think that in order for you to reap those benefits, you need to be sitting there constantly thinking about, "How can I invoke AI at this point to do this thing?" We want you to be able to plug it in to the way that you work and have it just start to do stuff without you having to think about it.

Lenny Rachitsky (00:15:44):
Okay. I have a lot of questions along those lines, but just how's it going? Is there any stats, any numbers you can share about how Codex is doing?

Alexander Embiricos (00:15:49):
Yeah, Codex has been growing absolutely explosively since the launch of GPT-5 back in August. There's definitely some interesting product insights to talk about as to how we unlock that growth, if you're interested. But again, the last stat we shared there was we were well over 10x since August. In fact, it's been 20x since then. Also, the Codex models are serving many trillions of tokens a week now, and it's basically our most served coding model. One of the really cool things that we've seen is that the way that we decided to set up the Codex team was to build a really tightly integrated product and research team that are iterating on the model and the harness together. And it turns out that lets you just do a lot more and try many more experiments as to how these things will work together.

(00:16:35):
And so we were just training these models for use in our first party harness that we were very opinionated about. And then what we've started to see more recently actually is that other major API coding customers are now starting to adopt these models as well. And so we've reached a point where actually the Codex model is the most served coding model in the API as well.

Lenny Rachitsky (00:16:55):
You hinted at this, what unlocked this growth, I'm extremely interested in hearing that. It felt like before, I don't know, maybe this was before you joined the team, it just felt like Cloud Code was killing it. Just everyone was sitting on top of Cloud Code. It was by far the best way to code. And then all of a sudden Codex comes around. I remember Karpathy tweeted that he just has never seen a model like this. I think the tweet was the gnarliest bugs that he runs into that he just spends hours trying to figure out nothing else has solved, he gives it to Codex, lets it run for an hour and it solves it. What'd you guys do?

Alexander Embiricos (00:17:30):
We have this strong sort of mission here at OpenAI basically to build AGI. And so we think a lot about how can we shape the product so that it can scale. Earlier I was mentioning like, "Hey, if you're an engineer, you should be getting help from AI thousands of times per day," and so we thought a lot about the primitives for that when we launched our first version of Codex, which was Codex Cloud. And that was basically a product that had its own computer, lived in the cloud, you could delegate to it. And the coolest part about that is you could run many, many tasks in parallel. But some of the challenges that we saw are that it's a little bit harder to set that up, both in terms of environment configuration, like giving the model the tools it needs to validate its changes and to learn how to prompt in that way.

(00:18:20):
My analogy for this is, going back to this teammate analogy, it's like if you hired a teammate, but you're never allowed to get on a call with them and you can only go back and forth asynchronously over time. That works for some teammates and eventually that's actually how you want to spend most of your time. So that's still the future, but it's hard to initially adopt. And so we still have that vision of like, that's what we're trying to get you to, a teammate that you delegate to and then is proactive, and we're seeing that growing. But the key unlock is actually first you need to land with users in a way that's much more intuitive and trivial to get value from.

(00:18:54):
So the way that most people discover, the vast majority of users discover Codex today is either they download an IDE extension or they run it in their CLI and the agent works there with you on your computer interactively. And it works within a sandbox, which is actually a really cool piece of tech to help that be safe and secure, but it has access to all those dependencies. So if the agent needs to do something, it needs to run a command, it can do so within the sandbox. We don't have to set up any environment. And if it's a command that doesn't work in the sandbox, it can just ask you. And so you can get into this really strong feedback loop using the model. And then over time, our team's job is to help turn that feedback loop into you as a byproduct of using the product, configuring it so that you can then be delegating to it down the line.

(00:19:38):
And again, analogy, keep coming back to it, but if you hire a teammate and you ask them to do work, but you just give them a fresh computer from the store, it's going to be hard for them to do their job. But if as you work with them side by side, you could be like, "Oh, you don't have a password for this service we use, here's the password for this service. Yeah, don't worry, feel free to run this command," then it's much easier for them to then go off and do work for hours without you.

Lenny Rachitsky (00:20:01):
So what I'm hearing is the initial version of Codex was almost too far in the future. It's like a remote in the cloud agent that's coding for you asynchronously. And what you did is, "Okay, let's actually come back a little bit, let's integrate into the way engineers already integrate into IDs and locally and help them on ramp to this new world,"

Alexander Embiricos (00:20:21):
Totally. And it was quite interesting because we dogfood product a ton at OpenAI. So dogfood as in we use our own product. And so Codex has been accelerating OpenAI over the course of the entire year, and the cloud product was a massive accelerant to the company as well. It just turns out that this was one of those places where the signal we got from dogfooding is a little bit different from the signal you get from the general market because at OpenAI, we train reasoning models all day and so we're very used to this kind of prompting and think upfront, run things massively in parallel and it would take some time and then come back to it later asynchronously. And so now when we build, we still get a ton of signal from dogfooding internally, but we're also very cognizant of the different ways that different audiences use the product.

Lenny Rachitsky (00:21:12):
That's really funny. It's like live in the future, but maybe not too far in the future. And I could see how everyone at OpenAI is living very far in the future, and sometimes that won't work for everyone.

Alexander Embiricos (00:21:23):
Yeah.

Lenny Rachitsky (00:21:23):
What about just intelligence training data? I don't know, is there something else that helped Codex accelerate its ability to actually code? Is it better, cleaner data? Is it more just models advancing? Is there anything else that really helped accelerate?

Alexander Embiricos (00:21:38):
Yeah, so there's a few components here. I guess you were mentioning models and the models have improved a ton. In fact, just last Wednesday, we shipped GPT-5.1-Codex-Max, a very accurately named model, that is awesome. It is awesome both because it is for any given task that you were using GPT-5.1-Codex for, it's roughly 30% faster at accomplishing that task. But also it unlocks a ton of intelligence. So if you use it at our higher reasoning levels, it's just even smarter. And that tweet you were saying Karpathy made about, "Hey, give this your gnarliest bugs," obviously there's a ton going on in the market right now, but Codex-Max is definitely carrying that mantle of us tackling the hardest bugs. So that is super cool.

(00:22:28):
But I will say it's like some of how we're thinking about this is evolving a little bit from being like, "Yeah, we're just going to think about the model and let's just train the best model," to really thinking about what is an agent actually overall? And I'm not going to try to define agent exactly, but at least the stack that we think of it as having is it's like you have this model, really smart reasoning model that knows how to do a specific kind of task really well, so we can talk about how we make that possible. But then actually we need to serve that model through an API into a harness, and both of those things also have a really big role here.

(00:23:02):
So for instance, one of the things that we're really proud of is you can have GPT-5.1-Codex-Max work for really long periods of time. That's not normal, but you can set it up to do that or that might happen. But now routinely we'll hear about people saying, "Yeah, it ran overnight or it ran for 24 hours." And so for a model to work continuously for that amount of time, it's going to exceed its context window. And so we have a solution for that, which we call compaction.

(00:23:28):
But compaction is actually a feature that uses all three layers of that stack. So you need to have a model that has a concept of compaction and knows like, "Okay, as I start to approach this context window, I might be asked to prepare to be run in a new context window." And then at the API layer, you need an API that understands this concept and has an endpoint that you can hit to do this change. And at the harness layer, you need a harness that can prepare the payload for this to be done. So shipping this compaction feature that now just made this behavior possible to anyone using Codex actually meant working across all three things. And I think that's increasingly going to be true.

(00:24:02):
Another maybe underappreciated version of this is if you think about all the different coding products out there, they all have very different tool harnesses with very different opinions on how the model should work. So if you want to train a model to be good at all the different ways it could work, maybe you have a strong opinion that it should work using semantic search. Maybe you have a strong opinion that it should call bespoke tools or maybe you have, in our case, a strong opinion that it should just use the shell and work in the terminal, you can move much faster if you're just optimizing for one of those worlds. So the way that we built Codex is that it just uses the shell, but in order to make that safer and secure, we have a sandbox that the model is used to operating in.

(00:24:45):
So I think one of the biggest accelerants, to go all the way back to answer to your question, is just we're building all three things in parallel and tuning each one and constantly experimenting with how those things work with a tightly integrated product and research team.

Lenny Rachitsky (00:24:59):
Do you think you win in this space? Do you think it'll always be this kind of race with other models constantly leapfrogging each other? Do you think there's a world where someone just runs away with it and no one else can ever catch up? Is there a path to just, "We win"?

Alexander Embiricos (00:25:15):
Again, comes back to this idea of building a teammate, and not just a teammate that participates in team planning and prioritization, not just a teammate that really tests its code and helps you maintain and deploy it. But even a teammate... If you think, again, an engineering teammate, they can also schedule a calendar invite or move standup or do whatever, right? And so in my mind, if we just imagine that every day or every week some crazy new capability is just going to be deployed by a research lab, it's just impossible for us as humans to keep up and use all this technology. So I think we need to get to this world where you kind of just have an AI teammate or super assistant that you just talk to and it just knows how to be helpful on its own. So you don't have to be reading the latest tips for how to use it, you've plugged it in and it just provides help.

(00:26:10):
So that's kind of the shape of what I think we're building. And I think that will be a very sticky winning product if we can do so. So the shape that in my head, at least I have, is that we build... Maybe a fun topic is like, "Is Chat the right interface for AI?" I actually think Chat is a very good interface when you don't know what you're supposed to use it for. In the same way that if I think of I'm on MS Teams or in Slack with a teammate, Chat is pretty good. I can ask for whatever I want. It's kind of the common denominator for everything. So you can chat with a super assistant about whatever topic you want, whether it be coding or not. And then if you are a functional expert in a specific domain such as coding, there's a GUI that you can pull up to go really deep and look at the code and work with the code.

(00:26:54):
So I think what we need to build as OpenAI is basically this idea of you have Chat, ChatGPT and not as a tool that's ubiquitously available to everyone, you start using it even outside of work to just help you. You become very comfortable with the idea of being accelerated with AI. So then you get to work and you just can naturally just, "Yeah, I'm just going to ask it for this and I don't need to know about all the connectors or all the different features. I'm just going to ask it for help and it'll surface to me the best way that it can help at this point in time and maybe even chime in when I didn't ask it for help." So in my mind, if we can get to that, I think that's how we really build the winning product.

Lenny Rachitsky (00:27:32):
This is so interesting because with my chat with Nick Turley, the head of ChatGPT, I think he shared that the original name for ChatGPT was Super Assistant or something like that. And it's interesting that there's that approach to the super assistant and then there's this Codex approach. It's almost like the B2C version and the B2B version. And what I'm hearing is the idea here is, okay, you start with coding and building and then it's doing all this other stuff for you, scheduling meetings, I don't know, probably posting in Slack, I don't know, shipping designs. I don't know, is the idea that this is the business version of ChatGPT in a sense, or is there something else there?

Alexander Embiricos (00:28:08):
Yeah. So we're getting to the one-year time horizon conversation. A lot of this might happen sooner, but in terms of fuzziness, I think we're at the one year. So I'll give you a contention and a plausible way we get there, but as for how it happens, who knows? So basically, if we're going to build a super assistant, it has to be able to do things. So we're going to have a model and it's going to be able to do stuff affecting your world. And one of the learnings I think we've seen over the past year or so is that for models to do stuff, they're much more effective when they can use a computer.

(00:28:41):
Right, okay, so now we're like, okay, we need the super assistant that can use a computer, or many computers. And now the question is, okay, well, how should it use the computer? And there's lots of ways to use a computer. You could try to hack the OS and use accessibility APIs, maybe a bit easier as you could point and click. That's a little slow and unpredictable sometimes. And another way, it turns out the best way for models to use computers is simply to write code. So we're kind of getting to this idea where, well, if you want to build any agent, maybe you should be building a coding agent and maybe to the user, a non-technical user, they won't even know they're using a coding agent, the same way that no one thinks about are they using the internet or not, which is they're more just like, "Is WiFi on?"

(00:29:23):
So I think that what we're doing with Codex is we're building a software engineering teammate, and as part of that, we're kind of building an agent that can use a computer by writing code. And so we're already seeing some pull for this. It's quite early, but we're starting to see people who are using Codex for coding adjacent product purposes. And so as that develops, I think we'll just naturally see that, oh, it turns out we should just always have the agent write code if there is a coding way to solve a problem instead of... Even if you're doing a financial analysis, maybe write some code for that.

(00:29:55):
So basically like you were like, "Hey, is this the two ends of this product for the super assistant of ChatGPT?" In my mind, just coding is a core competency of any agent including ChatGPT. And so really what we think we're building is that competency. So here's the really cool thing about agents writing code is that you can import code. Code is composable, interoperable. Because one very reductive view we could have for an agent is it's just going to be given a computer and it's just going to point and click and go around. But that is the future. And then how we get there is difficult to chart a path because a lot of the questions around building agents aren't like, "Can the agent do it?" But it's more about, "Well, how can we help the agent understand the context that it's working in?" And the team that's using it probably has a way that they like to do things. They have guidelines. They probably want certain deterministic guarantees about what the agent can or cannot do. Or they want to know that the agent understands this detail.

(00:30:57):
An example would be if we're looking at a crash reporting tool, hitting a connector for it, every sub-team probably has a different meta prompt for how they want the crashes to be analyzed. And so we start to get to this thing where, yeah, we have this agent sitting in front of a computer, but we need to make that configurable for the team or for the user and let them... Stuff that the agent does often, we probably just want to build in as a competency that this agent has that it can do.

(00:31:24):
So I think we end up with this generalizable thing, that you were saying, of an agent that can just write its own scripts for whatever it wants to do. But I think that the really key part here is can we make it so that everything that the agent has to do often or that it does well, we can just remember and store so that the agent doesn't have to write a script for that again? Or maybe if I just joined a team and you are already on the same team as me, I can just use all those scripts that the agents had written already.

Lenny Rachitsky (00:31:53):
Yeah, it's like if this is our teammate, they can share things that it's learned from working with other people at the company. It just makes sense as a metaphor.

Alexander Embiricos (00:32:01):
Right. Yeah.

Lenny Rachitsky (00:32:02):
It feels like you're in the Karpathy camp of, "Agents today are not that great and mostly slop and maybe in the future they'll be awesome." Does that resonate?

Alexander Embiricos (00:32:11):
So I think coding agents are pretty great. I think we're seeing a ton of value there.

Lenny Rachitsky (00:32:11):
Yeah, that feels right. That feels right, yeah.

Alexander Embiricos (00:32:17):
And then I think agents outside of coding, it's still very early. And this is just my opinion, but I think they're going to get a whole lot better once they can use coding too in a composable way. It's kind of the fun part of when you're building for software engineers, at my startup, we were building for software engineers too for a lot of that journey, and they're just such a fun audience to build for because they also like building for themselves and are often even more creative than we are in thinking about how to use the technology. So by building for software engineers, you get to just observe a ton of emergent behaviors and things that you should do and build into the product.

Lenny Rachitsky (00:32:54):
I love how you say that because a lot of people building for engineers get really annoyed because the engineers they're just always complaining about stuff. They're like, "Ah, that sucks. Why'd you build it this way?" I love that you enjoy it, but I think it's probably because you're building such an amazing tool for engineers that can actually solve problems and just code for them.

(00:33:12):
Kind of along those lines, there's always this talk of what will happen with jobs, engineers, coding, do you have to learn coding? All these things. Clearly the way you're describing it is it's a teammate, it's going to work with you, make you more superhuman, it's not going to replace you. What's the way you just think about the impact on the field of engineering, having this super intelligent engineering teammate?

Alexander Embiricos (00:33:33):
I think there's two sides to it, but the one we were just talking about is this idea that maybe every agent should actually use code and be a coding agent. And in my mind, that's just a small part of this broader idea that, hey, as we make code even more ubiquitous... I mean, you could probably claim it's ubiquitous today, even pre AI, right? But as we make code even more ubiquitous, it's actually just going to be used for many more purposes. And so there's just going to be a ton more need for humans with this competency.

(00:34:01):
So that's my view. I think this is quite a complex topic. So it's something we talk about a lot and we have to see how it pans out. But I think what we can do basically as a product team building in the space is just try to always think about how are we building a tool so that it feels like we're maximally accelerating people rather than building a tool that makes it more unclear what you should do as the human?

(00:34:27):
I think, to give an example right now, nowadays when you work with a coding agent, it writes a ton of code, but it turns out writing code is actually one of the most fun parts of software engineering for many software engineers. So then you end up reviewing AI code. And that's often a less fun part of the job for many software engineers. So I actually think we see that this plays out all the time in a ton of micro decisions. So we as a product team, we're always thinking about, "Okay, how do we make this more fun? How do we make you feel more empowered? Where is this not working?" And I would argue that reviewing agent written code is a place that today is less fun.

(00:35:04):
So then I think, "Okay, what can we do about that?" Well, we can ship a code review feature that helps you build confidence in the AI written code. Okay, cool. Another thing we could do is we can make it so that the agent's better able to validate its work. And it gets all the way down into micro decisions. If you're going to have an agent capability to validate work, and let's say you have... I'm thinking of Codex Web right now, you have a pain that sort of reflects the work the agent did, what do you see first? Do you see the diff or do you see the image preview of the code it wrote? And I think if you're thinking about this from perspective, "How do I empower the human? How do I make them feel as accelerated as possible?" You obviously see the image first. You shouldn't be reviewing the code unless first you've seen the image, unless maybe it's been reviewed by an AI and now it's time for you to take a look.

Lenny Rachitsky (00:35:49):
When I had Michael Truell, the CEO of Cursor on the podcast, he had this kind of vision of us moving to something beyond code. And I've seen this rise of something called spec-driven development where you just write the spec and then the AI writes code for you. So you start working at this higher abstraction level. Is that something you see where we're going, just like engineers not having to actually write code or look at code and there's going to be this higher level of abstraction that we focus on?

Alexander Embiricos (00:36:16):
Yeah. I mean, I think there's constantly these levels of abstraction and they're actually already played out today. Today, coding agents, mostly it's prompt to patch. We're starting to see people doing spec-driven development or planned and driven development. That's actually one of the ways when people ask, "Hey, how do you run Codex on a really long task?" Well, it's like often collaborate with it first to write a plan.md, like a markdown file that's your plan. And once you're happy with that, then you ask it to go off and do work. And if that plan has verifiable steps, it'll work for much longer. So we're totally seeing that.

(00:36:50):
I think spec-driven development is an interesting idea. It's not clear to me that it'll work out that way because a lot of people don't like writing specs either, but it seems plausible that some people will work that way. A bit of a joke idea though is if you think of the way that many teams work today, they often don't necessarily have specs, but the team is just really self-driven and so stuff just gets done. And so almost that it's like, I'm coming up with this on the spot, so it's not a good name, but chatter-driven development where it's just like stuff is happening on social media and in your team communications tools. And then as a result, code gets written and deployed.

(00:37:29):
So yeah, I think I'm a little bit more oriented in that way of I don't even necessarily want to have to write a spec. Sometimes I want to, only if I like writing specs. Other times I might just want to say like, "Hey, here's the customer service channel and tell me what's interesting to know, but if it's a small bug, just fix it." I don't want to have to write a spec for that, right?

(00:37:51):
I have this sort of hypothetical future that I like to share sometimes with people as a provocation, which is in a world where we have truly amazing agents, what does it look like to be a solopreneur? And one terrible idea for how it could look is that actually there's a mobile app and every idea that the agent has to do is just vertical video on your phone and then you can swipe left if you think it's a bad idea and you can swipe right if it's a good idea. And you can press and hold and speak to your phone if you want to give feedback on the idea before you swipe. And in this world, basically what your job is is just to plug in this app into every single signal system or system of record, and then you just sit back and swipe. I don't know.

Lenny Rachitsky (00:38:39):
I love this. So this is like Tinder meets TikTok meets Codex.

Alexander Embiricos (00:38:42):
It's pretty terrible.

Lenny Rachitsky (00:38:43):
No, this is great. So the idea here is this agent is watching and listening to you, paying attention to the market, your users, and it's like, "Cool, here's something I should do." It's like a proactive engineer just like, "Here, we should build this feature, fix this thing."

Alexander Embiricos (00:38:56):
Exactly. Exactly.

Lenny Rachitsky (00:38:58):
I think it's a really good idea.

Alexander Embiricos (00:39:00):
Communicating with you in the lowest effort way for your consumers.

Lenny Rachitsky (00:39:02):
Yeah, yeah, the modern way we communicate, swipe left to right and vertical feed. And then the Sora video, okay, so I see how this all connects now. I see.

Alexander Embiricos (00:39:11):
Yeah. To be clear, we're not building that, but it's a fun idea. I mean, in this example though, one of the things that it's doing is it's consuming external signals, right? I think the other really interesting thing is if we think about what is the most successful AI product to date, I would argue, it's funny actually not to confuse things at all, but the first time we used the brand Codex at OpenAI was actually the model powering GitHub Copilot. This is way back in the day, years ago. And so we decided to reuse that brand recently because it's just so good, Codex, code execution.

(00:39:46):
But I think actually auto completion and IDEs is one of the most successful AI products today. And part of what's so magical about it is that when it can surface ideas for helping you really rapidly, when it's right, you're accelerated. When it's wrong, it's not that annoying. It can be annoying, but it's not that annoying. So you can create this mixed initiative system that's contextually responding to what you're attempting to do.So in my mind, this is a really interesting thing for us as OpenAI as we're building.

(00:40:22):
So for instance, when I think about launching a browser, which we did with Atlas, in my mind, one of the really interesting things we can then do is we can then contextually surface ways that we can help you as you're going about your day. And so we break out of this, we're just looking at code or we're just in your terminal into this idea that, "Hey, a real teammate is dealing with a lot more than just code. They're dealing with a lot of things that are web content. So how can we help you with that?"

Lenny Rachitsky (00:40:51):
Man, there's so much there. I love this. Okay, so auto complete on web with the browser. That's so interesting. Just like, "Here's all the things that we can help you with as you're browsing and going about your day."

(00:41:01):
I want to talk about Atlas. I'll come back to that. Codex, code execution, did not know that. That's really clever. I get it now. Okay, and then this chatter, what is a chatter-driven development? No, this is a really good idea, but it reminds me, I had Dhanji on the podcast, CTO of Block, and they have this product called Goose, which is their own internal agent thing. And he talked about an engineer at Block just has Goose watch him with his screen and listens to every meeting and proactively does work that he should probably want to do. So ships to PR, sends an email, drafts a Slack message. So he's doing exactly what you're describing in kind of a very early way.

_[384 additional lines trimmed for context budget]_

---

### Iâ€™ve run 75+ businesses. Hereâ€™s why youâ€™re probably chasing the wrong idea. | Andrew Wilkinson
**Guest:** Andrew Wilkinson | **Date:** 2025-07-03 | [YouTube](https://www.youtube.com/watch?v=VxAwUb86MUE)  

# Iâ€™ve run 75+ businesses. Hereâ€™s why youâ€™re probably chasing the wrong idea. | Andrew Wilkinson

## Transcript

Andrew Wilkinson (00:00:00):
You don't want to walk into the gym on day one and try and deadlift 300 pounds. So when someone comes to me and they're a first time entrepreneur and they say, "I'm going to make the next great AI company," I think that is the equivalent.

Lenny Rachitsky (00:00:12):
I feel like you've actually started and run more companies than maybe anyone else in the world. What is your best advice for coming up with a great startup idea?

Andrew Wilkinson (00:00:20):
Charlie Munger, Warren Buffett's longtime business partner, has this amazing quote.

Speaker 3 (00:00:24):
Fish where the fish are.

Andrew Wilkinson (00:00:26):
The biggest mistakes I've made have been going into business models where other people have repeatedly failed and thinking, I can do this better.

Lenny Rachitsky (00:00:34):
It's so funny to watch you on Twitter. Clearly you've become AI obsessed.

Andrew Wilkinson (00:00:38):
It's like having the world's most reliable employee who costs $200 a month and works 24/7. So many knowledge work jobs are going to change massively. I think the fundamental question is, do all jobs just become a single prompt?

Lenny Rachitsky (00:00:54):
Today, my guest is Andrew Wilkinson. Andrew is the co-founder and CEO of Tiny, a holding company that's often called the Berkshire Hathaway of the internet. They own over 40 businesses ranging from Dribble to WeCommerce to the AeroPress coffee maker, and they focused on buying profitable businesses from founders and holding them for the long term. Andrew and his co-founder bootstrapped the business from zero to hundreds of millions of dollars in value, and Andrew personally was worth over $1 billion at one point. In our wide-ranging conversation, we cover a bunch of strategies for coming up with a good business idea, what common business ideas you should avoid, his experience automating much of his work and life using AI, and what that means for employment in the near future. Also, what he's learned about happiness and money and how they're not directly related, and also how getting diagnosed with ADHD and then taking SSRIs was the thing that most impacted his happiness in life.

(00:01:46):
This is both a powerful and also a very tactically useful conversation and I'm really excited for you to hear it. If you enjoy this podcast, don't forget to subscribe and follow it in your favorite podcasting app or YouTube. Also, if you become an annual subscriber of my newsletter, you get a year free of a bunch of amazing products including Bolt, Linear, Superhuman, Notion, Perplexity, Granola and more. Check it out at Lenny's newsletter.com and click bundle. With that, I bring you Andrew Wilkinson. This episode is brought to you by Sauce. The way teams turn feedback into product impact is stuck in the past, vague reports, static taxonomies, unactionable insights that don't move business metrics. The result, churn, lost deals, misgrowth. Sauce is the AI product copilot that helps CPOs and product teams uncover business impact and act faster. It listens to your sales calls, support tickets, churn reasons, and lost deals, surfacing the biggest product issues and opportunities in real time.

(00:02:41):
It then routes them to the right teams to turn signals into PRDs, prototypes, and even code that drives revenue retention and adoption. That's why Whatnot, Linktree, Incident.io and Zip use Sauce one enterprise uncovered a product gap that unlocked $16 million ARR, another caught a issue and prevented millions in churn. You can too at sauce.app/lenny. Sauce built for AI product teams, don't get left behind.

(00:03:10):
This episode is brought to you by Enterpret. Enterpret is a customer intelligence platform used by a leading CXN product orgs like Canva, Notion, Perplexity, Strava, Hinge, and Linear to leverage the voice of the customer and build best-in-class products. Enterpret unifies all customer conversations in real time, from Gong recordings to Zendesk, tickets to Twitter threads, and makes it available for your team for analysis and for action. What makes Enterpret unique is its ability to build and update a customer-specific knowledge graph that provides the most granular and accurate categorization of all customer feedback and connects that customer feedback to critical metrics like revenue and CSAT. If modernizing your voice of customer program to a generational upgrade is a 2025 priority, like customer-centric industry leaders like Canva, Notion, Perplexity, and Linear, reach out to the team at enterpret.com/lenny. That's E-N-T-E-R-P-R-E-T.com/lenny.

(00:04:12):
Andrew, thank you so much for being here. Welcome to the podcast.

Andrew Wilkinson (00:04:15):
Oh, thanks man, great to be here.

Lenny Rachitsky (00:04:17):
I've been wanting to chat with you for so long. There's so much that I want to talk about and I want to start with a topic that I know that you think a lot about and that it's also in the minds of a lot of people, which is coming up with a great startup idea and this is something that a lot of people are thinking about right now because AI makes it so easy to actually build your idea into something real, and I feel like this is something you spent a lot of time thinking about. I feel like you've actually started and run more companies than, I don't know, maybe anyone else in the world. I feel like you're in the top a hundred, top 10, something like that. I don't know. Does that feel right?

Andrew Wilkinson (00:04:51):
I've definitely had a lot of experiences and I've probably started or been involved with 75 different projects or businesses where I've been a primary contributor and I wouldn't say that's anything to brag about because I've been an inch deep in a mile wide. So that's been in a lot of ways kind of my Achilles heel is I get too excited about ideas and I start too many businesses, but as a result I've seen almost every business model under the sun.

Lenny Rachitsky (00:05:18):
Amazing. Okay. Yeah, I think the benefit is for us is we get to learn from your experience. Let me just ask you this question, what is your best advice for coming up with a great startup idea?

Andrew Wilkinson (00:05:28):
Ultimately the best thing is something you're going to be interested in, but I think a mistake a lot of people make is they choose something that everybody is interested in. So for example, they say I don't know anyone that has hadn't had the thought, man, I would love to start a really cool restaurant or I want to have a cool cafe or something. And in reality, what they think about is how cool would it be to come up with an amazing logo or all fun stuff, design the menu and stuff. But in reality, operating those businesses is miserable and it's also a very hard business because every morning millions of people wake up and they go, I should start a cafe. But on the flip side, almost nobody wakes up every morning and says, "You know what? I'd love to start a funeral home," or "I would like to start a pest control business," or "I should start software that helps people fill out forms for the government."

(00:06:26):
And Charlie Munger, Warren Buffett's longtime business partner, has this amazing quote. He says, "Fish where the fish are." And he gives this example, he says, "If you're a fisherman and you see a large pond and all around the pond, there's a whole bunch of fishermen and they're all elbowing each other out of the way. They're all using the best lures, the best fishing line, they all have amazing strategy. You actually want to walk off into the forest and find a small fishing hole with lots of fish and very little competition. And I think that's probably the most important thing in business is actually to find those niches where you can actually make real money because competition equals lower margin. The more competitors there are, the lower your prices have to be and the more competitive the business is ultimately.

Lenny Rachitsky (00:07:19):
It doesn't feel intuitive to go after small markets and to find niches. So let's just follow that thread. Well, why is that actually a source of some of the best ideas starting really small and niche?

Andrew Wilkinson (00:07:30):
I don't necessarily think it has to be something that is small forever, but it has to be something that like I think about if you're a first time entrepreneur or a student or something, you don't want to walk into the gym on day one and try and deadlift 300 pounds. So when someone comes to me and they're a first time entrepreneur and they say, "I'm going to make the next great AI company," or "I'm going to launch a new bank," or something like that, something that is very, very rigorous and complicated and highly competitive and regulated, I think that is the equivalent. I think you really want to take the baby weights and start slowly building your muscle. And I think about my own experience starting a business, I was so lucky because the first business I ever started my web design agency, which became Metalab, that was so easy and it worked immediately because all I had to do was know how to build websites and be able to talk to potential customers.

(00:08:33):
And then once they said, "Yes, I will pay you $5,000," I just had to send them an invoice, do the work, and that was it. It was a very simple business. And because of that, I got immediate positive feedback and I built my own narrative. And my narrative was, "I'm good at business, I can do this, keep going." And then I went off and I started taking my money that I made from that original business and that's when I fucked up. So I went out and I started a pizzeria and I lost all my money. I started a designer cat furniture business, a online DJ school, a skin cream business, all of these things. I've just lost all my money almost immediately, but because I had that first win I kept on going. And I just think it's so critical that people choose a business where they get that initial win.

Lenny Rachitsky (00:09:22):
Okay, this is a great topic. This is just like how do you avoid creating a job for yourself that you hate? There's a lot of business opportunities and ideas that like, "Yeah, you can make this work," and then it works, and then you're stuck doing this thing running a restaurant. My wife tells a story where a friend started a coffee shop and then he's like, "I thought I was starting a coffee shop, but I'm just replacing milk and buying milk all day. That's my job now." So just kind of along those lines to help people avoid creating a beast that they didn't expect any advice for Just how to know this is maybe even though this may work and make money, you probably don't want to be spending your life doing this sort of business.

Andrew Wilkinson (00:09:59):
So I just started a pressure washing business. I was speaking at a local business school and after I spoke, one of the kids walked up to me and he said, "Hey, I'm an entrepreneur. I've started two or three businesses in the past doing landscaping and I'm not enjoying school." And so just on the spot, I said to him, "Why don't you drop out and we'll start a business together?" And I'd been cooking on this idea of a pressure washing business because I'd studied that industry a little bit and I knew a few friends who'd done it in other cities. And I had this unfair advantage, which is I owned a whole bunch of media properties where I could advertise it for free basically. And so we started this business and I think for him, he kind of had this moment of, do I really want to be washing people's driveways for the rest of my life?

(00:10:47):
And what I said to him is, "Look, if this gets to scale, you'll never touch a pressure washer unless you want to." If the business can get big enough where you can have employees, he can just focus on sales or digital marketing or whatever aspect of the business that he really loves. And I think that's the biggest thing that a lot of entrepreneurs miss. They say, "I don't want to do that for the rest of my life. I don't want to be in the back of a dry cleaner dry cleaning clothes." And to me it's just a question of scale. I think the cafe is a great example because a cafe, if it doesn't get to a reasonable scale is just a job.

(00:11:26):
There's a big difference between a business and a job. I think if you could start a pressure washing business where you're the only employee, yeah, that's a job, but if you can get to a scale where you can drive 10 leads a day, then you don't have to do any of the pressure washing and you just do what you love. So I think a lot of people have this kind of Protestant work ethic where they think, well, I've got to be the person to do everything. And I think they really need to lean into what I call lazy leadership, which is how do I get away from the things I hate as quickly as humanly possible? How do I be Teflon for tasks?

Lenny Rachitsky (00:12:00):
It's interesting that this business is non-software related at all, this pressure washing business. So I guess just for folks that are coming up with ideas, and I imagine most ideas are there's a pull to make it software oriented, something AI is going to help build for you and help you run. What's your calculus on just going down a direction of real world physical business, like a restaurant power washing business versus a software just like what are the benefits of that direction when should someone actually seriously think about doing a business like a power washing business?

Andrew Wilkinson (00:12:30):
I think if someone's listening to this podcast, odds are they're someone who's kind of a digital native. And I think the question is, what's your unfair advantage and what are you great at? So for me, I think I have reasonably good taste. And so what I could do is I identify and I knew enough to identify great design and development talent, but mostly I was lucky because I was a talker. I was good at meeting with clients and selling myself. And so ultimately my highest and best use my superpower was sales. And so that can be applied to anything, anything where you need to go out and you need to sell a customer. And so really it just comes down to what do you get drawn to and then how do you find the profitable business within that? So here's an example. A friend of mine, he owns a restaurant and he said, "Oh, I love it. I'm so passionate about it. It's this stunning beautiful restaurant in my hometown."

(00:13:32):
And he said, "But it's really just a job for a few different people and we can't make any money at it, but I've been noticing that there's all these vendors that service the restaurant and those guys are making a killing." And so he told me about a business that cleans grease traps, another one that cleans exhaust vents for kitchens. So I think looking around and seeing where your passion is and then sniffing around within it, probably somewhere within your passion, there's an opportunity. So for example, I love movies. My happy place is a dark theater. That's the best way for me to de-stress is go to a dark theater and watch a movie and get lost in it. And probably about four or five years ago, I was like, "Man, how cool would it be to invest in movies in some way to be a part of that creative world?"

(00:14:26):
And so I started looking into funding movies and I realized that when you fund movies, like 90% of the time you lose all your money. And even if you do make some money, you rarely make a good return. But I spend a bunch of time understanding the industry and learning about it. And then two years ago I was in New Zealand and I met the founder of Letterboxd and I realized, "Oh my God, this is a business where this has a moat, so it has a network effect, it's a huge social network for film reviewers. It's something I'm passionate about and it's something that we can buy at a fair price." All those things came together and I was like, "Oh my god, I can invest in film now." In the same way I used to be a barista, we ended up buying the AeroPress coffee maker company. So I follow my passions and spend a long time learning different industries and then I find the profitable niche within.

Lenny Rachitsky (00:15:19):
Okay, and this is a great takeaway essentially when you're thinking about startup ideas, make sure there's some connection to something unique to you where you have some unfair advantage. This makes me think about Brian Armstrong. I saw him do a talk once and he gave this really amazing insight about why Coinbase did well and why he started Coinbase. And it's because if you look at his background, he had this really rare Venn diagram of background of computer science and I think it was cryptography and economics, which is the exact set of skills you need to start something like Coinbase. So I think the tip there is just what is that unique Venn diagram of skills in your background and just ideally what you're building connects to that and gives you an unfair advantage.

Andrew Wilkinson (00:16:02):
Well, yeah. I just met another UVic student at the local university student. She is interested in marketing and so she's gone out and she's found some local clients, so small restaurants and stuff and managed their social media and she said, "Yeah, it's okay. I can make a thousand bucks a month, but it's a lot of work," and the owners really want a lot for their money. And I said, "Well, if you just pivot that idea just ever so slightly, and instead of doing restaurants, you did realtors or wealth managers who have quite a large marketing budget and are used to spending serious money and it only has to work a little bit to make a lot of money for them. Those people, you can charge $5,000 a month."

(00:16:44):
So I think often it's you find your passion, your skill set, you zero in, and then you just kind of pivot and you find the most profitable way to do it. When I started my web design firm, I started mostly with local and small projects, but very quickly I found a job board in San Francisco where startups would share projects they needed help with. I could charge five times the amount for five times less work.

Lenny Rachitsky (00:17:09):
This point about fish where the fish are I think is really important. You can start something that's awesome that you love, that you're so excited about, but nobody needs it. Talk a bit more about just what that looks like. What have you seen when you're thinking about ideas that tell you that there's fish but also not overfished as you pointed out?

Andrew Wilkinson (00:17:25):
I think it's hard like Warren Buffett has this great quote, "I'm a better businessman because I'm an investor and I'm a better investor because I'm a businessman." And I feel like in order to know what problems are valuable to solve, you kind of need to have valuable problems. So it's a bit of a hard thing because I remember when I was like 20, I would say, I hate how all the cat furniture I can buy for my cats is I bet people would pay a lot of money to solve this, but I didn't understand anything about the realities of that business model. I didn't understand how little people were actually willing to pay. I didn't have a enough life experience to go, "Yes, that's actually a worthwhile opportunity." And I think that being able to know what people would pay to solve the problem realistically is incredibly valuable.

(00:18:20):
So my example of a realtor selling a house, because I've studied that industry, I know that a realtor can make like 20 to $50,000 selling a single house. So I can intuit that they'd be willing to spend a lot of money if I have a unique way of getting them clients that'll buy houses that'll convert at a high conversion rate that is worth $5,000 a lead maybe, right? So I think you kind of have to understand the problem in depth before you really know. Because when I was starting out, I would go down every rabbit trail, every infomercial idea I had I would think was genius.

Lenny Rachitsky (00:18:59):
Or people, I don't know, that don't have this experience, which is most people. Is there one thing you do, maybe a heuristic that gives you a sense, maybe there's something here, maybe there's a lot of fish, maybe it's a value of prompt or no, the soul tell me it probably isn't.

Andrew Wilkinson (00:19:12):
No, unfortunately, I think it's mostly gut. I mean Munger and Buffett talk about having a whole bunch of mental models in your brain and they form a latticework, right? And they all kind of piece together. And I feel like for me, there's so much of this that is, it's almost like I'm an AI model and I've trained on all this data of what works and what doesn't and what's a good moat and what's a bad business, et cetera. And when I see it, I just immediately know. I mean, I think all entrepreneurs are so lucky now to be able to go into Claude or ChatGPT and just say, "Hey, I'm thinking about starting a Botox clinic. Can you break down the numbers? Is this a good business? What's hard about it? What is the regulatory moat? What would my payroll look like?" I didn't have any of that and so I started so many incredibly stupid businesses, but I think there's no excuse at this point. You should be able to do it with AI.

Lenny Rachitsky (00:20:07):
That's a really good tip actually. Then there's this point you made about boring is good. That's a really good piece of advice. A lot of people are going after flashy stuff, things they condemn on Twitter. Your advice here is boring is actually a really good thing because fewer people are going after it. Is that the general tip?

Andrew Wilkinson (00:20:22):
Totally. I mean, so I started this business, one of my first called Flow and Flow was basically Asana before Asana came out. So it was a way to manage your to-do's and projects with your team in a web app and we basically did the thing. We made the mistake that so many entrepreneurs make, which is to go after an industry that everybody goes after just like cafes. Everybody has the idea, man, I wish I could design my own project management system or my own to-do system. And not only that, but everybody loves new things. They love jumping around. I know me personally, in the last three years, I've probably used three different productivity systems and I love jumping around in them and I didn't understand that and I ended up losing $10 million trying to compete with venture-backed businesses and bootstrapping.

(00:21:17):
It was all my own money, poured $10 million, lit it on fire trying to compete with Asana because I didn't understand how the business world worked really as they had raised hundreds of millions of dollars and they were run by the co-founder of Facebook and it was a little bit like, I'm Fiji and I'm deciding I'm going to invade the United States in retrospect, just completely silly. Now on the flip side, if I'd taken that same amount of energy and I'd instead focused on, let's see, what's a really boring one I've heard about, there was a business I saw recently.

(00:21:52):
They were making $30 million a year and all they did was help people fill out forms to get government assistance in certain programs. So it'd be like your uncle is disabled and you need to access government funding. The process to do it is incredibly time-consuming and miserable and you have to fill out all these forms. They just have software that fills out the forms for you and it says, look, you're going to get $20,000 as a grant, pay us a thousand bucks, and that is such a boring, nobody wakes up and goes, "I want to make form filling software," but I think they would if they could make 20 million a year

Lenny Rachitsky (00:22:29):
Along these lines of just bad ideas, things people shouldn't start, what are a few ideas that you think everyone thinks is going to be a great idea and then they do it and then they always fail?

Andrew Wilkinson (00:22:37):
Well, I think the biggest mistake, I can speak from my own experience, the biggest mistakes I've made business-wise have been going into business models where other people have repeatedly failed and thinking, I can do this better. So for example, me and one of my best friends about 10 years ago, we really wanted to start a bar. We just thought it'd be so cool to have a bar where us and all of our friends could go, we thought it'd be great for the city. We thought we can run this really high margin because we're tech guys, we know how to build systems, we're good at business, and we were utterly humbled. We realized that we didn't know anything about business compared to someone who runs a pizzeria. If you think about it, like my example earlier of if you run a software company, what has to happen?

(00:23:24):
You have to hire a bunch of nerds, they need internet connections and computers and you need to pay them and you need to coordinate online and everyone can work asynchronously. Nothing has to go right. It's not that complicated, at least at small-scale. A pizzeria, it's like if the baker doesn't wake up at three in the morning and start prepping dough, the entire thing is effed and all along the way there's a hundred different failure points from front of house, back of house, the deliveries, arriving on time, all this logistics. And so I think it's been stuff like that. I mean also I got into the news business, the local news business. I ended up buying an old paper in Vancouver and it's the exact same thing. It's like you can't take a brilliant management team and change a bad business model. Ultimately the business model wins.

Lenny Rachitsky (00:24:20):
This is really good advice. Basically if there's a bunch of dead bodies in that space, there's probably something there that keeps killing them that you're probably not aware of until maybe somebody figures something out, right? Once in a while, once in a blue moon, someone's like, "Okay, here's how we do this and then it works."

Andrew Wilkinson (00:24:34):
Well, look at Instacart. When Instacart came out, everyone said, "Well, Webvan failed. This is never going to work." And we still don't necessarily know if Instacart is a great business. I don't know, I haven't studied it, but I think it was like, okay, enough technology has changed that it can happen, but would it be easier to start Instacart, Amazon or Coupang or would it be easier to start an enterprise SaaS software company? Definitely the enterprise SaaS software company.

Lenny Rachitsky (00:25:02):
I see. So this is I guess is the advice there. The thing that is easy to start is the thing you should not do because everyone's going after that.

Andrew Wilkinson (00:25:10):
Well, it goes back to my gym analogy of if you're going to deadlift 300 pounds, trained for 15 years, you should have already had three startups and they should have worked and you've really built up the reps. And I think that if I was going to start Instacart, that's different than someone who's 20 starting it. Not that I would be good at it, but at least I would know what I'm getting into.

Lenny Rachitsky (00:25:32):
Let's talk about something that is this endless debate on Twitter. Maybe it's a false dichotomy between lifestyle businesses, bootstrap businesses, this idea of not raising money, just making revenue, living off the revenue versus venture-backed venture scale companies feels like you're very good at the first bucket and a lot of people, this is their dream, "I'm just going to start something. I'll start a lifestyle business, make a few million a year, never raise money. Who needs VCs? VCs suck." Advice for deciding which route to go with an idea you have.

Andrew Wilkinson (00:26:03):
Well, I think it's just not true that a "lifestyle" or bootstrap business can't get huge. I mean, we bootstrapped the entire business and now across all of our companies we do almost $300 million in revenue. And the whole time I had been focusing on taking, starting small businesses, small ideas, simple ideas, often buying small companies and watching them grow really big. And I think that the only difference between what we do and what a venture capitalist does is the level of tolerance of burning money on fire. And I just haven't seen that If we choose the right businesses that aren't in incredibly competitive markets or where they have some sort of moat. So what I mean by that is a great brand or a network effect like a social network or something like that, I don't feel we're holding them back by not letting them light up a bunch of money on fire because these things naturally grow.

(00:27:12):
The numbers, I mean they're like balloons, they just go as long as we don't mess them up too much. So I think the decision ultimately comes down to how hairy do you want your big, hairy, audacious goal to be? And if your big, hairy, audacious goal is I'm going to start the next satellite business that creates some sort of crazy technological revolution. Yes, you're going to have to raise a venture capital unless you're already a billionaire or something like that. But if you're wanting to just tinker and solve a problem that you think is not going to be hyper-competitive, so for example, that form-filling thing or software that just does a narrow thing that doesn't require 20, 30, 40 million to be lit on fire before it can make money, then you can have a wonderful life and build a wonderful company that can scale into the hundreds of millions of dollars if you play your cards right without ever raising money.

Lenny Rachitsky (00:28:09):
Per the story you shared with Flow where you're competing against a venture-funded company, if there is a venture-backed company in this space, is the advice you're not going to win competing with them most likely and try to do something else.

Andrew Wilkinson (00:28:22):
Well, look at Things. Do you know Things?

Lenny Rachitsky (00:28:25):
I have used Things

Andrew Wilkinson (00:28:26):
I do too. It's awesome. Things still exist and Things has existed for 20 years. It's run by I believe one or two or three people. It's not a big team, certainly under 10 people, it was, I believe, bootstrapped and it's just consistently delivered an exceptional product and built up a loyal following. They have their 10,000 true fans who use it, and that's enough for them, but they don't do a lot. They've been very intentional. They don't do any AI stuff, they don't have an API. They're not on Android. They're very focused and I think they have succeeded. But the question is, what would an MBA or a business professor say about their success? If the measure of success is did they maximize taking as much market share as possible and grow to be as big as possible, then the answer is no because Asana and other people have built multi-billion dollar companies.

(00:29:28):
If the goal is the founder has an incredible life, probably has three houses, flies all over the place, does whatever he wants, has recurring revenue and he gets to work with headphones on building beautiful piece of software that people love, then I think he's won. I am generally much more in the camp of the Things guy has won, not Dustin Moskovitz. Dustin Moskovitz is just playing a different game than the Things guy. And if Dustin Moskovitz was running Things, he'd be miserable and if the Things guy was running Asana, he would probably kill himself because he wants to put his headphones on and build.

Lenny Rachitsky (00:30:05):
I wonder why nobody has come. It feels like just with lifestyle businesses, if it's working and you would think somebody would come in with more money and more funding and just eat their lunch, is the key that the market is too small for a VC to ever be excited and so that's why no one's coming for them?

Andrew Wilkinson (00:30:21):
I think so. I mean, I think Things probably, I'm just guessing, I don't know any of the numbers or whatever, but I would assume it makes between five and $25 million of revenue, which to a VC is not even worth considering a VC. For a VC to invest in your business, you need to have a story where it's worth 300 million to a billion dollars or more. So it's just not that exciting to them, which again, going back to fish where the fish are, you don't want to be fighting against the commercial fishermen. If you just want to have a little business where you get enough fish to feed your family and your village or whatever, find the other fishing hole. Don't go where the trawlers are.

Lenny Rachitsky (00:31:04):
This is great advice if you're trying to start a non venture backed company is find. This metaphor just keeps working for us, which is fish where the fish are, but not where the professional fishermen are. Also, ideally not where there's just a ton of, I don't know, fishermen from all over the place. I don't know, solopreneur fishermen. Okay. Coming back to just starting a successful company, what would you say are the keys to an amazing business model, an amazing business broadly, what should you be thinking about and looking into? I know you spent a lot of time thinking about this. When you're buying companies, what are a few bullet points that you want to focus on?

Andrew Wilkinson (00:31:38):
What I do for a living is basically buy businesses. So in my early career I started one business, then I started about 10 more, and then I realized starting businesses was extremely hard and I was doing pretty well. I was making quite a bit of money and I'd sold one of my businesses and I was looking out at the next 10 years and really asking myself what do I want my career to be and am I happy doing what I'm doing? And the answer was no. I did not starting businesses and experiencing that failure rate, it was incredibly stressful. And so I picked up a book about investing and I got lucky. The first one I bought was about Warren Buffett, and for those that don't know, Warren Buffett is he's counter to every VC kind of hustle culture thing you might hear because he basically just sits quietly in a room and reads all day, despite owning 260 different businesses and being one of the 10 wealthiest people in the world, his life is incredibly calm.

(00:32:44):
He only does what he wants and he spends most of his time quietly reading and once or twice a year, he makes a big decision to buy a business. And so when I read about Warren Buffett, I just thought, wow, I'm a sucker. I'm running around like a crazy person trying to run all these businesses. Why am I not just buying businesses and letting them run? And so when I'm buying a business, what I'm really looking for is something that I can't mess up. Right now, obviously we buy a business, we try not to mess it up, and we're very intentional, actually very odd in that when Tiny buys a company, we generally just leave them alone. If they already have management in place, we say nothing changes, no one should know that we've even bought them. Obviously people know that, but really there should be no change whatsoever.

(00:33:31):
The only change that we generally make is if the founder wants to leave the business, then we'll bring in a CEO to run the company, and that's probably the most important decision that we make. So I'm looking for a business though where it is so good that it's hard to mess up, and most businesses are very easy to mess up. One person leaves and the whole business falls apart because it's held together with dental floss and duct tape. And so I'm really looking for what Warren Buffett would call a moat. So a moat is basically a brand, so that could be a brand like Coca-Cola, Tylenol, Advil, something like that. So something that gives you pricing power where you can consistently charge and you have loyal customers or where we usually find a lot of opportunity is in network effects.

(00:34:23):
So typically we're looking for a community that's gotten so big where people don't want to go elsewhere. So for example, we own Letterboxd, which is the largest social network for film lovers. And the question is, if someone else wants to compete with us, why would someone go to their social network that has a small number of users when all their friends are already on Letterboxd? And so you see that same kind of thing with Instagram or Facebook or similar. And so we're looking for businesses like that, something that has staying power that is hard to compete with and that we can hold over the very long term.

Lenny Rachitsky (00:35:01):
It was interesting when Elon bought X, how that was such a test of the power of network effects. If you think about it, back then he changed the name. So the brand completely changed the team. 80% of the people left. What was left, it was the network and the network effect and the simplest way. Just for folks that aren't super familiar with network effects, the way I think about it is just network effect is where every additional user that joins the network becomes more useful for everybody.

Andrew Wilkinson (00:35:27):
There's another moat, which is high switching costs, which I don't really like because it's not really very consumer friendly, but an example might be Salesforce where no one wants to, they've spent years and they've done all the implementation and trained everyone on it, and it's kind of the standard, everybody hates it, but ripping it out and switching is such a pain in the that they just won't bother. That's another form of sometimes a great business, but again, that's more depressing. I don't love that one.

Lenny Rachitsky (00:35:56):
What's the last company you guys bought?

Andrew Wilkinson (00:35:58):
We bought Serato. Serato is the largest DJ software company in the world. So if you ever see a DJ playing and they've got a laptop in front of them, probably using Serato to DJ.

Lenny Rachitsky (00:36:09):
Wow, that is super cool. What I'd like a funding to be walking into.

Andrew Wilkinson (00:36:14):
I used to DJ so I was aware of them and it was one of those moments we talked about earlier where I understood the business. They had a really interesting mode of this huge passionate user base and deep hardware integration. And I also love DJing and I was able to analyze the business and luck comes to the prepared mind and it worked out.

Lenny Rachitsky (00:36:35):
This is another great callback to your excellent piece of advice of just ideally what you're working on, whether you're starting the company or buying the company, is you have a unique unfair advantage or some kind of unique background that connects to that idea. Let me ask you one more question before we shift a whole different topic, hint, hint, AI. Something that I know you talk a lot about, something you believe strongly and something you deal with a lot is when you buy companies is just as people and the challenges of management and hiring and people. So what have you learned about just how to successfully, I don't know, find amazing people, keep amazing people, help people be more successful within the companies that you operate?

Andrew Wilkinson (00:37:15):
My business partner, Chris, has a really great quote. He says, "There are no problems. There is only people problems." And so we've found that you could tell us that there's a disaster that one of our business units is about to fail or something like that. As long as I'm surrounded by really great smart people, I feel fine. But when we have a bad actor, we have a psycho or a narcissist or a really horrible, difficult person we're dealing with, all bets are off and life becomes incredibly stressful. So I'd say I spend probably 20% of my time just trying to make sure we're filtering people very carefully and ensuring we're working with people that we enjoy. And I could talk about it in the AI, I've done some really interesting stuff to help me screen and identify difficult people, but I've just found that the biggest mistake I made in my early career was I would hire people because I liked them on gut and I would think that I could change them.

(00:38:12):
So it's kind of like in romantic relationships, it never works out well. If you get married to someone and you go, "Okay, well this is going to be a great relationship as long as I can convert them to Christianity or I can change their parents or their personality," or whatever it is. And so I've just found that I've never been able to change someone. You can never mentor someone out into being a good employee. And the kind of heuristic I have is if I ever think, should I fire this person even once, I should fire them immediately. It has always been a signal, at least for me, that when someone's a superstar, I can't imagine firing them. I think it's impossible, I'd be lost without them.

(00:38:56):
But when I repeatedly start going, "Man, is this person going to work out?" Almost always within six to 12 months, it doesn't work out. And so I really try and be really direct about stuff like that and just cut when it's not working. So I'd say the lesson really is hire for what you need. Don't hire just for potential, which is counter to what a lot of people say. A lot of people say hire for potential and coach them into being whatever you need. I just haven't found that. I think you need to hire someone who already is fully formed and can do what you need. But again, everyone does it differently.

Lenny Rachitsky (00:39:31):
This is big advice. Powerful advice. Is this true not just for the CEOs of the companies you run? As for folks under lower down the ladder too?

Andrew Wilkinson (00:39:39):
Well, for CEOs, one of the most interesting things we've observed is, so I'll give you an example. We had a CEO come in and we were interviewing them and in my opinion, the business needed to grow via organic marketing. In his opinion, it was enterprise sales. And so when we're interviewing him, he keeps going back to his experience building with enterprise sales. And there's this great saying, "To a man with a hammer, everything looks like a nail." And so for him, he's looking at this business and going enterprise sales, that's the way to grow. I hire him, but I say, "Look, if we're going to hire you, we need you to do the organic marketing." Lo and behold, of course, he goes off and he does the enterprise sales thing. And I've just found that hiring CEOs is like they're an elephant and you're the rider.

(00:40:31):
They're going to go wherever it is that they want to go. And so listening incredibly carefully to people's words and their experiences, because generally people will do the thing they tell you. So what I look for when I interview a CEO now, I want to be nodding along. I want to go, that's exactly what I would do or that's way smarter than my idea. And then I just leave them alone because I've found that anytime I try and pull them in a certain direction or coach them or whatever, it just doesn't work. And again, this could be my problem. I might be the world's worst coach and mentor, but for me, that's been what's true.

Lenny Rachitsky (00:41:06):
I imagine there's also an element of they won't love the job if they want to be doing say enterprise sales, and then they're creating viral TikTok videos all day and tweeting. They're like, "What the hell?"

Andrew Wilkinson (00:41:15):
Well, I've also found people will shoot themselves in the foot. If I tell them an idea in a board meeting and I say, "I really need you to try this," it never works because usually they sandbag it, right? They don't really put their heart into it or they're just placating me and they don't want it to work. They want their idea to work. And so I've learned not to do that.

Lenny Rachitsky (00:41:37):
This episode is brought to you by Miro. Every day new headlines are scaring us about all the ways that AI is coming for our jobs, creating a lot of anxiety and fear. But a recent survey for Miro tells a different story. 76% of people believe that AI can benefit their role, but over 50% of people struggle to know when to use it. Enter Miro's innovation Workspace, an intelligent platform that brings people and AI together in a shared space to get great work done. Miro has been empowering teams to transform bold ideas into the next big thing for over a decade. Today, they're at the forefront of bringing products to market even faster by unleashing the combined power of AI and human potential. Teams can work with Miro AI to turn unstructured data like sticky notes or screenshots into usable diagrams, product briefs, data tables, and prototypes in minutes.

(00:42:26):
You don't have to be an AI master or to toggle yet another tool. The work you're already doing in Miro's canvas is the prompt. Help your teams get great work done with Miro. Check it out at Miro.com to find out how. That's M-I-R-O.com.

(00:42:41):
Let's talk about AI. It's so funny to watch you on Twitter. Clearly you've become AI obsessed, AIphile. I don't know what the term is, and I love that you're not just the kind of person that just talks about it and tweets pontifications, you're clearly using it in every facet of your job and looking for more ways to use AI. So I have a lot of questions here because I think this is where more and more founders are going to be where you are today. First of all, just what's in your AI stack these days? What are the tools you find yourself coming back to most or finding most useful?

Andrew Wilkinson (00:43:10):
So the primary tool I use is a tool called Lindy, Lindy.ai. And what it basically lets you do is build workflows and agents. So it's very simple. You might say, "When I get an email in the Gmail API, I want to add an AI agent that reads the email and labels it based on that." Or you can build a crazy Rube Goldberg machine that sends different emails all sorts of different places. So for example, when I get an email and it's related to my kid's school, one of the big problems I have is that my kid's school sends so many emails, the field trip is on Thursday and you need to bring the following things. And meanwhile, there's parent teacher interviews and all these things get added to my calendar. The AI agent automatically takes that, puts it onto my calendar, makes sure there's notes for all the things I need to prepare and stuff. So I have just basically tried to take every single thing a human could do in my inbox and automate it with Lindy.

Lenny Rachitsky (00:44:10):
And you're sitting there in Lindy doing this yourself or do you have people that you've trained to help build these sorts of things for you?

Andrew Wilkinson (00:44:16):
No, I do it myself.

Lenny Rachitsky (00:44:17):
Okay. How many agents, if that's the term you want to use, do you have running for you roughly?

Andrew Wilkinson (00:44:22):
Oh man, I think I probably have not a crazy amount within... Let's see. Well, each workflow probably has four or five agents. So for example, my inbox has four or five agents, and then I have a whole bunch of other, I've got a calendar agent, I've got a meeting agent, I've got an email agent, I've got a scheduler, I've got a CRM, and basically they're all different workflows. So the whole bunch of agents within.

Lenny Rachitsky (00:44:50):
Okay, and you're not an investor in Lindy, right? You're just a fan, super user.

Andrew Wilkinson (00:44:53):
No, no.

Lenny Rachitsky (00:44:54):
Fun fact. I actually am. So this is great.

Andrew Wilkinson (00:44:56):
Oh, no way. That's awesome.

Lenny Rachitsky (00:44:57):
Tiny angel investor. So just want to put that out there, which I love that this came up organically. What are a couple other really interesting use cases, workflows you've built that people might be inspired by it?

Andrew Wilkinson (00:45:08):
Let me go through the email one a little more. So basically emails come in and the first agent says, does Andrew even need to see this? Let's say you're in a thread and you've already chimed in and everyone's just saying, cool, that works. I don't need to see that. So just archives that never. So that immediately reduces my email by about 20%. Then it decides is this something that is time sensitive? Is this something that needs to be dealt with in the next 24 hours? And if so, it labels it in a special 24-hour label. So when I go in my email, one of the biggest stresses I find is I go, "Shit, there's 200 emails in here, and if I don't go through all of them, I don't know if there's an emergency burning somewhere." So now I do. Then it takes any email or every single email and it decides, is this a simple decision?

(00:46:00):
So for example, let's say you email me and you say, "Hey, do you want to get lunch?" So it emails me privately and it says, "Hey, Lenny wants to get lunch. Do you want to say yes? Do you want to say yes, but in a few months? Do you want to say no? How do you want to say no?" And it gives me multiple choice. I just can say four, number four, and then it'll email you as me and it'll write out a nice thoughtful email or whatever. So stuff like that is so freaking cool and helpful. And I'd say that it's replaced stuff that my full-time, I used to have a full-time assistant just working on email that's all completely automated and there's all sorts of other cool stuff I'm doing there. Other agents, I'm working on one right now to manage my calendar, which is really complex. I do a lot of scheduling and a lot of rules. I have another really simple one. It adds emojis to every single calendar event. So when I look at my calendar, I've got beautiful little emojis for every single calendar event. Very silly.

Lenny Rachitsky (00:47:00):
What are the emojis? Do they represent something?

Andrew Wilkinson (00:47:03):
Yeah, so if I write weightlifting, it'll just do a guy weightlifting or whatever. I've got one that, this one's cool. Basically what it does, every time I email someone, it looks them up online, it figures out what city they live in. It checks my CRM and air table. It says, "Okay, I don't know where do you live."

Lenny Rachitsky (00:47:21):
In Marin, in the Bay Area.

Andrew Wilkinson (00:47:22):
Okay. So it'd say, "Okay, you email Lenny, he lives in Marin." It puts in the database you live in Marin. And then next time I travel to the Bay Area, the agent will see two weeks before and it'll say, "I saw you're going to San Francisco. Here's all the people in San Francisco you should try and have coffee with." Right? So just all these things that-

Lenny Rachitsky (00:47:22):
I would pay for that.

Andrew Wilkinson (00:47:43):
Yeah, yeah, exactly. All these things that I've dreamed of having my assistant do that my assistant could just never consistently do because she gets distracted, we're doing an event, there's something going on. It's like having the world's most reliable employee who costs $200 a month and works 24/7.

_[273 additional lines trimmed for context budget]_

---

### Behind the scenes of Calendlyâ€™s rapid growth | Annie Pearl (CPO)
**Guest:** Annie Pearl | **Date:** 2023-02-26 | [YouTube](https://www.youtube.com/watch?v=-tUIGpgmsZw)  

# Behind the scenes of Calendlyâ€™s rapid growth | Annie Pearl (CPO)

## Transcript

Annie Pearl (00:00:00):
Strategy is really just an integrated set of choices that outline how you're going to win in whatever marketplace you choose. And so, a good product strategy is going to answer questions like what's your winning aspiration? But maybe more importantly, where are you going to play? What are the markets you're going to go after? What are the segments of those markets? What are the personas in the segments of those markets? And then, how are you going to win with a target audience?

Lenny (00:00:27):
Welcome to Lenny's Podcast where I interview world-class product leaders and growth experts to learn from their hard one experiences building and growing today's most successful products. Today, my guest is Annie Pearl. Annie is currently chief product officer at Calendly. Before that, she was chief product officer at Glassdoor. And before, that she was director of product management at Fox. She's also a member of Skip, a community for chief product officers, and she's on the board of two different companies. 

(00:00:54):
In our conversation, we cover a lot of ground, including how Calendly builds product, how Calendly has grown, including the wild story of how they got their first 1,000 users, and also how they built a sales team on top of what historically has been a very product-led growth company. Annie also shares a ton of great advice on how to get into product management. I learned a ton from Annie and I know you'll too. Annie also shares a few killer tips for using Calendly, which I loved. And so, with all that, I bring you Annie Pearl through a short word from our wonderful sponsors. 

(00:01:25):
Today's episode is brought to you by Miro, an online collaborative whiteboard that's designed specifically for teams like yours. I have a quick request. Head on over to my Miro board at miro.com/lenny and let me know which guests you'd want me to have on this year. I've already gotten a bunch of great suggestions, which you'll see when you go there, so just keep it coming. And while you're on the Miro board, I encourage you to play around with the tool. It's a great shared space to capture ideas, get feedback, and collaborate with your colleagues on anything that you're working on. 

(00:01:56):
For example, with Miro, you can plan out next quarter's entire product strategy. You can start by brainstorming, using sticky notes, live reactions, a voting tool, even an estimation app to scope out your team's sprints. Then your whole distributed team can come together around wire frames, drive ideas with a pen tool, and then put full mocks right into the Miro board. And with one of Miro's readymade templates, you can go from discovery and research to product roadmaps to customer journey flows to final mocks, all in Miro. Head on over to miro.com/lenny to leave your suggestions. That's M-I-R-O.com/lenny. 

(00:02:32):
This episode is brought to you by Coda. You've heard me talk about how Coda is the dock that brings it all together and how can I help your team run smoother and be more efficient. I know this firsthand because Coda does that for me. I use Coda every day to wrangle my newsletter content calendar, my interview notes for podcasts, and to coordinate my sponsors. More recently, I actually wrote a whole post on how Coda's product team operates. And within that post, they shared a dozen templates that they use internally to run their product team, including managing the roadmap, their OKR process, getting internal feedback, and essentially their whole product development process is done within Koda.

(00:03:10):
If your team's work is spread out across different documents and spreadsheets and a stack of workflow tools, that's why you need Koda. Koda puts data in one centralized location regardless of format, eliminating roadblocks that can slow your team down. Koda allows your team to operate on the same information and collaborate in one place. Take advantage of the special limited time offer just for startups. Sign up today at koda.io/lenny and get $1,000 start credit on your first statement. That's C-O-D-A.io/lenny, the signup and get a startup credit of $1,000, koda.io/lenny.

(00:03:50):
Annie, welcome to the podcast.

Annie Pearl (00:03:53):
Thanks for having me, Lenny. Super excited to be here.

Lenny (00:03:55):
I've been a big fan of yours from afar. We've crossed paths a little bit on Reforge, on Twitter, probably been at events that maybe we didn't know each other at yet. So, I'm really excited to finally be chatting, real life, in real time, at least.

Annie Pearl (00:04:09):
Me as well.

Lenny (00:04:09):
I've got a Calendly question to kick things off. It feels like with Calendly one of the most awkward elements of it is I have to put the burden on someone else to book at Calendly. So, I'm sending a link and I haven't figured out a good way to send it to someone without it coming across like a power move. So, my question to you is how do I send a Calendly to someone without it feeling bad?

Annie Pearl (00:04:31):
All right, well I love this question to kick us off. We actually have a whole blog post about this if you're curious to learn more.

Lenny (00:04:31):
Oh, okay.

Annie Pearl (00:04:37):
But I think that at a high level, I think I recommend first really just kind of opening the door for the person you're trying to schedule time with to share their availability first. So instead of just sending the link, I usually start the email with something like looking forward to connecting, feel free to share, sometimes you're available or if easier, you can choose to find time on my calendar using the Calendly link here. So, opening the door to let them choose before you offer up your Calendly link, I think is a little bit of a subtle way to let them take the lead if they want. 

(00:05:08):
And the second piece I would recommend too is once you opened that door, you can further reduce the effort on the recipient by adding times you're available directly in the email. So, when you go to share a Calendly link, there's an option to add times to email and you can then just paste those directly into the email you're creating, so that reduces yet another sort of point of friction to ask the user to click the link and get taken to Calendly. So, opening the door and then adding times to email are two things that I do to really make sure that it's not awkward and it doesn't put the burden on the other person.

Lenny (00:05:39):
That is awesome advice. That first one is what I ended up doing actually. That's really interesting where you don't send the link immediately. You first just ask, "Hey, send me your Calendly." And actually, I always say, "Send me your Calendly." I assume that's what they're using. That's kind of funny.

Annie Pearl (00:05:53):
Right.

Lenny (00:05:53):
I don't even know what else is out there.

Annie Pearl (00:05:54):
That's good. That's what we like to hear.

Lenny (00:05:56):
Yeah, absolutely. It's like its own word now. Okay, that was awesome. So, there's this already actionable advice for anyone listening. 

Annie Pearl (00:06:03):
Sweet. 

Lenny (00:06:04):
Transitioning a little bit to product, the main focus of chat, you transitioned into product from being a lawyer. You told me at one point that a lot of people ask you for advice about how to transition into product from other functions, especially non-technical functions as someone without a technical background. So, what advice do you give people when they ask you how to transition into a product role?

Annie Pearl (00:06:25):
I got what I'll call lucky, which is I kind of stumbled into product management after law school, joined the founding team of a startup and ended up doing product management there. But when I think about folks who are looking to get in their product management, I think there's really two paths. I think one is more formal in nature. There are associate product manager programs out there and many scaled companies, Google, Meta. All have APM programs that you can formally apply to. And actually, when we were at Box, much earlier stage company than either of those companies I just mentioned, we actually created an APM program to help grow our bench of more junior PM. So, I think you can actually find APM programs even at smaller, earlier stage companies than even kind of big tech. So that's one, it's just formal APM programs.

(00:07:07):
I think another "more formal" way to get into PM is really by just directly applying to a junior PM role where there's no expectation of any sort of experience. I've usually seen this work best when you're already working somewhere in some product adjacency. Maybe you're in customer support, implementation, or maybe you're a sales engineer. But you can look at the internal job board and find junior PM roles that are posted and that's one way to make the move. So that's kind of on the formal side like APM programs and just applying via internal job boards.

(00:07:38):
I think on the informal side, really two suggestions here. The first one is to seek out opportunities to shadow or partner closely with a product manager and maybe even offer to take on some work. So, some of the best PMs that I've brought over to product from other functions, they really start by expressing interest in product and then start partnering closely with the product manager and maybe even doing a little bit of product work before they make that transition.

(00:08:02):
And one tactical suggestion is there's oftentimes companies will have subject matter expert programs where they want to pair someone from a go-to-market function with a certain product squad or a certain product area. And so that's becoming a SME. It allows you to really get more involved and embedded into the product team. So, that's one suggestion. And then maybe last one is just the path I did, which is joining an early stage startup. There's really usually an expectation that everyone's going to get their hands dirty doing a lot of different things. And so, I think that's one way where you might have an opportunity to try product management if you end up joining an early stage company.

Lenny (00:08:38):
So, maybe it was four, maybe it was more paths that you described. Join APM program. What was the second one again?

Annie Pearl (00:08:45):
Internal job board apply, when you're in the company.

Lenny (00:08:48):
As just like a junior PM. Two is, find someone that mentors you and helps you start doing the role. And is that internal? Is that the internal transfer app?

Annie Pearl (00:08:58):
Yeah, exactly. 

Lenny (00:08:58):
Cool. 

Annie Pearl (00:08:59):
And then, another flavor of that is sometimes companies will have these SME programs.

Lenny (00:09:03):
What is a SME program?

Annie Pearl (00:09:05):
Subject matter expert. So, you'll say, "Hey, I want to make sure we have subject matter expert in our CS team on this area of the product." And they'll partner really closely with the product manager and designer within that area.

Lenny (00:09:16):
Got it. And then the fourth bucket is, join a startup, start doing PM work and then you end up being a PM. 

Annie Pearl (00:09:21):
You got it. 

Lenny (00:09:21):
Which of those four do you find most common? And would you push people in one direction or another?

Annie Pearl (00:09:26):
Yeah, I've brought a lot of folks over internally for the path of someone's really interested in product, they express they're interested, they want to help, they want to learn, they're eager, they're curious. And so, they make that really well known and they're even willing to do some work on the side to help out and really show and demonstrate the skills before they have the job. So, I've seen that one to actually probably bring the most folks over in my role in terms of being on the product leadership side.

Lenny (00:09:53):
On the APM program route, are there any APM programs you recommend? Because I'm sure people hear this and they're like, "Yeah, but I don't know where to apply. I don't know which ones are good." I don't know if you have a list, but just what comes to mind as APM programs to go pursue? 

Annie Pearl (00:10:05):
The folks who started it all was Google, with the Google APM program. And Meta obviously has a pretty strong robust APM program. But as I mentioned around Box, I think those are obviously very, very competitive and most people want to get into them. It may be better to try and find a company like Box or a company that's a bit earlier stage, not as scaled to think about looking at those APM programs. And I'm sure if you want to, go to glassdoor.com where I used to work at Glassdoor, so had to throw that in there. You could search for associate product manager and I think you'll find a whole host of open roles that you might be able to apply to. 

Lenny (00:10:39):
That is a cool tip. I haven't heard of that. Go to Glassdoor and search for APM. So, you search for companies that have an APM title.

Annie Pearl (00:10:45):
Yes. You could just use associate product manager and you'll see all the open jobs out there and then go apply to them.

Lenny (00:10:50):
That's cool. Okay, good tip. What I find, and you mentioned this the best, if you have the option is internal transfer. If you're just like another function, you find someone that can help you move into the role.

Annie Pearl (00:11:01):
You have the relationships, you can show your work really well. The other thing I would say is when I think about folks who have successfully transferred over, I think they tend to have a couple of characteristics. They're usually very curious, they tend to be really passionate about the product and solving customer problems. And sometimes, they've even tinkered with a side project as a way to hone their PM skills. So, I think as you're thinking about making that transition, those types of characteristics really showing eagerness and interest in the product itself and solving customer problems are also great ways to get noticed and increase your chances.

Lenny (00:11:36):
Why do you think it is that not more companies have an APM program? It feels like such a win for so many people. Why is it just so rare? 

Annie Pearl (00:11:42):
Yeah, I think when we built this at Box, so drawing on that experience, it was a lot of work. If you're going to do it, you want to do it really well and you want to create an environment where you can help the associate product managers be successful, the goal is to ultimately graduate everyone from the APM program into being a product manager. And so, I think it takes a lot of intentionality and for us, it took a lot of work. We had to make sure we had clarity around the interview process. We had to make sure we had clarity around expectations in the role. We wanted to have a training element. 

(00:12:16):
We wanted to make sure that, again, we're setting people up for success. So, I think companies have to be at a stage of scale where they can really invest and they have the excess capacity to build the program in a way. I think that's going to help make sure everyone who comes through it has a chance at really learning, growing and ultimately being successful.

Lenny (00:12:33):
That's the same thing we found at Airbnb. There's a PM that was so excited to make the APM program and it just never really happens. It just takes so much work. And to your point, you have to set up for success. You want to make sure there's clear paths and you upgrade to a regular PM and have the interview.

Annie Pearl (00:12:49):
And are we doing, is this really an APM program for internal folks? Is this external? Are we going to be really trying to promote this? So, I think there's a lot of ancillary activities around the actual program itself that have to be taken into consideration to make sure that it is actually very successful.

Lenny (00:13:06):
Yeah. Maybe a last point we should probably imagine you agree with is generally just hard to get into product management. That's like the default. There's just not that many roles at companies versus say, engineers or some other functions. 

Annie Pearl (00:13:17):
Right.

Lenny (00:13:17):
So, I think that's just like there are not that many roles. It's a difficult role to break into, but these are the ways you can do it if you actually want to. 

Annie Pearl (00:13:17):
That's right. Yep.

Lenny (00:13:27):
Okay. So, I want to transition a little bit to talking about Calendly. 

Annie Pearl (00:13:29):
Sure. 

Lenny (00:13:29):
There are two areas I want to go. One is just how do you build product at Calendly? What have you learned about product development and team building? And then two, talk about how Calendly grows and what you've learned about growing a product like Calendly. It's such an interesting product, especially from a growth perspective. So, to start on just how product is built at Calendly, just a little context. How many product managers are there and how many PMs are there? How many people total, roughly? Yeah, just to give us a little bit of [inaudible 00:13:57].

Annie Pearl (00:13:56):
Let's see, when I joined about two years ago, I think the company was about 150 people and I think we're about 600 now. And then the product team, there were about 15 product managers and designers when I joined again about two years ago, and I think we're around 60 this year.

Lenny (00:14:08):
Wow. So, 60 product managers.

Annie Pearl (00:14:14):
Product managers, designers, and a research team. Yeah.

Lenny (00:14:16):
Got it. What about just PMs?

Annie Pearl (00:14:20):
PMs probably, my guess is 20. 

Lenny (00:14:23):
Cool.

Annie Pearl (00:14:24):
Twenty-ish. Yeah.

Lenny (00:14:25):
Cool. And then, can you talk about how the product team is structured roughly? If you think about a tree, [inaudible 00:14:30] tree.

Annie Pearl (00:14:30):
Yeah. So, as I mentioned, we've got product managers, we have designers, we have a research team, and then product operations. And then, on my product leadership team, we have head of design, head of research, head of product operations. And then, within the product management team, I have leaders across core, across enterprise and platform.

Lenny (00:14:50):
Got it. So, you manage the design team and engineering team, you said?

Annie Pearl (00:14:54):
Not engineering. Design, product and research. Yeah.

Lenny (00:14:56):
Got it. Something that I find is one of the big differences between product orgs is design reporting up to a product leader versus not. What's the rationale there? And then has Calendly tried a different approach?

Annie Pearl (00:15:08):
Yeah. So, when I was at Glassdoor in the CPO role, I had the opportunity to lead design for the first time. So, coming into Calendly, I had led both product and design as well as research. And so, I think it made sense given I'd already done it once to keep that structure coming into Calendly. I think at the end of the day, the real benefit of the structure is really to say at the end, we want to be thinking about everything we're doing through the lens of the end-to-end user experience. And so, if we have product managers who are really prioritizing the problems we're going to go after, and we've got designers who are really trying to think about how do we bring solutions to life to solve those problems, having both of those functions roll into one person just really allows us to think more holistically around the end-to-end user experience.

(00:15:53):
So, certainly, it can work where you have product and design reporting into different leaders that ultimately report into the CEO, but when you get to this level of scale from just a pure people management, but also just the scale of the business, you know often see this consolidation where product and design start to roll into one leader. And at least in my experience, I think it can help ensure that all the different pieces of work are integrated well together and ultimately deliver a better experience for customers.

Lenny (00:16:23):
So, it sounds like before you joined it wasn't like that. And if that's true, was there something that improved with that shift?

Annie Pearl (00:16:29):
So, the structure was there that way. At that time, we didn't have a head of design, so we had a lot of really great individual contributors and who had been, many of whom had been with the company for quite some time and really contributed to the great user experience that existed in the product. But we didn't have a design leader. So, one of the first leadership hires I made was to bring in a head of design to really build out that function. And then, that head of design is a peer partnering with the different heads of products across the product management organization as well.

Lenny (00:16:59):
What about in terms of the structure, whatever you can share one level below, how do you structure teams? Is it around outcomes? Is it around features of the product? Is it around type of persona? How do you think about that? 

Annie Pearl (00:17:13):
Yeah, yeah. So, we have a core team who's really responsible for the core end to-end user experience. In many ways, they're both doing feature development and then they're also doing growth work. So, they're thinking about how do we build new features and functionalities to help our core personas, which is typically folks who are in sales, recruiting and customer success. So, anyone in an externally facing role, we're really trying to help them do their jobs better. So, the core team's thinking about features and functionalities to really help our core end user persona. And then growth work to think about the PLG funnel, everything from acquisition, activation, conversion, and retention. So, that's one group. 

(00:17:49):
And then, second group is our "enterprise group." And they're really thinking about two different personas. One is the IT admin, the person who needs to make sure that Calendly is secure and that they have all the reporting mechanisms to be able to manage their account and all the tools to manage users and groups at scale. And the second piece of that is also departmental leaders. So as Calendly selling into or being used by a sales organization, the head of sales is not the IT admin, but they are a Teams admin who needs to manage their organization within Calendly. So, the enterprise group really thinks both about the admin, but also sort of the departments and how do we better serve departments. 

(00:18:31):
And then lastly, we have a platform team who's really thinking about how do we embed Calendly into the business processes of the organizations that we support and that we provide our product into. And so, that's everything from partnerships and integrations to our APIs.

Lenny (00:18:46):
Interesting. So, it's like problem focused/persona focused. Who are you trying to sell it to?

Annie Pearl (00:18:54):
That's right, that's right. Yeah, trying to sell it to, and then the persona of who's going to be using the functionality. And then, really having those teams hone and own those personas as they're developing functionality within the product.

Lenny (00:19:07):
What's your take on OKRs? Do you all use OKRs in some form?

Annie Pearl (00:19:11):
Yes, we do. We use OKRs both at the company level. So, we have three main OKRs that we're focused on for this year, for example, across the whole company. And then we have department level OKRs, many of which are in support of the company level OKRs, but then there's some additional things that we'll be doing at the department level, for example, that aren't going to show up at the company level. So yeah, we use them both at the company as well as on the product side.

Lenny (00:19:34):
Is there anything you've learned about making OKRs work? People love them. People hate them. 

Annie Pearl (00:19:39):
Yeah.

Lenny (00:19:39):
Is there something you do to make OKRs work? Something you've changed, something you've learned over time in how to work with OKRs?

Annie Pearl (00:19:44):
Yeah. When I first joined, I'd say we didn't have this muscle well built out. We didn't really have a clear product strategy at the time or clear OKRs guiding the work. And so, there was a lot of great work happening, but it really was unclear how it all fit together or how we were going to measure success in that work. So that was a first phase. I think the second phase for us was we developed a product strategy. We then had product team OKRs that corresponded to that product strategy, but they were really contained to the product team and each department across the organization had their own kind of siloed OKRs. 

(00:20:20):
And then, phase three, where really, I'd say we headed into this year, we have a really clear set, as I mentioned, of company OKRs and then in these really tightly integrated plans across the company around how we're going to support the key results and ultimately deliver on the objectives. And this has been a really incredible transformation of dependency mapping, being able to make sure that we're pulling all the levers across the organization to drive our most important objective.

(00:20:47):
So, I think it's just the kind of maturing of the business from almost no OKRs to product team OKRs to now company OKRs in a really tight planning process to make sure there's a lot of integration across the company to support what we need to do as a business.

Lenny (00:21:01):
So, what I'm hearing is one of the biggest changes in learnings was to connect OKRs across from the top to the bottom, right?

Annie Pearl (00:21:08):
Absolutely. Absolutely.

Lenny (00:21:09):
Is there anything else that has made a big impact on your ability to build and ship and execute as a company in terms of changes you've made in terms of how the company and how the teams build?

Annie Pearl (00:21:20):
I think one of the biggest changes that we've made, when I first joined, again, we had a product that served a lot of horizontal users. We help solo users who are freelancers, consultants. We help sales teams, we help recruiting teams, we help customer success, we help folks in education. So, we had a very broad user base. And what that means is that product managers in particular are I think had a really hard time prioritizing. At any point in time, it was really difficult to say, should I do work on feature A or for feature B without that clarity? And so, I think one of the most impactful things we did pretty early on in my tenure here was to hone in on our overall product strategy, but a poor piece of that being what's the actual market we're going after? What are the segments of that market? Who are the personas within the segments of that market?

(00:22:09):
And so, we've made a pretty clear distinction now that while a lot of the feature work that we'll do to support our target personas of sales teams, customer success teams and recruiting teams will impact folks who are not in those personas. Those are the core ICPs that we're going after. And so, historically, that would've been always a sort of trade off decision and a question. And now I think we have a lot of rigor around who our target market and then persona we're going after. And so, teams can use that to prioritize and also just deliver better value for those users.

Lenny (00:22:45):
So, it sounds like the biggest unlock and one of the biggest unlocks for making the team more efficient, move faster, make decisions quicker, is narrowing in on exactly who you're going to be selling to.

Annie Pearl (00:22:55):
I think it's one of the harder things for companies to do. So, it sounds relatively easy, and I think most companies believe that they have clarity around this. But then when you go down into the weeds of asking someone who's product manager or a designer, I don't know that it's always as clear because there's always a bit of a hesitation to say no, right? And the idea of saying no is scary. When in reality, the ability to say no is going to allow you to make sure you're building something that's going to be amazing for the people that matter most and not something that's going to be average or okay for a lot of different people.

Lenny (00:23:28):
Was there anything that was really hard about actually executing that, convincing people we're going to narrow and not worry about these people and any lessons from going through that process? Because I imagine a lot of founders listening are like, "Oh, that sounds we should be doing this, but oh man, we're leaving all this money on the table, people are going to be pissed."

Annie Pearl (00:23:45):
Yeah, I think it's a pretty big cultural shift. So, some of this intersects with the shift from product led growth to adding in a sales motion. So, when I joined Calendly, all of our ARR came from our PLG channel. We didn't have a sales team, we just hired a CRO who was going to build out a sales team. And so, in that world, the way you think about product, the way you think about processes, even the people you have on the team are tailored to that business model. And then, as we sort of moved up market and have now explicitly started to go after teams of users and departments of users and organizations of larger scale, everything about people, process, and product all changes. 

(00:24:29):
I think I touched on culture because I think that's pervasive across the entire organization. The way that things get done has to be highly integrated versus can be a bit more siloed when you're just sort of the self-service PLG business that in many ways runs itself through the product being well optimized. So, there's a lot of process change that needs to happen, the type of people that you need to bring into the organization, that changes as you layer in the new selling motion. And then the product itself of course has to change. 

(00:24:56):
So, I guess let's just say the example of PLG and SLG or the direct selling motion is tied in to your question around what are the things that need to change in order to get clear on your target user? And I think it's highly cultural in nature across people, across process, and then obviously across the actual product itself.

Lenny (00:25:15):
I have a whole bunch of questions about how Calendly grows and maybe we just get into some of the stuff because I imagine a lot of people are interested. First, let me ask this. I imagine Calendly mostly grows through, I sign up for Calendly, they send it to everyone when I book a meeting and they're like, "Oh, what is this?" And they're like, "Oh, cool, I'm going to use this." And then they start using, it spreads, and then sales eventually finds people at a company that are using it a lot and tries to get the whole company in it. Is that roughly right?

_[435 additional lines trimmed for context budget]_

---

